{
  "model_type": "sklearn_logistic_regression",
  "training_history": {},
  "metadata": {
    "dataset_info": {
      "total_assistant_tokens": 35038,
      "truth_tokens": 29014,
      "lie_tokens": 6024,
      "truth_ratio": 0.8280723785604202,
      "lie_ratio": 0.17192762143957988
    },
    "num_examples": 500,
    "num_assistant_tokens": 35038,
    "hidden_size": 4096,
    "config": {
      "dataset_path": "data/NTML-datasets/4T1L_500samples.jsonl",
      "max_length": 512,
      "model_name": "meta-llama/Llama-3.1-8B-Instruct",
      "hook_point": "blocks.1.hook_resid_pre",
      "hook_layer": 1,
      "device": "cuda",
      "dtype": "bfloat16",
      "batch_size": 32,
      "learning_rate": 0.001,
      "num_epochs": 20,
      "weight_decay": 0.001,
      "train_ratio": 0.8,
      "handle_class_imbalance": true,
      "optimizer_type": "AdamW",
      "scheduler_type": "cosine",
      "warmup_ratio": 0.1,
      "gradient_clip_norm": 1.0,
      "cache_dir": "./cache/ntml_binary/meta_llama_Llama_3.1_8B_Instruct_4T1L_500samples",
      "activation_batch_size": 16,
      "force_recache": false,
      "output_dir": "./ntml_4T1L_500samples_sklearnC1",
      "probe_name": "ntml_binary_4T1L_500samples_layer_1",
      "save_checkpoints": true,
      "checkpoint_every": 5,
      "verbose": true,
      "log_every": 10,
      "eval_every": 1,
      "ignore_system_tokens": true,
      "ignore_user_tokens": true,
      "min_tokens_per_statement": 1,
      "token_overlap_strategy": "majority",
      "probe_method": "sklearn",
      "sklearn_C": 1.0,
      "sklearn_C_sweep": false,
      "sklearn_C_values": [
        0.0001,
        0.001,
        0.01,
        0.1,
        1.0,
        10.0,
        100.0,
        1000.0,
        10000.0
      ],
      "sklearn_solver": "liblinear",
      "sklearn_max_iter": 1000,
      "pytorch_bias": true,
      "pytorch_normalize_weights": true
    },
    "training_history": {},
    "final_metrics": {
      "accuracy": 0.6532534246575342,
      "precision": 0.24774774774774774,
      "recall": 0.505008347245409,
      "f1": 0.3324175824175824,
      "auroc": 0.6265936629896645
    },
    "training_time": 1152.0552015304565,
    "num_parameters": 4097,
    "regularization_results": {},
    "best_C": 1.0,
    "used_dtype": "<class 'numpy.float16'>"
  },
  "config": {
    "dataset_path": "data/NTML-datasets/4T1L_500samples.jsonl",
    "max_length": 512,
    "model_name": "meta-llama/Llama-3.1-8B-Instruct",
    "hook_point": "blocks.1.hook_resid_pre",
    "hook_layer": 1,
    "device": "cuda",
    "dtype": "bfloat16",
    "batch_size": 32,
    "learning_rate": 0.001,
    "num_epochs": 20,
    "weight_decay": 0.001,
    "train_ratio": 0.8,
    "handle_class_imbalance": true,
    "optimizer_type": "AdamW",
    "scheduler_type": "cosine",
    "warmup_ratio": 0.1,
    "gradient_clip_norm": 1.0,
    "cache_dir": "./cache/ntml_binary/meta_llama_Llama_3.1_8B_Instruct_4T1L_500samples",
    "activation_batch_size": 16,
    "force_recache": false,
    "output_dir": "./ntml_4T1L_500samples_sklearnC1",
    "probe_name": "ntml_binary_4T1L_500samples_layer_1",
    "save_checkpoints": true,
    "checkpoint_every": 5,
    "verbose": true,
    "log_every": 10,
    "eval_every": 1,
    "ignore_system_tokens": true,
    "ignore_user_tokens": true,
    "min_tokens_per_statement": 1,
    "token_overlap_strategy": "majority",
    "probe_method": "sklearn",
    "sklearn_C": 1.0,
    "sklearn_C_sweep": false,
    "sklearn_C_values": [
      0.0001,
      0.001,
      0.01,
      0.1,
      1.0,
      10.0,
      100.0,
      1000.0,
      10000.0
    ],
    "sklearn_solver": "liblinear",
    "sklearn_max_iter": 1000,
    "pytorch_bias": true,
    "pytorch_normalize_weights": true
  },
  "used_dtype": "<class 'numpy.float16'>"
}