Directory structure:
└── curt-tigges-probity.git/
    ├── README.md
    ├── LICENSE
    ├── pyproject.toml
    ├── probity/
    │   ├── __init__.py
    │   ├── .DS_Store
    │   ├── collection/
    │   │   ├── __init__.py
    │   │   ├── activation_store.py
    │   │   └── collectors.py
    │   ├── datasets/
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── position_finder.py
    │   │   ├── templated.py
    │   │   └── tokenized.py
    │   ├── pipeline/
    │   │   └── pipeline.py
    │   ├── probes/
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── config.py
    │   │   ├── directional.py
    │   │   ├── inference.py
    │   │   ├── linear.py
    │   │   ├── logistic.py
    │   │   ├── probe_set.py
    │   │   └── sklearn_logistic.py
    │   ├── training/
    │   │   ├── __init__.py
    │   │   └── trainer.py
    │   └── utils/
    │       └── dataset_loading.py
    ├── tests/
    │   ├── test_probe_save_load.py
    │   └── unit/
    │       ├── datasets/
    │       │   ├── test_base.py
    │       │   ├── test_position_finder.py
    │       │   ├── test_templated.py
    │       │   └── test_tokenized.py
    │       ├── pipeline/
    │       │   └── test_pipeline.py
    │       └── trainer/
    │           └── test_trainer.py
    └── tutorials/
        ├── 1-probity-basics.py
        ├── 2-dataset-creation.py
        ├── 3-probe-variants.py
        └── 4-multiclass-probe.py

================================================
FILE: README.md
================================================
# Probity

A Python library for creating, managing, and analyzing probes for large language models.

## Overview

Probity is a toolkit for interpretability research on neural networks, with a focus on analyzing internal representations through linear probing. It provides a comprehensive suite of tools for:

- Creating and managing datasets for probing experiments
- Collecting and storing model activations
- Training various types of probes (linear, logistic, PCA, etc.)
- Analyzing and interpreting probe results

### What Makes This Different From SKLearn?
- Runs on Pytorch natively where applicable
- Extensive dataset management tools, including tools for keeping track of character and token positions for labeled items
- Built-in activation collector, using TransformerLens as a model runner (will support NNsight in the near future as well)
- Designed for mech interp specifically

## Installation

```bash
# Clone the repository
git clone https://github.com/curt-tigges/probity.git
cd probity

# Install the library
pip install -e .
```

## Quick Start

```python
from probity.datasets.templated import TemplatedDataset, TemplateVariable, Template
from probity.datasets.tokenized import TokenizedProbingDataset
from probity.probes.linear_probe import LogisticProbe, LogisticProbeConfig
from probity.training.trainer import SupervisedProbeTrainer, SupervisedTrainerConfig
from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig
from probity.probes.inference import ProbeInference
from transformers import AutoTokenizer

# Create a templated dataset
template_var = TemplateVariable(
    name="SENTIMENT", 
    values=["positive", "negative"],
    attributes={"label": [1, 0]},
    class_bound=True,  # This ensures this variable is always tied to the class 
    class_key="label"  
)

template = Template(
    template="This is a {SENTIMENT} example.",
    variables={"SENTIMENT": template_var},
    attributes={"task": "sentiment_analysis"}
)

dataset = TemplatedDataset(templates=[template])
probing_dataset = dataset.to_probing_dataset(
    label_from_attributes="label",
    auto_add_positions=True
)

# Tokenize the dataset
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
    dataset=probing_dataset,
    tokenizer=tokenizer,
    padding=True,
    max_length=128,
    add_special_tokens=True
)

# Configure the probe, trainer, and pipeline
model_name = "gpt2"
hook_point = "blocks.7.hook_resid_pre"

# Set up logistic probe configuration
probe_config = LogisticProbeConfig(
    input_size=768,
    normalize_weights=True,
    bias=False,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=7,
    name="sentiment_probe"
)

# Set up trainer configuration
trainer_config = SupervisedTrainerConfig(
    batch_size=32,
    learning_rate=1e-3,
    num_epochs=10,
    weight_decay=0.01,
    train_ratio=0.8,  # 80-20 train-val split
    handle_class_imbalance=True,
    show_progress=True
)

# Set up pipeline configuration
pipeline_config = ProbePipelineConfig(
    dataset=tokenized_dataset,
    probe_cls=LogisticProbe,
    probe_config=probe_config,
    trainer_cls=SupervisedProbeTrainer,
    trainer_config=trainer_config,
    position_key="SENTIMENT",  # Probe at the sentiment word position
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/sentiment_probe_cache"
)

# Train the probe
pipeline = ProbePipeline(pipeline_config)
probe, training_history = pipeline.run()

# Use the probe for inference
inference = ProbeInference(
    model_name=model_name,
    hook_point=hook_point,
    probe=probe
)

texts = ["This is a positive example.", "This is a negative example."]
probabilities = inference.get_probabilities(texts)
```

## Key Features

### Dataset Creation

Probity offers multiple ways to create and manage datasets:

1. **Templated Datasets**: Create datasets with parametrized templates
2. **Custom Datasets**: Build datasets from scratch with fine-grained control
3. **Tokenization**: Convert character-based datasets to token-based for model compatibility
4. **Position Tracking**: Automatically track positions of interest in text

### Probe Types

The library supports various types of probes:

- **LinearProbe**: Simple linear probe with MSE loss
- **LogisticProbe**: Probe using logistic regression (binary classification)
- **MultiClassLogisticProbe**: Probe using logistic regression for multi-class classification
- **PCAProbe**: Probe using principal component analysis
- **KMeansProbe**: Probe using K-means clustering
- **MeanDifferenceProbe**: Probe using mean differences between classes

### Pipeline Components

- **Collection**: Tools for collecting and storing model activations
- **Training**: Supervised and unsupervised training frameworks
- **Inference**: Utilities for applying trained probes to new data
- **Analysis**: Methods for analyzing and visualizing probe results

## Examples

See the `tutorials/` directory for comprehensive examples of using Probity:

- `1-probity-basics.py`: Demonstrates the basic workflow with a Logistic Probe for sentiment analysis.
- `2-dataset-creation.py`: Shows various methods for creating templated and custom datasets.
- `3-probe-variants.py`: Compares different probe types (Linear, Logistic, PCA, KMeans, MeanDiff).
- `4-multiclass-probe.py`: Explains how to use the MultiClass Logistic Probe.

## Project Structure

- **probity/collection/**: Activation collection and storage
- **probity/datasets/**: Dataset creation and management
- **probity/probes/**: Probe implementation and utilities
- **probity/training/**: Training frameworks for probes
- **probity/utils/**: Utility functions and helpers
- **tutorials/**: Example notebooks and tutorials
- **tests/**: Unit and integration tests

## Citation

If you use Probity in your research, please cite:

```
@software{probity,
  author = {Tigges, Curt},
  title = {Probity: A Toolkit for Neural Network Probing},
  year = {2025},
  url = {https://github.com/curttigges/probity}
}
```


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Curt Tigges

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[tool.black]
line-length = 88
target-version = ['py37']
include = '\.pyi?$'

[tool.flake8]
max-line-length = 88
extend-ignore = "E203"  # Ignore whitespace before ':' (conflicts with Black)
per-file-ignores = []
exclude = [
    ".git",
    "__pycache__",
    "build",
    "dist",
]

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "probity"
version = "0.1.0"
description = "A library for probing neural networks"
requires-python = ">=3.7"
dependencies = [
    "torch>=2.0.0",
    "transformers>=4.30.0",
    "numpy>=1.24.0",
    "matplotlib>=3.7.0",
    "transformer_lens>=1.0.0",
    "scikit-learn>=1.3.0",
    "datasets>=2.12.0",
    "tqdm>=4.65.0",
    "tabulate>=0.9.0",
    "neuronpedia",
]

[tool.setuptools]
packages = ["probity"]


================================================
FILE: probity/__init__.py
================================================



================================================
FILE: probity/.DS_Store
================================================
[Non-text file]


================================================
FILE: probity/collection/__init__.py
================================================



================================================
FILE: probity/collection/activation_store.py
================================================
import os
import torch
from dataclasses import dataclass
from typing import Callable, List, Tuple
from probity.datasets.tokenized import TokenizedProbingDataset

@dataclass
class ActivationStore:
    """Stores and provides access to model activations."""
    
    # Core storage
    raw_activations: torch.Tensor  # Shape: (num_examples, seq_len, hidden_size)
    hook_point: str  # Which part of model these came from
    
    # Dataset information
    labels: torch.Tensor  # Shape: (num_examples,) numeric labels
    label_texts: List[str]  # Original text labels
    example_indices: torch.Tensor  # Shape: (num_examples,) maps to dataset indices
    sequence_lengths: torch.Tensor  # Shape: (num_examples,) actual lengths before padding
    hidden_size: int
    
    # Keep reference to original dataset for position lookups
    dataset: TokenizedProbingDataset

    def get_full_sequence_activations(self) -> torch.Tensor:
        """Get activations for complete sequences.
        
        Returns:
            Tensor of shape (num_examples, seq_len, hidden_size)
        """
        return self.raw_activations

    def get_position_activations(self, position_key: str) -> torch.Tensor:
        """Get activations for a specific position key from dataset.
        
        Args:
            position_key: Key from TokenPositions dictionary
            
        Returns:
            If single position: (num_examples, hidden_size)
            If multiple positions: (total_positions, hidden_size)
        """
        positions = []
        print(f"Examining position key: {position_key}")
        for idx in self.example_indices:
            example = self.dataset.examples[idx]
            if example.token_positions:
                pos = example.token_positions[position_key]
                if isinstance(pos, int):
                    positions.append(self.raw_activations[idx, pos])
                else:  # List[int]
                    positions.extend([self.raw_activations[idx, p] for p in pos])
        
        return torch.stack(positions)

    def get_activations_by_fn(self, position_fn: Callable) -> torch.Tensor:
        """Get activations at positions specified by a function.
        
        Args:
            position_fn: Function that takes example and returns position(s)
            
        Returns:
            Tensor of gathered activations
        """
        # Stub for now
        raise NotImplementedError
    

    def get_probe_data(self, position_key: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get activations and labels formatted for probe training.
        
        Args:
            position_key: Key from TokenPositions dictionary
            
        Returns:
            Tuple of (activations, labels) where:
            - For single positions: (num_examples, hidden_size), (num_examples,)
            - For multiple positions: (total_positions, hidden_size), (total_positions,)
              Labels are repeated for examples with multiple positions
        """
        activations = self.get_position_activations(position_key)
        
        # Handle label replication for multiple positions
        if len(activations) > len(self.labels):
            # Count positions per example to know how many times to repeat each label
            position_counts = [
                len(ex.token_positions[position_key]) if isinstance(ex.token_positions[position_key], list)
                else 1 
                for ex in self.dataset.examples
            ]
            labels = torch.repeat_interleave(self.labels, torch.tensor(position_counts))
        else:
            labels = self.labels
            
        return activations, labels

    def save(self, path: str) -> None:
        """Save cache to disk."""
        os.makedirs(path, exist_ok=True)
        torch.save({
            'raw_activations': self.raw_activations,
            'hook_point': self.hook_point,
            'labels': self.labels,
            'label_texts': self.label_texts,
            'example_indices': self.example_indices,
            'sequence_lengths': self.sequence_lengths,
            'hidden_size': self.hidden_size
        }, os.path.join(path, 'cache.pt'))
        
        # Save dataset separately since it contains complex objects
        self.dataset.save(os.path.join(path, 'dataset'))

    @classmethod
    def load(cls, path: str) -> 'ActivationStore':
        """Load cache from disk."""
        cache_data = torch.load(os.path.join(path, 'cache.pt'))
        dataset = TokenizedProbingDataset.load(os.path.join(path, 'dataset'))
        
        return cls(
            dataset=dataset,
            **cache_data
        )


================================================
FILE: probity/collection/collectors.py
================================================
import torch
from dataclasses import dataclass
from typing import List, Dict
from transformer_lens import HookedTransformer
from probity.datasets.tokenized import TokenizedProbingDataset
from probity.collection.activation_store import ActivationStore


@dataclass
class TransformerLensConfig:
    """Configuration for TransformerLensCollector."""

    model_name: str
    hook_points: List[str]  # e.g. ["blocks.12.hook_resid_post"]
    batch_size: int = 32
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


class TransformerLensCollector:
    """Collects activations using TransformerLens."""

    def __init__(self, config: TransformerLensConfig):
        self.config = config
        print(f"Initializing collector with device: {config.device}")
        self.model = HookedTransformer.from_pretrained_no_processing(config.model_name)
        print(f"Moving model to device: {config.device}")
        self.model.to(config.device)

    @staticmethod
    def get_layer_from_hook_point(hook_point: str) -> int:
        """Extract layer number from hook point string.
        
        Args:
            hook_point: Hook point string (e.g. "blocks.12.hook_resid_post")
            
        Returns:
            Layer number
        """
        try:
            # Extract number after "blocks."
            layer = int(hook_point.split(".")[1])
            return layer
        except (IndexError, ValueError):
            raise ValueError(f"Could not extract layer from hook point: {hook_point}")

    def collect(
        self,
        dataset: TokenizedProbingDataset,
    ) -> Dict[str, ActivationStore]:
        """Collect activations for each hook point.

        Returns:
            Dictionary mapping hook points to ActivationCache objects
        """
        all_activations = {}

        # Set model to evaluation mode
        self.model.eval()

        # Get maximum layer needed
        max_layer = max(
            self.get_layer_from_hook_point(hook)
            for hook in self.config.hook_points
        )

        # Process in batches
        with torch.no_grad():  # Disable gradient computation for determinism
            for batch_start in range(0, len(dataset.examples), self.config.batch_size):
                batch_end = min(batch_start + self.config.batch_size, len(dataset.examples))
                batch_indices = list(range(batch_start, batch_end))

                # Get batch tensors
                batch = dataset.get_batch_tensors(batch_indices)

                # Run model with caching
                _, cache = self.model.run_with_cache(
                    batch["input_ids"].to(self.config.device),
                    names_filter=self.config.hook_points,
                    return_cache_object=True,
                    stop_at_layer=max_layer + 1
                )

                # Store activations for each hook point
                for hook in self.config.hook_points:
                    if hook not in all_activations:
                        all_activations[hook] = []
                    all_activations[hook].append(cache[hook].cpu())

        # Create ActivationCache objects
        return {
            hook: ActivationStore(
                raw_activations=torch.cat(activations, dim=0),
                hook_point=hook,
                example_indices=torch.arange(len(dataset.examples)),
                sequence_lengths=torch.tensor(dataset.get_token_lengths()),
                hidden_size=activations[0].shape[-1],
                dataset=dataset,
                labels=torch.tensor([ex.label for ex in dataset.examples]),
                label_texts=[ex.label_text for ex in dataset.examples],
            )
            for hook, activations in all_activations.items()
        }



================================================
FILE: probity/datasets/__init__.py
================================================



================================================
FILE: probity/datasets/base.py
================================================
from dataclasses import dataclass
from typing import Dict, List, Optional, Union, Set, Callable, cast, Any
from datasets import Dataset
import json
from .position_finder import Position
import os


@dataclass
class CharacterPositions:
    """Container for character positions of interest."""

    positions: Dict[str, Union[Position, List[Position]]]

    def __getitem__(self, key: str) -> Union[Position, List[Position]]:
        return self.positions[key]

    def keys(self) -> Set[str]:
        return set(self.positions.keys())


@dataclass
class ProbingExample:
    """Single example for probing."""

    text: str
    label: Union[int, float]  # Numeric label
    label_text: str  # Original text label
    character_positions: Optional[CharacterPositions] = None
    group_id: Optional[str] = None  # For cross-validation/grouping
    attributes: Optional[Dict] = (
        None  # Additional attributes about this example (previously called metadata)
    )


class ProbingDataset:
    """Base dataset for probing experiments."""

    def __init__(
        self,
        examples: List[ProbingExample],
        task_type: str = "classification",  # or "regression"
        valid_layers: Optional[List[str]] = None,
        label_mapping: Optional[Dict[str, int]] = None,
        dataset_attributes: Optional[
            Dict
        ] = None,  # Dataset-level attributes (previously metadata)
    ):
        self.task_type = task_type
        self.valid_layers = valid_layers
        self.label_mapping = label_mapping
        self.dataset_attributes = dataset_attributes or {}
        self.examples = examples
        self.dataset = self._to_hf_dataset()

        # Infer position types from examples if available
        self.position_types = set()
        for example in self.examples:
            if example.character_positions:
                self.position_types.update(example.character_positions.keys())

    def __len__(self) -> int:
        """Return the number of examples in the dataset."""
        return len(self.examples)

    def add_target_positions(
        self, key: str, finder: Callable[[str], Union[Position, List[Position]]]
    ) -> None:
        """Add target positions using a finding strategy.

        Args:
            key: Name for this set of positions
            finder: Function that finds positions in text
        """
        for example in self.examples:
            positions = finder(example.text)

            # Initialize character_positions if needed
            if example.character_positions is None:
                example.character_positions = CharacterPositions({})

            # Add new positions
            example.character_positions.positions[key] = positions

        # Update the position_types attribute with the new key
        self.position_types.add(key)

    def _to_hf_dataset(self) -> Dataset:
        """Convert to HuggingFace dataset format."""
        data_dict: Dict[str, List[Optional[Union[str, int, float, List]]]] = {
            "text": [],
            "label": [],
            "label_text": [],
            "group_id": [],
            "attributes_json": [],  # Add column for attributes JSON string
        }

        # Add position data if available
        position_keys = set()
        for ex in self.examples:
            if ex.character_positions:
                position_keys.update(ex.character_positions.keys())

        for key in position_keys:
            data_dict[f"char_pos_{key}_start"] = []
            data_dict[f"char_pos_{key}_end"] = []
            data_dict[f"char_pos_{key}_multi"] = []  # For multiple matches

        # Populate data
        for ex in self.examples:
            data_dict["text"].append(ex.text)
            data_dict["label"].append(ex.label)
            data_dict["label_text"].append(ex.label_text)
            data_dict["group_id"].append(ex.group_id)
            # Save attributes as JSON string
            attributes_str = json.dumps(ex.attributes) if ex.attributes else None
            data_dict["attributes_json"].append(attributes_str)

            # Add position data
            if ex.character_positions:
                for key in position_keys:
                    pos = ex.character_positions.positions.get(key)
                    if pos is None:
                        data_dict[f"char_pos_{key}_start"].append(None)
                        data_dict[f"char_pos_{key}_end"].append(None)
                        data_dict[f"char_pos_{key}_multi"].append([])
                    elif isinstance(pos, Position):
                        data_dict[f"char_pos_{key}_start"].append(pos.start)
                        data_dict[f"char_pos_{key}_end"].append(pos.end)
                        data_dict[f"char_pos_{key}_multi"].append([])
                    else:  # List[Position]
                        # Store first match in main columns
                        data_dict[f"char_pos_{key}_start"].append(
                            pos[0].start if pos else None
                        )
                        data_dict[f"char_pos_{key}_end"].append(
                            pos[0].end if pos else None
                        )
                        # Store all matches in multi column
                        data_dict[f"char_pos_{key}_multi"].append(
                            [(p.start, p.end) for p in pos]
                        )
            else:
                for key in position_keys:
                    data_dict[f"char_pos_{key}_start"].append(None)
                    data_dict[f"char_pos_{key}_end"].append(None)
                    data_dict[f"char_pos_{key}_multi"].append([])

        return Dataset.from_dict(data_dict)

    @classmethod
    def from_hf_dataset(
        cls,
        dataset: Dataset,
        position_types: List[str],
        label_mapping: Optional[Dict[str, int]] = None,
        **kwargs,
    ) -> "ProbingDataset":
        """Create ProbingDataset from a HuggingFace dataset."""
        examples = []

        for item_raw in dataset:
            item = cast(Dict[str, Any], item_raw)  # Cast to Dict with Any values
            # Explicitly type the positions dictionary
            positions: Dict[str, Union[Position, List[Position]]] = {}
            for pt in position_types:
                # Check for single position
                start = item.get(f"char_pos_{pt}_start")
                end = item.get(f"char_pos_{pt}_end")
                multi = item.get(f"char_pos_{pt}_multi", [])

                if multi:
                    # Handle multiple positions
                    positions[pt] = [Position(start=s, end=e) for s, e in multi]
                elif start is not None and end is not None:
                    # Handle single position
                    positions[pt] = Position(start=start, end=end)
                # Only add key if multi or start/end were found
                elif multi or (start is not None and end is not None):
                    pass  # Already handled above
                else:
                    # If no position data found for this key, don't add it
                    pass

            # Only create CharacterPositions if positions dict is not empty
            char_positions_obj = CharacterPositions(positions) if positions else None

            # Load attributes from JSON string
            attributes = None
            attributes_str = item.get("attributes_json")
            if isinstance(attributes_str, str):
                try:
                    attributes = json.loads(attributes_str)
                except json.JSONDecodeError:
                    print(
                        f"Warning: Failed to decode attributes JSON: {attributes_str}"
                    )
                    attributes = None  # Or handle error as appropriate

            examples.append(
                ProbingExample(
                    text=str(item["text"]),
                    label=float(item["label"]),
                    label_text=str(item["label_text"]),
                    character_positions=char_positions_obj,  # Use potentially None object
                    group_id=item.get("group_id"),
                    attributes=attributes,  # Add loaded attributes
                )
            )

        return cls(
            examples=examples,
            label_mapping=label_mapping,  # Remove position_types from kwargs
            **kwargs,
        )

    def save(self, path: str) -> None:
        """Save dataset to disk."""
        # Save HF dataset
        self.dataset.save_to_disk(f"{path}/hf_dataset")

        # Get position types from the examples
        position_types = set()
        for example in self.examples:
            if example.character_positions:
                position_types.update(example.character_positions.keys())

        # Save attributes (previously metadata)
        attributes = {
            "task_type": self.task_type,
            "valid_layers": self.valid_layers,
            "label_mapping": self.label_mapping,
            "dataset_attributes": self.dataset_attributes,  # Renamed from metadata
            "position_types": list(position_types),  # Add position_types to attributes
        }

        with open(
            f"{path}/dataset_attributes.json", "w"
        ) as f:  # Renamed from metadata.json
            json.dump(attributes, f)

    @classmethod
    def load(cls, path: str) -> "ProbingDataset":
        """Load dataset from disk."""
        # Load HF dataset
        dataset = Dataset.load_from_disk(f"{path}/hf_dataset")

        # Try the new attributes filename first, fall back to old metadata.json for backward compatibility
        attributes_file = (
            f"{path}/dataset_attributes.json"
            if os.path.exists(f"{path}/dataset_attributes.json")
            else f"{path}/metadata.json"
        )

        # Load attributes
        with open(attributes_file, "r") as f:
            attributes = json.load(f)

        # Handle backward compatibility - if loading old file format
        dataset_attributes_key = (
            "dataset_attributes" if "dataset_attributes" in attributes else "metadata"
        )

        return cls.from_hf_dataset(
            dataset=dataset,
            position_types=attributes["position_types"],
            label_mapping=attributes["label_mapping"],
            task_type=attributes["task_type"],
            valid_layers=attributes["valid_layers"],
            dataset_attributes=attributes[dataset_attributes_key],
        )

    def train_test_split(
        self, test_size: float = 0.2, shuffle: bool = True, seed: Optional[int] = None
    ) -> tuple["ProbingDataset", "ProbingDataset"]:
        """Split into train and test datasets."""
        split = self.dataset.train_test_split(
            test_size=test_size, shuffle=shuffle, seed=seed
        )

        # Get position types from the original dataset
        original_position_types = list(self.position_types)

        # Pass original position types when creating new datasets
        train = self.from_hf_dataset(
            dataset=split["train"],
            position_types=original_position_types,
            label_mapping=self.label_mapping,
            task_type=self.task_type,
            valid_layers=self.valid_layers,
            dataset_attributes=self.dataset_attributes,  # Renamed metadata
        )

        test = self.from_hf_dataset(
            dataset=split["test"],
            position_types=original_position_types,
            label_mapping=self.label_mapping,
            task_type=self.task_type,
            valid_layers=self.valid_layers,
            dataset_attributes=self.dataset_attributes,  # Renamed metadata
        )

        return train, test



================================================
FILE: probity/datasets/position_finder.py
================================================
from typing import Callable, List, Union, Optional
import re
from dataclasses import dataclass
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast


@dataclass
class Position:
    """Represents a position or span in text."""

    start: int  # Character start position
    end: Optional[int] = None  # Character end position (exclusive)

    def __post_init__(self):
        if self.start < 0:
            raise ValueError("Start position must be non-negative")
        if self.end is not None and self.end < self.start:
            raise ValueError("End position must be greater than start position")


class PositionFinder:
    """Strategies for finding target positions in text."""

    @staticmethod
    def from_template(template: str, marker: str) -> Callable[[str], Position]:
        """Create finder for template-based positions.

        Args:
            template: Template string with marker (e.g. "The movie was {ADJ}")
            marker: The marker to find (e.g. "{ADJ}")

        Returns:
            Function that finds the position in a string matching the template
        """

        def finder(text: str) -> Position:
            # First escape the entire template
            regex = re.escape(template)

            # Then replace the escaped marker with a capture group
            escaped_marker = re.escape(marker)
            regex = regex.replace(escaped_marker, "(.*?)")

            # Replace other template variables with non-capturing wildcards
            for var in re.findall(r"\\\{([^}]+)\\\}", regex):
                var_marker = f"\\{{{var}\\}}"
                regex = regex.replace(var_marker, ".*?")
            # print(f"Template: {template}")
            # print(f"Marker: {marker}")
            # print(f"Regex: {regex}")
            # print(f"Text: {text}")
            match = re.match(regex, text)
            if not match:
                raise ValueError(f"Text does not match template: {text}")

            # Get the position of the matching group
            start = match.start(1)
            end = match.end(1)
            return Position(start, end)

        return finder

    @staticmethod
    def from_regex(pattern: str, group: int = 0) -> Callable[[str], List[Position]]:
        """Create finder for regex-based positions.

        Args:
            pattern: Regex pattern to match
            group: Which capture group to use for position (0 = full match)

        Returns:
            Function that finds all matching positions in a string
        """
        compiled = re.compile(pattern)

        def finder(text: str) -> List[Position]:
            positions = []
            for match in compiled.finditer(text):
                start = match.start(group)
                end = match.end(group)
                positions.append(Position(start, end))
            return positions

        return finder

    @staticmethod
    def from_char_position(pos: int) -> Callable[[str], Position]:
        """Create finder for fixed character positions.

        Args:
            pos: Character position to find

        Returns:
            Function that returns the fixed position
        """

        def finder(text: str) -> Position:
            if pos >= len(text):
                raise ValueError(f"Position {pos} is beyond text length {len(text)}")
            return Position(pos)

        return finder

    @staticmethod
    def convert_to_token_position(
        position: Position,
        text: str,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        space_precedes_token: bool = True,
        add_special_tokens: bool = True,
        padding_side: Optional[str] = None,
    ) -> int:
        """
        Convert character position to token position, handling special tokens properly.

        Args:
            position: Character position to convert
            text: Input text
            tokenizer: Tokenizer to use
            space_precedes_token: Whether space precedes token (default: True)
            add_special_tokens: Whether to add special tokens (default: True)
            padding_side: Override tokenizer padding side (default: None)

        Returns:
            Token index corresponding to the character position
        """
        # Unused: Get tokenizer padding side.
        # pad_side = padding_side if padding_side is not None \
        #            else getattr(tokenizer, "padding_side", "right")

        # First get clean offsets without special tokens
        clean_encoding = tokenizer(
            text, return_offsets_mapping=True, add_special_tokens=False
        )
        clean_offsets = clean_encoding["offset_mapping"]
        # clean_tokens = clean_encoding["input_ids"]

        # Find the token index in the clean encoding
        clean_token_idx = None
        assert isinstance(clean_offsets, list)  # Assure type checker it's iterable
        for idx, (start, end) in enumerate(clean_offsets):
            if start <= position.start < end:
                clean_token_idx = idx
                break

        if clean_token_idx is None:
            msg = (
                f"Character position {position.start} not aligned "
                f"with any token offset."
            )
            raise ValueError(msg)

        # If we don't need to account for special tokens, just return the clean index
        if not add_special_tokens:
            return clean_token_idx

        # Get tokens with special tokens to check for actual prefix tokens
        tokens_with_special = tokenizer(text, add_special_tokens=True)["input_ids"]

        # Count special tokens at the beginning IF they actually appear
        prefix_tokens = 0
        if (
            hasattr(tokenizer, "bos_token_id")
            and tokenizer.bos_token_id is not None
            and tokens_with_special  # Ensure list is not empty
            and tokens_with_special[0] == tokenizer.bos_token_id
        ):
            prefix_tokens += 1
        # Add checks for other potential prefix tokens if necessary (e.g., cls_token)
        # elif (
        #     hasattr(tokenizer, "cls_token_id")
        #     and tokenizer.cls_token_id is not None
        #     and tokens_with_special
        #     and tokens_with_special[0] == tokenizer.cls_token_id
        # ):
        #    prefix_tokens += 1

        # Calculate the token position relative to the sequence *with* special tokens
        position_with_special = clean_token_idx + prefix_tokens

        # Note: At this point, the position only accounts for special tokens like BOS,
        # but not for padding. Padding is dynamic and handled at batch preparation time
        # in get_batch_tensors.

        return position_with_special

    @staticmethod
    def validate_token_position(token_position: int, tokens: List[int]) -> bool:
        """Validate that a token position is valid for a sequence.

        Args:
            token_position: Position to validate
            tokens: Token sequence

        Returns:
            True if position is valid, False otherwise
        """
        return 0 <= token_position < len(tokens)



================================================
FILE: probity/datasets/templated.py
================================================
from dataclasses import dataclass
from typing import Dict, List, Optional, Union, Set
import re
from .base import ProbingDataset, ProbingExample
from .position_finder import PositionFinder


@dataclass
class TemplateVariable:
    """Represents a variable in a template."""

    name: str
    values: List[str]
    attributes: Optional[Dict] = None  # e.g., sentiment polarity, POS tag, etc.
    class_bound: bool = (
        False  # Whether this variable must match classes with other variables
    )
    class_key: Optional[str] = (
        None  # Attributes key that defines the class (e.g., "sentiment")
    )


@dataclass
class Template:
    """Represents a template with variables."""

    template: str  # e.g., "I thought this movie was {ADJ}, I {VERB} it."
    variables: Dict[str, TemplateVariable]  # e.g., {"ADJ": TemplateVariable(...)}
    attributes: Optional[Dict] = None

    def get_marker(self, var_name: str) -> str:
        """Get the marker for a variable in the template."""
        return f"{{{var_name}}}"

    def get_all_markers(self) -> Dict[str, str]:
        """Get all variable markers in the template."""
        return {name: self.get_marker(name) for name in self.variables.keys()}

    def validate(self) -> bool:
        """Validate that all variables in template exist in variables dict."""
        template_vars = set(re.findall(r"\{([^}]+)\}", self.template))
        defined_vars = set(self.variables.keys())
        return template_vars == defined_vars


class TemplatedDataset:
    """Dataset based on templates with variables."""

    def __init__(
        self,
        templates: List[Template],
        attributes: Optional[Dict] = None,  # Previously called metadata
    ):
        """Initialize TemplatedDataset.

        Args:
            templates: List of templates to use
            attributes: Additional attributes about the dataset
        """
        self.templates = templates
        self.attributes = attributes or {}  # Renamed from metadata

        # Validate all templates
        for template in templates:
            if not template.validate():
                raise ValueError(f"Invalid template: {template.template}")

    def to_probing_dataset(
        self,
        label_from_attributes: Optional[str] = None,
        label_map: Optional[Dict[str, int]] = None,
        auto_add_positions: bool = True,
    ) -> ProbingDataset:
        """Convert to ProbingDataset.

        Args:
            label_from_attributes: Key in variable attributes to use as label
            label_map: Mapping from attribute values to numeric labels
            auto_add_positions: Whether to automatically add positions for variables
        """
        examples = []

        # Generate all combinations for each template
        for template in self.templates:
            # Separate class-bound and neutral variables
            bound_vars = []
            neutral_vars = []
            for name, var in template.variables.items():
                if var.class_bound:
                    bound_vars.append(name)
                else:
                    neutral_vars.append(name)

            # Get all possible classes from the first bound variable
            possible_classes = set()
            if bound_vars:
                first_var = template.variables[bound_vars[0]]
                if first_var.attributes and first_var.class_key:
                    possible_classes = set(first_var.attributes[first_var.class_key])

            # For each class, generate combinations
            for class_value in possible_classes or [None]:
                # Get class-consistent values for bound variables
                bound_values = []
                for var_name in bound_vars:
                    var = template.variables[var_name]
                    if class_value is not None:
                        # Filter values to match the current class
                        class_indices = [
                            i
                            for i, v in enumerate(var.attributes[var.class_key])
                            if v == class_value
                        ]
                        values = [var.values[i] for i in class_indices]
                    else:
                        values = var.values
                    bound_values.append(values)

                # Get values for neutral variables
                neutral_values = [
                    template.variables[name].values for name in neutral_vars
                ]

                # Generate combinations
                from itertools import product

                bound_combinations = (
                    list(product(*bound_values)) if bound_values else [()]
                )
                neutral_combinations = (
                    list(product(*neutral_values)) if neutral_values else [()]
                )

                for bound_combo in bound_combinations:
                    for neutral_combo in neutral_combinations:
                        # Create text by substituting values
                        text = template.template
                        var_attributes: Dict[str, Optional[Dict]] = {}

                        # Insert bound variables
                        for name, value in zip(bound_vars, bound_combo):
                            marker = template.get_marker(name)
                            text = text.replace(marker, value)

                            # UPDATED: Slice out the relevant attributes
                            var = template.variables[name]
                            sliced_attr = {}
                            if var.attributes:
                                # For each key in var.attributes, pick the sub-value that matches this 'value'
                                # based on its position in var.values
                                value_index = var.values.index(value)
                                sliced_attr = {
                                    k: var.attributes[k][value_index]
                                    for k in var.attributes
                                    if isinstance(var.attributes[k], list)
                                    and len(var.attributes[k]) > value_index
                                }
                            var_attributes[name] = (
                                sliced_attr if sliced_attr else var.attributes
                            )

                        # Insert neutral variables
                        for name, value in zip(neutral_vars, neutral_combo):
                            marker = template.get_marker(name)
                            text = text.replace(marker, value)

                            # UPDATED: Only store relevant slice for neutral variables as well
                            var = template.variables[name]
                            sliced_attr = {}
                            if var.attributes:
                                value_index = var.values.index(value)
                                sliced_attr = {
                                    k: var.attributes[k][value_index]
                                    for k in var.attributes
                                    if isinstance(var.attributes[k], list)
                                    and len(var.attributes[k]) > value_index
                                }
                            var_attributes[name] = (
                                sliced_attr if sliced_attr else var.attributes
                            )

                        # Determine label if specified
                        label = 0
                        label_text = ""
                        if (
                            label_from_attributes
                            and label_map
                            and class_value is not None
                        ):
                            label = label_map[class_value]
                            label_text = class_value

                        # Create example
                        example = ProbingExample(
                            text=text,
                            label=label,
                            label_text=label_text,
                            attributes={
                                "template": template.template,
                                "variables": var_attributes,
                                "class": class_value,
                                **(template.attributes if template.attributes else {}),
                            },
                        )
                        examples.append(example)

        # Create dataset
        dataset = ProbingDataset(
            examples=examples,
            dataset_attributes=self.attributes,  # Renamed from metadata
        )

        # Add positions for each variable if requested
        if auto_add_positions:
            for template in self.templates:
                for var_name in template.variables:
                    finder = PositionFinder.from_template(
                        template=template.template, marker=template.get_marker(var_name)
                    )
                    dataset.add_target_positions(key=var_name, finder=finder)

        return dataset

    @classmethod
    def from_movie_sentiment_template(
        cls,
        adjectives: Dict[
            str, List[str]
        ],  # e.g., {"positive": [...], "negative": [...]}
        verbs: Dict[str, List[str]],  # e.g., {"positive": [...], "negative": [...]}
    ) -> "TemplatedDataset":
        """Create dataset from movie sentiment template.

        Args:
            adjectives: Dictionary mapping sentiment to list of adjectives
            verbs: Dictionary mapping sentiment to list of verbs
        """
        # Create variables
        adj_var = TemplateVariable(
            name="ADJ",
            values=[adj for lst in adjectives.values() for adj in lst],
            attributes={"sentiment": [k for k, v in adjectives.items() for _ in v]},
            class_bound=True,
            class_key="sentiment",
        )

        verb_var = TemplateVariable(
            name="VERB",
            values=[verb for lst in verbs.values() for verb in lst],
            attributes={"sentiment": [k for k, v in verbs.items() for _ in v]},
            class_bound=True,
            class_key="sentiment",
        )

        # Create template with exact spacing
        template = Template(
            template="I thought this movie was {ADJ}, I {VERB} it.",
            variables={"ADJ": adj_var, "VERB": verb_var},
            attributes={"task": "sentiment_classification"},
        )

        return cls(templates=[template])

    @classmethod
    def from_mood_story_template(
        cls,
        names: List[str],
        verbs: Dict[str, List[str]],  # e.g., {"positive": [...], "negative": [...]}
    ) -> "TemplatedDataset":
        """Create dataset from mood story template."""
        # Create variables
        name_var = TemplateVariable(name="NAME", values=names)

        verb_var = TemplateVariable(
            name="VERB",
            values=[v for lst in verbs.values() for v in lst],
            attributes={"sentiment": [k for k, v in verbs.items() for _ in v]},
        )

        # Create template
        template = Template(
            template="{NAME} {VERB} parties, and does so whenever possible.",
            variables={"NAME": name_var, "VERB": verb_var},
            attributes={"task": "mood_prediction"},
        )

        return cls(templates=[template])



================================================
FILE: probity/datasets/tokenized.py
================================================
from typing import Optional, List, Dict, Set, Union, cast, Any
import dataclasses
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from .position_finder import Position
from dataclasses import dataclass, field
import json
import torch
from datasets import Dataset
from .base import ProbingDataset, ProbingExample
from .position_finder import PositionFinder


@dataclass
class TokenizationConfig:
    """Stores information about how the dataset was tokenized."""

    tokenizer_name: str
    tokenizer_kwargs: Dict
    vocab_size: int
    pad_token_id: Optional[int]
    eos_token_id: Optional[int]
    bos_token_id: Optional[int] = None  # Add BOS token ID
    padding_side: str = "right"  # "right" or "left" padding


@dataclass
class TokenPositions:
    """Container for token positions of interest."""

    positions: Dict[str, Union[int, List[int]]]

    def __getitem__(self, key: str) -> Union[int, List[int]]:
        return self.positions[key]

    def keys(self) -> Set[str]:
        return set(self.positions.keys())


@dataclass
class TokenizedProbingExample(ProbingExample):
    """Extends ProbingExample with tokenization information."""

    tokens: List[int] = field(default_factory=list)
    attention_mask: Optional[List[int]] = None
    token_positions: Optional[TokenPositions] = None


class TokenizedProbingDataset(ProbingDataset):
    """ProbingDataset with tokenization information and guarantees."""

    def __init__(
        self,
        examples: List[TokenizedProbingExample],
        tokenization_config: TokenizationConfig,
        position_types: Optional[Set[str]] = None,
        **kwargs,
    ):
        # Call parent constructor with the examples
        super().__init__(examples=examples, **kwargs)  # type: ignore
        self.tokenization_config = tokenization_config
        self.position_types = position_types or set()

        # Validate that all examples are properly tokenized
        self._validate_tokenization()

    def __len__(self) -> int:
        """Return the number of examples in the dataset."""
        # Delegate to the underlying list of examples
        return len(self.examples)

    def _validate_tokenization(self):
        """Ensure all examples have valid tokens."""
        for example in self.examples:
            if not isinstance(example, TokenizedProbingExample):
                raise TypeError(
                    "All examples must be TokenizedProbingExample instances"
                )
            if any(t >= self.tokenization_config.vocab_size for t in example.tokens):
                raise ValueError(f"Invalid token ID found in example: {example.text}")

    def _to_hf_dataset(self) -> Dataset:
        """Override parent method to include tokenization data."""
        # Get base dictionary from parent
        data_dict = super()._to_hf_dataset().to_dict()

        # Add tokenization data
        examples = [cast(TokenizedProbingExample, ex) for ex in self.examples]
        data_dict["tokens"] = [ex.tokens for ex in examples]
        if all(ex.attention_mask is not None for ex in examples):
            data_dict["attention_mask"] = [ex.attention_mask for ex in examples]

        # Save token positions if they exist
        position_keys = set()
        for ex in examples:
            if ex.token_positions:
                position_keys.update(ex.token_positions.keys())

        # Add token position data columns
        for key in position_keys:
            data_dict[f"token_pos_{key}"] = []

        # Fill in token position data
        for ex in examples:
            for key in position_keys:
                if ex.token_positions and key in ex.token_positions.keys():
                    data_dict[f"token_pos_{key}"].append(ex.token_positions[key])
                else:
                    # Add a placeholder for missing positions
                    data_dict[f"token_pos_{key}"].append(None)

        return Dataset.from_dict(data_dict)

    @classmethod
    def from_probing_dataset(
        cls,
        dataset: ProbingDataset,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        add_special_tokens: bool = True,  # Explicitly control special tokens
        **tokenizer_kwargs,
    ) -> "TokenizedProbingDataset":
        """Create TokenizedProbingDataset from ProbingDataset."""
        tokenized_examples = []

        # Get the tokenizer's padding side
        padding_side = getattr(tokenizer, "padding_side", "right")
        is_left_padding = padding_side == "left"

        # Ensure add_special_tokens is in tokenizer_kwargs for consistency
        tokenizer_kwargs["add_special_tokens"] = add_special_tokens

        for example in dataset.examples:
            # First, tokenize without padding to get accurate token positions
            no_padding_kwargs = {
                k: v for k, v in tokenizer_kwargs.items() if k != "padding"
            }
            # Remove return_tensors if present, as we explicitly set it below
            no_padding_kwargs.pop("return_tensors", None)
            no_padding_kwargs["padding"] = False  # Explicitly set padding to False

            # Tokenize the text without padding but with special tokens if requested
            no_pad_tokens_info = tokenizer(
                example.text, return_tensors="pt", **no_padding_kwargs
            )
            no_pad_tokens = no_pad_tokens_info["input_ids"][0].tolist()

            # Now tokenize with all requested kwargs (may include padding)
            # Remove return_tensors if present, as we explicitly set it below
            full_tokenizer_kwargs = tokenizer_kwargs.copy()
            full_tokenizer_kwargs.pop("return_tensors", None)
            tokens_info = tokenizer(
                example.text, return_tensors="pt", **full_tokenizer_kwargs
            )
            padded_tokens = tokens_info["input_ids"][0].tolist()
            attention_mask = (
                tokens_info["attention_mask"][0].tolist()
                if "attention_mask" in tokens_info
                else None
            )

            # Find padding length if left padding is used
            pad_length = 0
            if is_left_padding and attention_mask:
                # Find where the content starts (first 1 in attention mask)
                for i, mask in enumerate(attention_mask):
                    if mask == 1:
                        pad_length = i
                        break

            # Convert character positions to token positions if they exist
            token_positions = None
            if example.character_positions:
                positions = {}
                for key, pos in example.character_positions.positions.items():
                    # Use the no-padding tokens for position conversion to get accurate positions
                    if isinstance(pos, Position):
                        # Convert single position relative to unpadded tokens
                        token_pos = PositionFinder.convert_to_token_position(
                            pos,
                            example.text,
                            tokenizer,
                            add_special_tokens=add_special_tokens,
                            padding_side=padding_side,
                        )

                        # If using left padding, the positions in padded tokens will be offset
                        if is_left_padding and pad_length > 0:
                            positions[key] = token_pos + pad_length
                        else:
                            positions[key] = token_pos

                    else:  # List[Position]
                        # Convert list of positions
                        token_positions = [
                            PositionFinder.convert_to_token_position(
                                p,
                                example.text,
                                tokenizer,
                                add_special_tokens=add_special_tokens,
                                padding_side=padding_side,
                            )
                            for p in pos
                        ]

                        # Adjust for left padding if needed
                        if is_left_padding and pad_length > 0:
                            positions[key] = [tp + pad_length for tp in token_positions]
                        else:
                            positions[key] = token_positions

                token_positions = TokenPositions(positions)

            # Create tokenized example with accurate token positions
            tokenized_example = TokenizedProbingExample(
                text=example.text,
                label=example.label,
                label_text=example.label_text,
                character_positions=example.character_positions,
                token_positions=token_positions,
                group_id=example.group_id,
                attributes=example.attributes,
                tokens=padded_tokens,  # Use the full tokens with padding
                attention_mask=attention_mask,
            )
            tokenized_examples.append(tokenized_example)

        # Create tokenization config with explicit record of special token handling
        tokenization_config = TokenizationConfig(
            tokenizer_name=tokenizer.name_or_path,
            tokenizer_kwargs=tokenizer_kwargs,
            vocab_size=tokenizer.vocab_size,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            bos_token_id=tokenizer.bos_token_id,
            padding_side=padding_side,
        )

        return cls(
            examples=tokenized_examples,
            tokenization_config=tokenization_config,
            task_type=dataset.task_type,
            valid_layers=dataset.valid_layers,
            label_mapping=dataset.label_mapping,
            position_types=dataset.position_types,
            dataset_attributes=dataset.dataset_attributes,
        )

    def get_token_lengths(self) -> List[int]:
        """Get length of each sequence in tokens."""
        return [len(cast(TokenizedProbingExample, ex).tokens) for ex in self.examples]

    def get_max_sequence_length(self) -> int:
        """Get maximum sequence length in dataset."""
        return max(self.get_token_lengths())

    def validate_positions(self) -> bool:
        """Check if all token positions are valid given the sequence lengths."""
        for example in self.examples:
            tokenized_ex = cast(TokenizedProbingExample, example)
            if tokenized_ex.token_positions is None:
                continue
            seq_len = len(tokenized_ex.tokens)

            # Skip validation for BOS token if it exists
            bos_token_id = self.tokenization_config.bos_token_id
            bos_offset = 0
            if (
                bos_token_id is not None
                and tokenized_ex.tokens
                and tokenized_ex.tokens[0] == bos_token_id
            ):
                bos_offset = 1  # Adjust sequence length to exclude BOS token

            # Note: For un-padded sequences, we don't need to adjust for padding side
            # since positions are stored relative to the unpadded sequence
            for key, pos in tokenized_ex.token_positions.positions.items():
                if isinstance(pos, int):
                    if pos < 0 or pos >= (seq_len - bos_offset):
                        return False
                elif isinstance(pos, list):
                    if any(p < 0 or p >= (seq_len - bos_offset) for p in pos):
                        return False
        return True

    def get_batch_tensors(
        self, indices: List[int], pad: bool = True
    ) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]:
        """Get a batch of examples as tensors.

        Args:
            indices: Which examples to include in batch
            pad: Whether to pad sequences to max length in batch

        Returns:
            Dictionary with keys 'input_ids', 'attention_mask', and 'positions'
        """
        # Get examples and cast them to TokenizedProbingExample
        examples = [cast(TokenizedProbingExample, self.examples[i]) for i in indices]

        # Get max length for this batch
        max_len = max(len(ex.tokens) for ex in examples)

        # Prepare tensors
        input_ids: List[List[int]] = []
        attention_mask: List[List[int]] = []
        positions: Dict[str, List[Union[int, List[int]]]] = {
            pt: [] for pt in self.position_types
        }

        # Check padding direction
        is_left_padding = self.tokenization_config.padding_side == "left"

        for ex in examples:
            seq_len = len(ex.tokens)
            padding_length = max_len - seq_len

            if pad:
                if is_left_padding:
                    # Left padding (add pad tokens at the beginning)
                    pad_token = self.tokenization_config.pad_token_id or 0
                    padded_tokens = [pad_token] * padding_length + ex.tokens
                    input_ids.append(padded_tokens)

                    # Left pad the attention mask
                    if ex.attention_mask is not None:
                        padded_mask = [0] * padding_length + ex.attention_mask
                        attention_mask.append(padded_mask)
                else:
                    # Right padding (add pad tokens at the end)
                    pad_token = self.tokenization_config.pad_token_id or 0
                    padded_tokens = ex.tokens + [pad_token] * padding_length
                    input_ids.append(padded_tokens)

                    # Right pad the attention mask
                    if ex.attention_mask is not None:
                        padded_mask = ex.attention_mask + [0] * padding_length
                        attention_mask.append(padded_mask)
            else:
                input_ids.append(ex.tokens)
                if ex.attention_mask is not None:
                    attention_mask.append(ex.attention_mask)

            # Add positions for each position type
            for pt in self.position_types:
                if ex.token_positions is not None and pt in ex.token_positions.keys():
                    token_pos = ex.token_positions[pt]

                    # For left padding, we need to adjust positions when padding
                    # This is critical because the token positions from PositionFinder.convert_to_token_position
                    # are based on unpadded tokens. When using left padding, all token indices need to be
                    # shifted by the padding length to account for pad tokens inserted at the beginning.
                    #
                    # Example:
                    # Original tokens: [BOS, token1, token2, token3]
                    # Original position: 1 (points to token1)
                    # After left padding with 2 tokens: [PAD, PAD, BOS, token1, token2, token3]
                    # Adjusted position: 3 (1 + padding_length of 2)
                    if pad and is_left_padding:
                        if isinstance(token_pos, int):
                            # Shift position by padding length
                            adjusted_pos = token_pos + padding_length
                            positions[pt].append(adjusted_pos)
                        else:  # List of positions
                            # Shift all positions by padding length
                            adjusted_pos = [pos + padding_length for pos in token_pos]
                            positions[pt].append(adjusted_pos)
                    else:
                        # For right padding or no padding, positions stay the same
                        # since right padding adds tokens at the end, which doesn't affect
                        # the indices of the existing tokens
                        positions[pt].append(token_pos)
                else:
                    # Use a default position when position type doesn't exist for this example
                    positions[pt].append(0)

        # Convert to tensors
        batch: Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]] = {
            "input_ids": torch.tensor(input_ids),
            "positions": {
                pt: torch.tensor(positions[pt]) for pt in self.position_types
            },
        }

        if attention_mask:
            batch["attention_mask"] = torch.tensor(attention_mask)

        return batch

    def save(self, path: str) -> None:
        """Override save to include tokenization info."""
        super().save(path)

        # Save tokenization info
        with open(f"{path}/tokenization_config.json", "w") as f:
            json.dump(dataclasses.asdict(self.tokenization_config), f)

    @classmethod
    def load(cls, path: str) -> "TokenizedProbingDataset":
        """Override load to include tokenization info."""
        # Load base dataset
        base_dataset = ProbingDataset.load(path)

        # Load tokenization info
        with open(f"{path}/tokenization_config.json", "r") as f:
            tokenization_config = TokenizationConfig(**json.load(f))

        # Convert examples to TokenizedProbingExamples
        tokenized_examples = []
        hf_dataset = Dataset.load_from_disk(f"{path}/hf_dataset")

        # Extract position information from the dataset
        position_types = base_dataset.position_types
        position_data = {}

        # Find token position columns if they exist
        token_position_columns = [
            col for col in hf_dataset.column_names if col.startswith("token_pos_")
        ]

        if token_position_columns:
            for col in token_position_columns:
                # Extract the position key from the column name
                key = col.replace("token_pos_", "")
                position_data[key] = hf_dataset[col]

        for i, (base_ex, hf_item) in enumerate(zip(base_dataset.examples, hf_dataset)):
            # Reconstruct token positions only if base example had character positions
            token_positions = None
            if base_ex.character_positions and position_types:
                positions = {}
                # Try to get from token_position columns if they exist
                if position_data:
                    for key in position_types:
                        # Only add if key exists for this example in the loaded data
                        if (
                            key in position_data
                            and i < len(position_data[key])
                            and position_data[key][i] is not None
                        ):
                            positions[key] = position_data[key][i]
                # If we don't have token positions but have character positions,
                # we may need to reconstruct them (or handle appropriately)
                # elif base_ex.character_positions:
                # pass # Reconstruction might be complex/require tokenizer

                # Only create TokenPositions if positions dict is not empty
                if positions:
                    token_positions = TokenPositions(positions)
            # If base_ex.character_positions was None, token_positions remains None

            tokenized_examples.append(
                TokenizedProbingExample(
                    text=base_ex.text,
                    label=base_ex.label,
                    label_text=base_ex.label_text,
                    character_positions=base_ex.character_positions,
                    token_positions=token_positions,
                    group_id=base_ex.group_id,
                    attributes=base_ex.attributes,
                    tokens=cast(Dict[str, Any], hf_item)["tokens"],
                    attention_mask=cast(Dict[str, Any], hf_item).get("attention_mask"),
                )
            )

        dataset = cls(
            examples=tokenized_examples,
            tokenization_config=tokenization_config,
            task_type=base_dataset.task_type,
            valid_layers=base_dataset.valid_layers,
            label_mapping=base_dataset.label_mapping,
            position_types=base_dataset.position_types,
            dataset_attributes=base_dataset.dataset_attributes,
        )

        # Validate that positions are still valid
        if not dataset.validate_positions():
            print(
                "Warning: Some token positions appear to be invalid. "
                + "Call verify_position_tokens to diagnose issues."
            )

        return dataset

    def verify_position_tokens(
        self,
        tokenizer: Optional[Union[PreTrainedTokenizer, PreTrainedTokenizerFast]] = None,
        position_key: Optional[str] = None,
    ) -> Dict[str, Dict[str, Any]]:
        """Verify that positions point to the expected tokens.

        Args:
            tokenizer: Optional tokenizer to decode tokens. If None, uses token IDs.
            position_key: Optional specific position key to verify. If None, verifies all.

        Returns:
            Dictionary mapping example indices to position verification results
        """
        results = {}

        for i, example in enumerate(self.examples):
            tokenized_ex = cast(TokenizedProbingExample, example)
            if tokenized_ex.token_positions is None:
                continue

            # Determine which position keys to check
            keys_to_check = (
                [position_key] if position_key else tokenized_ex.token_positions.keys()
            )
            example_results = {}

            # Get sequence length for validation
            seq_len = len(tokenized_ex.tokens)

            for key in keys_to_check:
                if key not in tokenized_ex.token_positions.keys():
                    continue

                # Get the position for this key
                pos = tokenized_ex.token_positions[key]

                # Handle both single positions and lists of positions
                positions = [pos] if isinstance(pos, int) else pos

                # Check all positions
                for j, position in enumerate(positions):
                    position_key = f"{key}_{j}" if isinstance(pos, list) else key

                    if 0 <= position < seq_len:
                        token_id = tokenized_ex.tokens[position]
                        token_text = (
                            tokenizer.decode([token_id])
                            if tokenizer
                            else f"ID:{token_id}"
                        )
                        example_results[position_key] = {
                            "position": position,
                            "token_id": token_id,
                            "token_text": token_text,
                        }
                    else:
                        example_results[position_key] = {
                            "position": position,
                            "error": f"Position out of bounds (0-{seq_len-1})",
                        }

            if example_results:
                results[i] = example_results

        return results

    def show_token_context(
        self,
        example_idx: int,
        position_key: str,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        context_size: int = 5,
    ) -> Dict[str, Any]:
        """Show the token context around the position of interest for debugging.

        Args:
            example_idx: Index of the example to examine
            position_key: Position key to check
            tokenizer: Tokenizer to decode tokens
            context_size: Number of tokens to show before and after the position

        Returns:
            Dictionary with context information
        """
        if example_idx >= len(self.examples):
            raise ValueError(f"Example index {example_idx} out of range")

        example = cast(TokenizedProbingExample, self.examples[example_idx])
        if (
            example.token_positions is None
            or position_key not in example.token_positions.keys()
        ):
            raise ValueError(
                f"Position key {position_key} not found in example {example_idx}"
            )

        padding_side = self.tokenization_config.padding_side
        is_left_padding = padding_side == "left"

        # Get the position directly from the TokenizedProbingExample
        # This position should already be calculated relative to the *padded* sequence
        # by from_probing_dataset or loaded correctly.
        pos_value = example.token_positions[position_key]

        # Determine the positions to analyze (could be single int or list of ints)
        if isinstance(pos_value, int):
            padded_positions = [pos_value]
        elif isinstance(pos_value, list):
            padded_positions = pos_value
        else:
            raise TypeError(
                f"Unexpected position type for key '{position_key}': {type(pos_value)}"
            )

        # Find the actual tokens (non-padding)
        pad_token_id = self.tokenization_config.pad_token_id or 0
        padding_length = 0
        real_tokens_start_index = 0
        if is_left_padding:
            for i, token_id in enumerate(example.tokens):
                if token_id != pad_token_id:
                    real_tokens_start_index = i
                    break
            else:  # Handle case where sequence is all padding
                real_tokens_start_index = len(example.tokens)
            padding_length = real_tokens_start_index

        real_tokens_end_index = len(example.tokens)
        if not is_left_padding:
            for i in range(len(example.tokens) - 1, -1, -1):
                if example.tokens[i] != pad_token_id:
                    real_tokens_end_index = i + 1
                    break
            else:  # Handle case where sequence is all padding
                real_tokens_end_index = 0

        real_tokens = example.tokens[real_tokens_start_index:real_tokens_end_index]

        # Get the full token list with indices for debugging
        full_token_mapping = [
            (i, token_id, tokenizer.decode([token_id]))
            for i, token_id in enumerate(example.tokens)
        ]

        results = []
        for i, position_in_padded_sequence in enumerate(padded_positions):

            # Calculate original position relative to real tokens
            original_position = -1  # Default if out of bounds
            if is_left_padding:
                if position_in_padded_sequence >= padding_length:
                    original_position = position_in_padded_sequence - padding_length
            else:  # Right padding or no padding
                if position_in_padded_sequence < real_tokens_end_index:
                    original_position = position_in_padded_sequence

            # Get context tokens (working with the full padded sequence)
            start = max(0, position_in_padded_sequence - context_size)
            end = min(
                len(example.tokens), position_in_padded_sequence + context_size + 1
            )

            context_tokens = example.tokens[start:end]
            context_texts = [tokenizer.decode([t]) for t in context_tokens]

            # Mark the target position
            rel_position = (
                position_in_padded_sequence - start
            )  # Position within the context window
            marked_context = context_texts.copy()
            if 0 <= rel_position < len(marked_context):
                marked_context[rel_position] = f"[[ {marked_context[rel_position]} ]]"

            # Get the token at the specified position
            token_info = None
            if 0 <= position_in_padded_sequence < len(example.tokens):
                token_id = example.tokens[position_in_padded_sequence]
                token_text = tokenizer.decode([token_id])
                token_info = {"id": token_id, "text": token_text}

            results.append(
                {
                    "position": position_in_padded_sequence,  # Position in padded sequence
                    "original_position": original_position,  # Calculated position relative to non-padded tokens
                    "token_info": token_info,
                    "context_window": {
                        "start": start,
                        "end": end,
                        "tokens": context_tokens,
                        "texts": context_texts,
                        "marked_context": " ".join(marked_context),
                    },
                    "padding_side": padding_side,
                    "real_tokens_length": len(real_tokens),
                    "padding_length": (
                        padding_length
                        if is_left_padding
                        else len(example.tokens) - real_tokens_end_index
                    ),
                }
            )

        return {
            "example_idx": example_idx,
            "position_key": position_key,
            "example_text": example.text,
            "token_count": len(example.tokens),
            "first_few_tokens": full_token_mapping[:5],
            "last_few_tokens": (
                full_token_mapping[-5:] if len(full_token_mapping) > 5 else []
            ),
            "target_positions": padded_positions,
            "padding_side": padding_side,
            "results": results[0] if len(results) == 1 else results,
        }

    def verify_padding(
        self,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        max_length: Optional[int] = None,
        examples_to_check: int = 3,
    ) -> Dict[str, Any]:
        """Verify token positions with explicit padding applied.

        This helper method applies padding explicitly and verifies that token positions
        are correctly aligned after padding, which is especially important for
        left-padding tokenizers.

        Args:
            tokenizer: Tokenizer to use for padding and decoding
            max_length: Maximum length for padding (if None, uses max sequence length)
            examples_to_check: Number of examples to check

        Returns:
            Dictionary with verification results
        """
        # Find the max length if not provided
        if max_length is None:
            max_length = self.get_max_sequence_length()

        # Get padding side
        padding_side = self.tokenization_config.padding_side
        is_left_padding = padding_side == "left"

        # Select a sample of examples to check
        indices = list(range(min(examples_to_check, len(self.examples))))

        # Get batch tensors with padding applied
        batch = self.get_batch_tensors(indices, pad=True)

        # DEBUG: Check type and content of batch
        print(f"DEBUG verify_padding: type(batch) = {type(batch)}, batch = {batch}")

        results = {}
        for i, idx in enumerate(indices):
            example = cast(TokenizedProbingExample, self.examples[idx])
            if example.token_positions is None:
                continue

            example_results = {}
            # Ensure batch["input_ids"] is a tensor before indexing
            input_ids_tensor = batch.get("input_ids")
            if not isinstance(input_ids_tensor, torch.Tensor):
                raise TypeError(
                    f"Expected 'input_ids' to be a Tensor, but got {type(input_ids_tensor)}"
                )
            if i >= input_ids_tensor.shape[0]:
                raise IndexError(
                    f"Index {i} out of bounds for input_ids tensor with shape {input_ids_tensor.shape}"
                )

            padded_tokens = input_ids_tensor[i].tolist()

            # Check each position type
            for key in self.position_types:
                if key not in example.token_positions.keys():
                    continue

                # Get original position from the example
                orig_pos = example.token_positions[key]

                # Get position from the batch (already adjusted for padding)
                try:
                    # Handle different possible formats of batch["positions"]
                    if (
                        isinstance(batch["positions"], dict)
                        and key in batch["positions"]
                    ):
                        # If positions is a dict with position_type keys
                        batch_pos_tensor = batch["positions"][key][i]
                    elif isinstance(batch["positions"], torch.Tensor):
                        # If positions is directly a tensor
                        batch_pos_tensor = batch["positions"][i]
                    else:
                        # Fallback
                        raise ValueError(
                            f"Unsupported format for batch positions: {type(batch['positions'])}"
                        )

                    # Convert tensor to list if needed
                    if isinstance(batch_pos_tensor, torch.Tensor):
                        batch_pos = batch_pos_tensor.tolist()
                        # Handle scalar tensor
                        if not isinstance(batch_pos, list):
                            batch_pos = [batch_pos]
                    else:
                        # Handle raw value
                        batch_pos = batch_pos_tensor
                        if not isinstance(batch_pos, list):
                            batch_pos = [batch_pos]
                except Exception as e:
                    # If we can't get batch_pos properly, use default position handling
                    padding_length = max_length - len(example.tokens)
                    if isinstance(orig_pos, int):
                        batch_pos = (
                            [orig_pos + padding_length]
                            if is_left_padding
                            else [orig_pos]
                        )
                    else:
                        batch_pos = (
                            [p + padding_length for p in orig_pos]
                            if is_left_padding
                            else orig_pos
                        )

                # Handle both single and list positions
                orig_positions = [orig_pos] if isinstance(orig_pos, int) else orig_pos

                position_results = []
                for j, orig in enumerate(orig_positions):
                    if j < len(batch_pos):
                        # Rename inner loop variable to avoid collision with outer 'batch' dict
                        current_batch_pos = batch_pos[j]
                        # Calculate expected position with padding
                        padding_length = max_length - len(example.tokens)
                        expected_pos = (
                            orig + padding_length if is_left_padding else orig
                        )

                        # Check if position matches expectation
                        position_match = expected_pos == current_batch_pos

                        # Get token at the position in padded sequence
                        try:
                            token_id = (
                                padded_tokens[current_batch_pos]
                                if 0 <= current_batch_pos < len(padded_tokens)
                                else None
                            )
                            token_text = (
                                tokenizer.decode([token_id])
                                if token_id is not None
                                else "OUT_OF_RANGE"
                            )
                        except Exception as e_decode:
                            token_text = f"ERROR: {str(e_decode)}"

                        position_results.append(
                            {
                                "original_position": orig,
                                "padded_position": current_batch_pos,
                                "expected_position": expected_pos,
                                "position_matches": position_match,
                                "token_text": token_text,
                                "padding_length": padding_length,
                            }
                        )

                example_results[key] = (
                    position_results[0]
                    if len(position_results) == 1
                    else position_results
                )

            results[idx] = {
                "text": example.text,
                "token_length": len(example.tokens),
                "padded_length": max_length,
                "padding_side": padding_side,
                "positions": example_results,
            }

        return {
            "padding_side": padding_side,
            "max_length": max_length,
            "examples": results,
        }



================================================
FILE: probity/pipeline/pipeline.py
================================================
from dataclasses import dataclass
from typing import List, Optional, Dict, Type, Generic, TypeVar, Any
import torch
from pathlib import Path

from probity.collection.collectors import (
    TransformerLensCollector,
    TransformerLensConfig,
)
from probity.collection.activation_store import ActivationStore
from probity.datasets.tokenized import TokenizedProbingDataset
from probity.probes import BaseProbe, ProbeConfig
from probity.training.trainer import BaseProbeTrainer, BaseTrainerConfig

C = TypeVar("C", bound=BaseTrainerConfig)
P = TypeVar("P", bound=ProbeConfig)


@dataclass
class ProbePipelineConfig:
    """Configuration for probe pipeline."""

    dataset: TokenizedProbingDataset
    probe_cls: Type[BaseProbe]
    probe_config: ProbeConfig
    trainer_cls: Type[BaseProbeTrainer]
    trainer_config: BaseTrainerConfig
    position_key: str
    cache_dir: Optional[str] = None
    model_name: Optional[str] = None
    hook_points: Optional[List[str]] = None
    activation_batch_size: int = 32
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


class ProbePipeline(Generic[C, P]):
    """Pipeline for end-to-end probe training including activation collection."""

    def __init__(self, config: ProbePipelineConfig):
        self.config = config
        self.activation_stores: Optional[Dict[str, ActivationStore]] = None
        self.probe: Optional[BaseProbe] = None

        # Synchronize device settings
        if hasattr(self.config.trainer_config, "device"):
            # If trainer has device setting, use it for pipeline
            self.config.device = self.config.trainer_config.device
        else:
            # Otherwise, update trainer and probe configs with pipeline's device
            self.config.trainer_config.device = self.config.device

        # Ensure probe config has the same device setting
        if hasattr(self.config.probe_config, "device"):
            self.config.probe_config.device = self.config.device

    def _collect_activations(self) -> Dict[str, ActivationStore]:
        """Collect activations if needed."""
        if not self.config.model_name or not self.config.hook_points:
            raise ValueError(
                "Model name and hook points required for activation collection"
            )

        self.collector = TransformerLensCollector(
            TransformerLensConfig(
                model_name=self.config.model_name,
                hook_points=self.config.hook_points,
                batch_size=self.config.activation_batch_size,
                device=self.config.device,
            )
        )

        return self.collector.collect(self.config.dataset)

    def _load_or_collect_activations(self) -> Dict[str, ActivationStore]:
        """Load activations from cache if available and valid, otherwise collect them."""
        if self.config.cache_dir:
            cache_path = Path(self.config.cache_dir)
            if cache_path.exists():
                stores = {}
                cache_valid = True

                # Try to load and validate each hook point
                for hook_point in self.config.hook_points or []:
                    store_path = cache_path / hook_point.replace(".", "_")
                    if store_path.exists():
                        try:
                            store = ActivationStore.load(str(store_path))
                            if self._validate_cache_compatibility(store, hook_point):
                                stores[hook_point] = store
                            else:
                                print(
                                    f"Cache for {hook_point} is incompatible with current configuration"
                                )
                                cache_valid = False
                                break
                        except Exception as e:
                            print(f"Error loading cache for {hook_point}: {e}")
                            cache_valid = False
                            break

                if cache_valid and stores:
                    return stores
                else:
                    # If cache is invalid, clear it
                    import shutil

                    shutil.rmtree(cache_path)
                    print("Cleared invalid cache directory")

        # If we get here, need to collect activations
        stores = self._collect_activations()

        # Cache if directory specified
        if self.config.cache_dir:
            cache_path = Path(self.config.cache_dir)
            cache_path.mkdir(parents=True, exist_ok=True)

            # Save configuration hash with the cache
            for hook_point, store in stores.items():
                store_path = cache_path / hook_point.replace(".", "_")
                store.save(str(store_path))

        return stores

    def _validate_cache_compatibility(
        self, store: ActivationStore, hook_point: str
    ) -> bool:
        """Validate if cached activation store matches current configuration.

        Args:
            store: Loaded activation store
            hook_point: Hook point being validated

        Returns:
            bool: Whether cache is compatible
        """
        # Check dataset compatibility
        if len(store.dataset.examples) != len(self.config.dataset.examples):
            return False

        # Check if tokenization configs match
        if not hasattr(store.dataset, "tokenization_config") or not hasattr(
            self.config.dataset, "tokenization_config"
        ):
            return False

        store_config = store.dataset.tokenization_config
        current_config = self.config.dataset.tokenization_config

        if store_config.tokenizer_name != current_config.tokenizer_name:
            return False

        # Check if position types match
        if store.dataset.position_types != self.config.dataset.position_types:
            return False

        # Check if model name matches
        if (
            hasattr(store, "model_name")
            and self.config.model_name
            and store.model_name != self.config.model_name
        ):
            return False

        return True

    def run(self, hook_point: Optional[str] = None) -> tuple[BaseProbe, Any]:
        """Run the full pipeline.

        Args:
            hook_point: Which activation layer to use. If not specified,
                      uses first available hook point.
        """
        # Get activations
        if self.activation_stores is None:
            self.activation_stores = self._load_or_collect_activations()

        if not hook_point:
            hook_point = list(self.activation_stores.keys())[0]

        if hook_point not in self.activation_stores:
            raise ValueError(f"Hook point {hook_point} not found in activation stores")

        # Initialize probe
        self.probe = self.config.probe_cls(self.config.probe_config)
        # Move probe to the specified device
        self.probe.to(self.config.device)

        # Initialize trainer
        trainer = self.config.trainer_cls(self.config.trainer_config)

        # Prepare data and train
        train_loader, val_loader = trainer.prepare_supervised_data(
            self.activation_stores[hook_point], self.config.position_key
        )

        history = trainer.train(self.probe, train_loader, val_loader)

        return self.probe, history

    @classmethod
    def load(cls, path: str) -> "ProbePipeline":
        """Load pipeline from saved state."""
        load_path = Path(path)

        # Load config
        config = torch.load(str(load_path / "config.pt"))

        # Create pipeline
        pipeline = cls(config)

        # Try loading vector format first, then fall back to regular probe
        probe_vector_path = load_path / "probe_vector.json"
        probe_path = load_path / "probe.pt"

        # Determine device
        device = (
            config.trainer_config.device if hasattr(config, "trainer_config") else "cpu"
        )

        if probe_vector_path.exists():
            try:
                # We don't need ProbeVector anymore, BaseProbe handles JSON loading
                # from probity.probes.probe_vector import ProbeVector
                probe_cls = config.probe_cls
                pipeline.probe = probe_cls.load_json(str(probe_vector_path))

                # Move to correct device
                pipeline.probe.to(device)
            except Exception as e:
                print(f"Error loading from ProbeVector format: {e}")
                # Fall back to regular probe if vector loading fails
                if probe_path.exists():
                    try:
                        pipeline.probe = config.probe_cls.load(str(probe_path))
                        pipeline.probe.to(device)
                    except Exception as e2:
                        print(f"Error loading probe from pt format: {e2}")
        elif probe_path.exists():
            try:
                pipeline.probe = config.probe_cls.load(str(probe_path))
                pipeline.probe.to(device)
            except Exception as e:
                print(f"Error loading probe: {e}")

        return pipeline

    def _get_cache_key(self) -> str:
        """Generate a unique key for the current configuration."""
        import hashlib
        import json

        # Collect relevant configuration details
        config_dict = {
            "model_name": self.config.model_name,
            "hook_points": self.config.hook_points,
            "position_key": self.config.position_key,
            "tokenizer_name": self.config.dataset.tokenization_config.tokenizer_name,
            "dataset_size": len(self.config.dataset.examples),
            "position_types": sorted(list(self.config.dataset.position_types)),
        }

        # Create deterministic string representation
        config_str = json.dumps(config_dict, sort_keys=True)

        # Generate hash
        return hashlib.md5(config_str.encode()).hexdigest()

    def _get_cache_path(self) -> Path:
        """Get cache path including configuration hash."""
        if not self.config.cache_dir:
            raise ValueError("Cache directory not specified")

        base_path = Path(self.config.cache_dir)
        config_hash = self._get_cache_key()
        return base_path / config_hash



================================================
FILE: probity/probes/__init__.py
================================================
# __init__.py for probity.probes

# Import probe configuration classes
from .config import (
    ProbeConfig,
    LinearProbeConfig,
    LogisticProbeConfig,
    MultiClassLogisticProbeConfig,
    KMeansProbeConfig,
    PCAProbeConfig,
    MeanDiffProbeConfig,
    LogisticProbeConfigBase,  # Base for sklearn
    SklearnLogisticProbeConfig,
)

# Import the base probe class
from .base import BaseProbe

# Import concrete probe implementations
from .linear import LinearProbe
from .logistic import LogisticProbe, MultiClassLogisticProbe
from .directional import (
    DirectionalProbe,  # Base for non-learned direction probes
    KMeansProbe,
    PCAProbe,
    MeanDifferenceProbe,
)
from .sklearn_logistic import SklearnLogisticProbe

# Import the ProbeSet class
from .probe_set import ProbeSet


# Define __all__ for explicit public API
__all__ = [
    # Configs
    "ProbeConfig",
    "LinearProbeConfig",
    "LogisticProbeConfig",
    "MultiClassLogisticProbeConfig",
    "KMeansProbeConfig",
    "PCAProbeConfig",
    "MeanDiffProbeConfig",
    "LogisticProbeConfigBase",
    "SklearnLogisticProbeConfig",
    # Base Class
    "BaseProbe",
    # Concrete Probes
    "LinearProbe",
    "LogisticProbe",
    "MultiClassLogisticProbe",
    "DirectionalProbe",
    "KMeansProbe",
    "PCAProbe",
    "MeanDifferenceProbe",
    "SklearnLogisticProbe",
    # Probe Collection
    "ProbeSet",
]



================================================
FILE: probity/probes/base.py
================================================
from abc import ABC, abstractmethod
import os
import torch
import torch.nn as nn
import json
from typing import Optional, Generic, TypeVar, TYPE_CHECKING, get_args, get_origin, Any
import importlib

# Use TYPE_CHECKING to avoid circular imports at runtime
if TYPE_CHECKING:
    from .config import (
        ProbeConfig,
        LinearProbeConfig,
        LogisticProbeConfig,
        MultiClassLogisticProbeConfig,
        KMeansProbeConfig,
        PCAProbeConfig,
        MeanDiffProbeConfig,
        SklearnLogisticProbeConfig,  # Assuming this might be needed
        LogisticProbeConfigBase,  # Add the base config too
    )

# Generic type variable bound by ProbeConfig
T = TypeVar("T", bound="ProbeConfig")


# Helper function to get attribute safely
def _get_config_attr(config, attr_name, default=None):
    return getattr(config, attr_name, default)


# Helper function to set attribute safely if it exists
def _set_config_attr(config, attr_name, value):
    if hasattr(config, attr_name):
        setattr(config, attr_name, value)


class BaseProbe(ABC, nn.Module, Generic[T]):
    """Abstract base class for probes. Probes store directions in the original activation space."""

    config: T  # Type hint for config instance

    def __init__(self, config: T):
        super().__init__()
        self.config = config
        self.dtype = (
            torch.float32
            if _get_config_attr(config, "dtype", "float32") == "float32"
            else torch.float16
        )
        self.name = (
            _get_config_attr(config, "name", "unnamed_probe") or "unnamed_probe"
        )  # Ensure name is not None
        # Standardization is handled by the trainer, not stored in the probe.

    @abstractmethod
    def get_direction(self, normalized: bool = True) -> torch.Tensor:
        """Get the learned probe direction in the original activation space.

        Args:
            normalized: Whether to normalize the direction vector to unit length.
                      The probe's internal configuration (`normalize_weights`)
                      also influences this. Normalization occurs only if
                      `normalized` is True AND `config.normalize_weights` is True.

        Returns:
            The processed (optionally normalized) direction vector
            representing the probe in the original activation space.
        """
        pass

    @abstractmethod
    def _get_raw_direction_representation(self) -> torch.Tensor:
        """Return the raw internal representation (weights/vector) before normalization."""
        pass

    @abstractmethod
    def _set_raw_direction_representation(self, vector: torch.Tensor) -> None:
        """Set the raw internal representation (weights/vector) from a (potentially adjusted) vector."""
        pass

    def encode(self, acts: torch.Tensor) -> torch.Tensor:
        """Compute dot product between activations and the probe direction."""
        # Ensure direction is normalized for consistent projection magnitude
        direction = self.get_direction(normalized=True)
        # Ensure consistent dtypes for einsum
        acts = acts.to(dtype=self.dtype)
        direction = direction.to(dtype=self.dtype)
        return torch.einsum("...d,d->...", acts, direction)

    def save(self, path: str) -> None:
        """Save probe state and config in a single .pt file."""
        save_dir = os.path.dirname(path)
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)

        config_dict = self.config.__dict__.copy()  # Work on a copy
        additional_info = config_dict.get("additional_info", {})

        # Clear previous standardization info if present in older saves
        additional_info.pop("is_standardized", None)
        additional_info.pop("feature_mean", None)
        additional_info.pop("feature_std", None)

        # Save bias info if relevant (e.g., for LinearProbe, LogisticProbe)
        # Check for linear layer and bias attribute
        has_bias_param = False
        if hasattr(self, "linear") and isinstance(
            self.linear, nn.Module
        ):  # Ensure self.linear exists and is a module
            if hasattr(self.linear, "bias") and self.linear.bias is not None:
                has_bias_param = True
        elif hasattr(self, "intercept_"):
            # Handle SklearnLogisticProbe case where bias is stored in intercept_
            has_bias_param = self.intercept_ is not None

        additional_info["has_bias"] = has_bias_param

        # Ensure config reflects runtime normalization choice if needed for reconstruction
        if hasattr(self.config, "normalize_weights"):
            additional_info["normalize_weights"] = _get_config_attr(
                self.config, "normalize_weights"
            )
        if hasattr(self.config, "bias"):
            additional_info["bias_config"] = _get_config_attr(self.config, "bias")

        config_dict["additional_info"] = additional_info

        # Re-create config object from dict to ensure it's serializable if it was a complex type initially
        # This step might not be strictly necessary if the config is always a simple dataclass
        # but adds robustness.
        config_to_save = type(self.config)(**config_dict)

        # Save full state
        torch.save(
            {
                "state_dict": self.state_dict(),
                "config": config_to_save,  # Save the potentially modified config
                "probe_type": self.__class__.__name__,
            },
            path,
        )

    @classmethod
    def load(cls, path: str, device: Optional[str] = None) -> "BaseProbe":
        """Load probe from saved state (.pt or .json file). Dynamically determines format."""
        if not os.path.exists(path):
            raise FileNotFoundError(f"No saved probe file found at {path}")

        if path.endswith(".json"):
            # Delegate to load_json if it's explicitly JSON
            return cls.load_json(path, device=device)
        else:
            # Assume .pt format otherwise
            map_location = device or ("cuda" if torch.cuda.is_available() else "cpu")
            data = torch.load(path, map_location=map_location, weights_only=False)

            saved_config = data["config"]
            probe_type_name = data.get("probe_type", cls.__name__)

            # Dynamically find the correct probe class using the saved type name
            probe_cls = cls._get_probe_class_by_name(probe_type_name)

            # Create probe instance using the loaded config
            # Ensure the config object is of the correct type expected by the probe_cls
            if not isinstance(saved_config, probe_cls.__orig_bases__[0].__args__[0]):
                print(
                    f"Warning: Loaded config type {type(saved_config)} might not match expected type {probe_cls.__orig_bases__[0].__args__[0]} for {probe_type_name}. Attempting to recreate."
                )
                try:
                    config_cls = probe_cls.__orig_bases__[0].__args__[0]
                    # Create a new config instance, transferring attributes
                    new_config = config_cls(
                        input_size=saved_config.input_size
                    )  # Start with mandatory fields
                    for k, v in saved_config.__dict__.items():
                        if hasattr(new_config, k):
                            setattr(new_config, k, v)
                        else:
                            # Store extra fields in additional_info if possible
                            if hasattr(new_config, "additional_info") and isinstance(
                                new_config.additional_info, dict
                            ):
                                new_config.additional_info[k] = v
                    saved_config = new_config
                except Exception as e:
                    print(
                        f"Error recreating config: {e}. Proceeding with loaded config."
                    )

            probe = probe_cls(saved_config)

            # Load the state dict (potentially strict=False if needed)
            try:
                probe.load_state_dict(data["state_dict"], strict=True)
            except RuntimeError as e:
                print(
                    f"Warning: Error loading state_dict strictly for {probe.name}: {e}. Trying non-strict loading."
                )
                probe.load_state_dict(data["state_dict"], strict=False)

            # Move probe to the final target device (config might specify one, load might specify another)
            final_device = map_location  # Use the device specified for loading
            probe.to(final_device)
            # Update config device to reflect actual location
            if hasattr(probe.config, "device"):
                probe.config.device = str(final_device)

            # Set the probe to evaluation mode
            probe.eval()

            return probe

    def save_json(self, path: str) -> None:
        """Save probe's internal direction and metadata as JSON."""
        if not path.endswith(".json"):
            path += ".json"

        save_dir = os.path.dirname(path)
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)

        # Get the internal representation (always in original activation space)
        try:
            vector = self._get_raw_direction_representation()
            if vector is None:
                raise ValueError("Raw direction representation is None.")
            vector_np = vector.detach().clone().cpu().numpy()
        except Exception as e:
            print(
                f"Error getting raw direction for {self.name}: {e}. Cannot save JSON."
            )
            return

        # Prepare metadata using helper
        metadata = self._prepare_metadata(vector_np)

        # Save as JSON
        save_data = {
            "vector": vector_np.tolist(),
            "metadata": metadata,
        }

        try:
            with open(path, "w") as f:
                json.dump(save_data, f, indent=2)
        except TypeError as e:
            print(f"Error serializing probe data to JSON for {self.name}: {e}")
            # Attempt to serialize with a fallback for non-serializable types
            try:
                import numpy as np

                def default_serializer(obj):
                    if isinstance(obj, np.ndarray):
                        return obj.tolist()
                    if isinstance(obj, torch.Tensor):
                        return obj.cpu().numpy().tolist()
                    # Add more types if needed
                    return str(obj)  # Fallback to string representation

                with open(path, "w") as f:
                    json.dump(save_data, f, indent=2, default=default_serializer)
                print("Successfully saved JSON with fallback serializer.")
            except Exception as final_e:
                print(
                    f"Fallback JSON serialization also failed for {self.name}: {final_e}"
                )

    def _prepare_metadata(self, vector_np: Any) -> dict:
        """Helper to prepare metadata dictionary for saving."""
        metadata = {
            "model_name": _get_config_attr(self.config, "model_name", "unknown_model"),
            "hook_point": _get_config_attr(self.config, "hook_point", "unknown_hook"),
            "hook_layer": _get_config_attr(self.config, "hook_layer", 0),
            "hook_head_index": _get_config_attr(self.config, "hook_head_index"),
            "name": self.name,
            "vector_dimension": (
                vector_np.shape[-1] if hasattr(vector_np, "shape") else None
            ),
            "probe_type": self.__class__.__name__,
            "dataset_path": _get_config_attr(self.config, "dataset_path"),
            "prepend_bos": _get_config_attr(self.config, "prepend_bos", True),
            "context_size": _get_config_attr(self.config, "context_size", 128),
            "dtype": _get_config_attr(self.config, "dtype", "float32"),
            "device": _get_config_attr(self.config, "device"),
        }

        # Add bias info if relevant
        bias_value = None
        has_bias_param = False
        if hasattr(self, "linear") and isinstance(
            self.linear, nn.Module
        ):  # Check linear layer first
            if hasattr(self.linear, "bias") and self.linear.bias is not None:
                bias_param = self.linear.bias
                if isinstance(bias_param, torch.Tensor):
                    bias_value = bias_param.data.detach().clone().cpu().numpy().tolist()
                    has_bias_param = True
        elif (
            hasattr(self, "intercept_") and self.intercept_ is not None
        ):  # Check sklearn intercept
            intercept_param = self.intercept_
            if isinstance(intercept_param, torch.Tensor):
                bias_value = (
                    intercept_param.data.detach().clone().cpu().numpy().tolist()
                )
                has_bias_param = True

        metadata["has_bias"] = has_bias_param
        if has_bias_param:
            metadata["bias"] = bias_value

        # Save relevant config flags needed for reconstruction
        config_flags_to_save = [
            "normalize_weights",
            "bias",
            "loss_type",  # Linear/Logistic
            "n_clusters",
            "n_init",
            "random_state",  # KMeans
            "n_components",  # PCA
            "standardize",
            "max_iter",
            "solver",  # SklearnLogistic
        ]
        for flag in config_flags_to_save:
            if hasattr(self.config, flag):
                metadata_key = (
                    "bias_config" if flag == "bias" else flag
                )  # Map 'bias' config attr to 'bias_config' metadata key
                metadata[metadata_key] = _get_config_attr(self.config, flag)

        # Add any other info from config.additional_info
        additional_info = _get_config_attr(self.config, "additional_info", {})
        if isinstance(additional_info, dict):
            metadata.update(additional_info)  # Merge additional info

        return metadata

    @classmethod
    def load_json(cls, path: str, device: Optional[str] = None) -> "BaseProbe":
        """Load probe from JSON file.

        Args:
            path: Path to the JSON file
            device: Optional device override. If None, uses device from metadata or default.

        Returns:
            Loaded probe instance
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"No saved probe JSON file found at {path}")

        with open(path, "r") as f:
            data = json.load(f)

        # Extract data
        vector_list = data.get("vector")
        metadata = data.get("metadata", {})
        if vector_list is None:
            raise ValueError(f"JSON file {path} missing 'vector' field.")

        # Determine device
        target_device_str = (
            device
            or metadata.get("device")
            or ("cuda" if torch.cuda.is_available() else "cpu")
        )
        target_device = torch.device(target_device_str)

        # Get probe type and dynamically load classes
        probe_type_name = metadata.get("probe_type", cls.__name__)
        probe_cls = cls._get_probe_class_by_name(probe_type_name)
        config_cls = cls._get_config_class_for_probe(probe_cls, probe_type_name)

        # Instantiate config
        dim = metadata.get("vector_dimension")
        if dim is None:
            # Attempt to infer dim from vector if not in metadata
            try:
                # Need to convert to tensor first to get shape
                temp_vector = torch.tensor(vector_list)
                dim = temp_vector.shape[-1]
                print(
                    f"Warning: vector_dimension not found in metadata, inferred as {dim} from vector shape."
                )
            except Exception as e:
                raise ValueError(
                    f"Could not determine vector dimension from metadata or vector in {path}: {e}"
                )

        # Create config instance, ensuring mandatory fields are present
        try:
            config = config_cls(input_size=dim)
        except TypeError as e:
            raise TypeError(
                f"Error instantiating config class {config_cls.__name__} with input_size={dim}. Is it the correct config class for {probe_type_name}? Error: {e}"
            )

        # Update config with metadata fields using helper
        cls._update_config_from_metadata(config, metadata, target_device_str)

        # Create the probe instance with the configured settings
        probe = probe_cls(config)
        probe.to(target_device)  # Move to target device early

        # Convert vector list to tensor and set representation
        try:
            vector_tensor = torch.tensor(vector_list, dtype=probe.dtype).to(
                target_device
            )
            probe._set_raw_direction_representation(vector_tensor)
        except Exception as e:
            raise ValueError(f"Error processing 'vector' data from {path}: {e}")

        # Restore bias/intercept if it exists in metadata
        cls._restore_bias_intercept(probe, metadata, target_device)

        # Set to evaluation mode
        probe.eval()

        return probe

    @classmethod
    def _get_probe_class_by_name(cls, probe_type_name: str) -> type["BaseProbe"]:
        """Dynamically imports and returns the probe class."""
        try:
            # Heuristic: module name is lowercase probe type name without 'probe'
            module_name_part = probe_type_name.lower().replace("probe", "")
            if not module_name_part:  # Handle case like 'Probe' -> ''
                raise ImportError("Could not determine module name part.")
            # Special case mapping
            if "sklearn" in module_name_part:
                module_name_part = "sklearn_logistic"
            elif module_name_part == "linear":
                module_name_part = "linear"
            elif (
                module_name_part == "logistic"
                or module_name_part == "multiclasslogistic"
            ):
                module_name_part = "logistic"
            elif module_name_part in ["kmeans", "pca", "meandifference", "directional"]:
                module_name_part = "directional"
            # Add more specific mappings if needed

            package_name = "probity.probes"
            module_full_name = f"{package_name}.{module_name_part}"
            probe_module = importlib.import_module(module_full_name)
            probe_cls = getattr(probe_module, probe_type_name)

            if not issubclass(probe_cls, BaseProbe):
                raise TypeError(
                    f"{probe_type_name} found in {module_full_name} is not a subclass of BaseProbe"
                )
            return probe_cls
        except (ImportError, AttributeError, TypeError, ValueError) as e:
            print(
                f"Warning: Could not dynamically load probe class {probe_type_name} from module {module_full_name if 'module_full_name' in locals() else 'unknown'}. Error: {e}. Falling back to {cls.__name__}."
            )
            # Ensure the fallback class is actually a BaseProbe subclass
            if issubclass(cls, BaseProbe):
                return cls
            else:
                raise ImportError(
                    f"Fallback class {cls.__name__} is not a valid BaseProbe subclass."
                )

    @classmethod
    def _get_config_class_for_probe(
        cls, probe_cls: type["BaseProbe"], probe_type_name: str
    ) -> type["ProbeConfig"]:
        """Finds the corresponding config class for a given probe class."""
        config_cls = None
        # 1. Try direct name convention
        config_cls_name = f"{probe_type_name}Config"
        try:
            config_module = importlib.import_module(".config", package="probity.probes")
            config_cls = getattr(config_module, config_cls_name)
            if issubclass(config_cls, ProbeConfig):
                return config_cls
        except (ImportError, AttributeError):
            pass  # Continue trying other methods

        # 2. Try inferring from probe class Generic hint
        try:
            # Iterate through original bases to find the BaseProbe generic definition
            for base in getattr(probe_cls, "__orig_bases__", []):
                if get_origin(base) is BaseProbe:
                    config_arg = get_args(base)[0]
                    # Resolve forward references if necessary
                    if isinstance(config_arg, TypeVar):
                        # This might be harder to resolve robustly here
                        pass
                    elif isinstance(config_arg, str):
                        # Attempt to evaluate the forward reference string
                        try:
                            config_module = importlib.import_module(
                                ".config", package="probity.probes"
                            )
                            config_cls = eval(
                                config_arg, globals(), config_module.__dict__
                            )
                        except NameError:
                            pass  # Could not resolve forward ref
                    elif isinstance(config_arg, type) and issubclass(
                        config_arg, ProbeConfig
                    ):
                        config_cls = config_arg

                    if config_cls and issubclass(config_cls, ProbeConfig):
                        return config_cls
                    break  # Found BaseProbe base, stop searching bases
        except Exception as e:
            print(
                f"Warning: Exception while inferring config type from Generic hint for {probe_type_name}: {e}"
            )

        # 3. Fallback to base ProbeConfig
        print(
            f"Warning: Could not determine specific config class for {probe_type_name}. Using base ProbeConfig."
        )
        try:
            config_module = importlib.import_module(".config", package="probity.probes")
            return getattr(config_module, "ProbeConfig")
        except (ImportError, AttributeError):
            raise ImportError(
                "Fatal: Could not load even the base ProbeConfig class from probity.probes.config"
            )

    @classmethod
    def _update_config_from_metadata(
        cls, config: "ProbeConfig", metadata: dict, target_device_str: str
    ) -> None:
        """Populates the config object with values from the metadata dict."""
        # Update common fields
        common_fields = [
            "model_name",
            "hook_point",
            "hook_layer",
            "hook_head_index",
            "name",
            "dataset_path",
            "prepend_bos",
            "context_size",
            "dtype",
        ]
        for key in common_fields:
            if key in metadata:
                _set_config_attr(config, key, metadata[key])

        # Set device separately
        _set_config_attr(config, "device", target_device_str)

        # Update probe-specific config fields from metadata
        # Use all remaining metadata keys, mapping 'bias_config' back to 'bias'
        specific_metadata_keys = (
            set(metadata.keys())
            - set(common_fields)
            - {"device", "probe_type", "vector_dimension", "has_bias", "bias"}
        )

        for key in specific_metadata_keys:
            config_key = "bias" if key == "bias_config" else key
            _set_config_attr(config, config_key, metadata[key])

        # Ensure additional_info exists if needed
        if not hasattr(config, "additional_info") or config.additional_info is None:
            if isinstance(
                config, ProbeConfig
            ):  # Check if it's a base ProbeConfig or subclass
                config.additional_info = {}

        # Put known bias info into additional_info for consistency (optional)
        if (
            "has_bias" in metadata
            and hasattr(config, "additional_info")
            and isinstance(config.additional_info, dict)
        ):
            config.additional_info["has_bias"] = metadata["has_bias"]

    @classmethod
    def _restore_bias_intercept(
        cls, probe: "BaseProbe", metadata: dict, target_device: torch.device
    ) -> None:
        """Restores bias/intercept from metadata if available."""
        if metadata.get("has_bias", False) and "bias" in metadata:
            bias_or_intercept_data = metadata["bias"]
            if bias_or_intercept_data is None:
                print(
                    f"Warning: 'has_bias' is true but 'bias' data is null in metadata for {probe.name}."
                )
                return

            try:
                tensor_data = torch.tensor(
                    bias_or_intercept_data, dtype=probe.dtype
                ).to(target_device)
            except Exception as e:
                print(
                    f"Warning: Could not convert bias/intercept metadata to tensor for {probe.name}: {e}"
                )
                return

            restored = False
            # Try restoring to linear layer bias first
            if (
                hasattr(probe, "linear")
                and isinstance(probe.linear, nn.Module)
                and hasattr(probe.linear, "bias")
            ):
                if probe.linear.bias is not None:
                    with torch.no_grad():
                        try:
                            # Ensure shapes match or attempt reshape
                            if tensor_data.shape == probe.linear.bias.shape:
                                probe.linear.bias.copy_(tensor_data)
                                restored = True
                            else:
                                print(
                                    f"Warning: Bias shape mismatch during load. Metadata: {tensor_data.shape}, Probe: {probe.linear.bias.shape}. Attempting reshape."
                                )
                                probe.linear.bias.copy_(
                                    tensor_data.reshape(probe.linear.bias.shape)
                                )
                                restored = True
                        except Exception as e:
                            print(
                                f"Warning: Could not copy bias data to linear layer for {probe.name}: {e}"
                            )
                else:
                    print(
                        f"Warning: Bias metadata found for {probe.name}, but probe.linear.bias is None."
                    )

            # If not restored, try restoring to intercept_ buffer (for SklearnLogisticProbe)
            if not restored and hasattr(probe, "intercept_"):
                with torch.no_grad():
                    try:
                        if probe.intercept_ is not None:
                            # Check shape before copying
                            if tensor_data.shape == probe.intercept_.shape:
                                probe.intercept_.copy_(tensor_data)
                                restored = True
                            else:
                                # Handle potential shape mismatch for intercept (e.g., [1] vs [])
                                print(
                                    f"Warning: Intercept shape mismatch. Metadata: {tensor_data.shape}, Probe: {probe.intercept_.shape}. Attempting reshape."
                                )
                                probe.intercept_.copy_(
                                    tensor_data.reshape(probe.intercept_.shape)
                                )
                                restored = True
                        else:
                            # Intercept buffer might not exist yet, create it
                            print(
                                f"Warning: Intercept buffer 'intercept_' was None for {probe.name}. Creating buffer from metadata."
                            )
                            # setattr directly might not work for buffers, use register_buffer
                            probe.register_buffer("intercept_", tensor_data.clone())
                            restored = True
                    except Exception as e:
                        print(
                            f"Warning: Could not copy/register intercept data for {probe.name}: {e}"
                        )

            if not restored:
                print(
                    f"Warning: Bias/intercept metadata was present for {probe.name} but could not be restored to either linear.bias or intercept_."
                )



================================================
FILE: probity/probes/config.py
================================================
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, Literal
import torch


@dataclass
class ProbeConfig:
    """Base configuration for probes with metadata."""

    # Core configuration
    input_size: int
    device: str = field(
        default_factory=lambda: "cuda" if torch.cuda.is_available() else "cpu"
    )

    # Metadata fields (from former ProbeVector)
    model_name: str = "unknown_model"
    hook_point: str = "unknown_hook"
    hook_layer: int = 0
    hook_head_index: Optional[int] = None
    name: str = "unnamed_probe"

    # Dataset information
    dataset_path: Optional[str] = None
    prepend_bos: bool = True
    context_size: int = 128

    # Technical settings
    dtype: str = "float32"

    # Additional metadata
    additional_info: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LinearProbeConfig(ProbeConfig):
    """Configuration for linear probe."""

    loss_type: Literal["mse", "cosine", "l1"] = "mse"
    normalize_weights: bool = True
    bias: bool = False
    output_size: int = 1  # Number of output dimensions


@dataclass
class LogisticProbeConfig(ProbeConfig):
    """Configuration for logistic regression probe."""

    normalize_weights: bool = True
    bias: bool = True
    output_size: int = 1  # Number of output dimensions


@dataclass
class MultiClassLogisticProbeConfig(ProbeConfig):
    """Configuration for multi-class logistic regression probe."""

    output_size: int = 2  # Must be specified, > 1
    normalize_weights: bool = True
    bias: bool = True


@dataclass
class KMeansProbeConfig(ProbeConfig):
    """Configuration for K-means clustering probe."""

    n_clusters: int = 2
    n_init: int = 10
    normalize_weights: bool = True
    random_state: int = 42


@dataclass
class PCAProbeConfig(ProbeConfig):
    """Configuration for PCA-based probe."""

    n_components: int = 1
    normalize_weights: bool = True


@dataclass
class MeanDiffProbeConfig(ProbeConfig):
    """Configuration for mean difference probe."""

    normalize_weights: bool = True


# Configs for SklearnLogisticProbe
@dataclass
class LogisticProbeConfigBase(ProbeConfig):
    """Base config shared by sklearn implementations."""

    standardize: bool = True  # Internal standardization for sklearn
    normalize_weights: bool = True
    bias: bool = True
    output_size: int = 1  # Usually 1 for logistic


@dataclass
class SklearnLogisticProbeConfig(LogisticProbeConfigBase):
    """Config for sklearn-based probe."""

    max_iter: int = 100
    random_state: int = 42
    solver: str = "lbfgs"  # Example of adding solver



================================================
FILE: probity/probes/directional.py
================================================
from abc import abstractmethod
import torch
import numpy as np
from typing import Optional, Generic, TypeVar
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

from .base import BaseProbe, T, _get_config_attr  # Re-use T and helper from base
from .config import KMeansProbeConfig, PCAProbeConfig, MeanDiffProbeConfig


class DirectionalProbe(BaseProbe[T]):
    """Base class for probes computing direction directly (KMeans, PCA, MeanDiff).
    Stores the final direction in the original activation space.
    """

    # Explicitly type hint the buffer
    direction_vector: torch.Tensor

    def __init__(self, config: T):
        super().__init__(config)
        # Stores the final direction (in original activation space)
        # Initialize buffer with zeros matching input size and dtype
        input_size = _get_config_attr(config, "input_size")
        if input_size is None:
            raise ValueError("Config must have input_size for DirectionalProbe")
        self.register_buffer(
            "direction_vector",
            torch.zeros(input_size, dtype=self.dtype),
            persistent=True,
        )
        # Flag to indicate if fit has been run and direction is valid
        self.has_fit: bool = False

    @abstractmethod
    def fit(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Fit the probe (e.g., run KMeans/PCA) and compute the initial direction.
        The input x may be standardized by the trainer.

        Returns:
            The computed direction tensor *before* potential unscaling by the trainer.
            The internal `direction_vector` buffer should NOT be set here.
        """
        pass

    def _get_raw_direction_representation(self) -> torch.Tensor:
        """Return the computed final internal direction (in original activation space)."""
        # Check if the buffer has been initialized and fit has run
        if (
            not hasattr(self, "direction_vector")
            or self.direction_vector is None
            or not self.has_fit
        ):
            print(
                f"Warning: Accessing direction for probe {self.name} before fit() or after loading incompatible state. Returning zero vector."
            )
            # Return a zero vector of the correct shape and device
            input_size = _get_config_attr(self.config, "input_size")
            device = _get_config_attr(self.config, "device")
            if input_size is None:
                # Should not happen if constructor succeeded, but handle defensively
                raise ValueError("Cannot determine input_size to create zero vector.")
            return torch.zeros(input_size, dtype=self.dtype, device=device)
        return self.direction_vector

    def _set_raw_direction_representation(self, vector: torch.Tensor) -> None:
        """Set the final internal direction (in original activation space)."""
        input_size = _get_config_attr(self.config, "input_size")
        device = _get_config_attr(self.config, "device")
        if input_size is None or device is None:
            raise ValueError(
                "Config missing input_size or device for setting direction."
            )

        # Check if buffer exists and shape mismatch
        if hasattr(self, "direction_vector") and self.direction_vector is not None:
            if self.direction_vector.shape != vector.shape:
                # Allow setting if current vector is the initial zeros and target shape matches config
                is_zero_buffer = torch.all(self.direction_vector == 0)
                matches_config_shape = (
                    vector.ndim == 1 and vector.shape[0] == input_size
                )

                if is_zero_buffer and matches_config_shape:
                    print(f"Info: Initializing direction vector for {self.name}.")
                else:
                    raise ValueError(
                        f"Shape mismatch loading vector for {self.name}. Probe direction: {self.direction_vector.shape}, Loaded vector: {vector.shape}"
                    )

        # Ensure the vector is on the correct device and dtype
        vector = vector.to(device=device, dtype=self.dtype)

        # Register or update the buffer
        if not hasattr(self, "direction_vector") or self.direction_vector is None:
            # Register the buffer if it doesn't exist or is None
            self.register_buffer("direction_vector", vector.clone())
        else:
            # Use copy_ for in-place update of existing buffer
            with torch.no_grad():
                self.direction_vector.copy_(vector)

        # Mark that the direction is now set (either by loading or fitting)
        self.has_fit = True

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Project input onto the final (normalized) direction. Input x is in original activation space."""
        # Standardization is handled by the trainer externally if needed

        # Get the final interpretable direction (normalized by default)
        direction = self.get_direction(normalized=True)

        # Ensure consistent dtypes for matmul
        x = x.to(dtype=self.dtype)
        direction = direction.to(dtype=self.dtype)

        # Project onto the direction
        # Handle potential dimension mismatch (e.g., x: [B, D], direction: [D])
        if direction.dim() == 1 and x.dim() >= 2:
            # Standard case: project batch onto single vector -> [B]
            return torch.matmul(x, direction)
        elif direction.dim() == x.dim() and direction.shape[-1] == x.shape[-1]:
            # If direction has batch dim matching x, do batch matmul or elementwise product and sum
            # This case is less common for typical directional probes
            # Assuming elementwise product and sum over last dim: [B, D] * [B, D] -> [B]
            return torch.sum(x * direction, dim=-1)
        else:
            # Fallback or error for unexpected shapes
            try:
                # Attempt standard matmul, let PyTorch handle errors
                return torch.matmul(x, direction)
            except RuntimeError as e:
                raise RuntimeError(
                    f"Shape mismatch during forward pass for {self.name}. Input: {x.shape}, Direction: {direction.shape}. Original error: {e}"
                )

    def get_direction(self, normalized: bool = True) -> torch.Tensor:
        """Get the computed direction (already in original activation space), applying normalization."""
        direction = self._get_raw_direction_representation().clone()

        # Normalize if requested and configured
        should_normalize = normalized and getattr(
            self.config, "normalize_weights", False
        )
        if should_normalize:
            norm = torch.norm(direction)
            # Avoid division by zero if norm is very small
            if norm > 1e-8:
                direction = direction / norm
            else:
                # If norm is zero (or close), return the zero vector
                direction = torch.zeros_like(direction)

        return direction


class KMeansProbe(DirectionalProbe[KMeansProbeConfig]):
    """K-means clustering based probe."""

    def __init__(self, config: KMeansProbeConfig):
        super().__init__(config)
        # Sklearn model stored internally, not part of state_dict
        self.kmeans_model: Optional[KMeans] = None

    def fit(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Fit K-means and compute direction from centroids.
        Input x may be standardized by the trainer.
        Returns the computed direction tensor *before* potential unscaling.
        """
        if y is None:
            raise ValueError(
                "KMeansProbe requires labels (y) to determine centroid polarity."
            )

        # K-means expects float32
        x_np = x.cpu().numpy().astype(np.float32)
        y_np = y.cpu().numpy()

        self.kmeans_model = KMeans(
            n_clusters=self.config.n_clusters,
            n_init=self.config.n_init,
            random_state=self.config.random_state,
            init="k-means++",  # Specify init strategy
        )

        # Fit K-means
        try:
            cluster_assignments = self.kmeans_model.fit_predict(x_np)
            centroids = self.kmeans_model.cluster_centers_  # Shape: [n_clusters, dim]
        except Exception as e:
            raise RuntimeError(f"Sklearn KMeans fitting failed: {e}")

        # Determine positive and negative centroids based on label correlation
        # Ensure y_np is 1D
        if y_np.ndim > 1:
            y_np = y_np.squeeze()

        # Handle case where a cluster might be empty (highly unlikely with k-means++)
        cluster_labels_mean = np.full(
            self.config.n_clusters, np.nan
        )  # Initialize with NaN
        for i in range(self.config.n_clusters):
            mask = cluster_assignments == i
            if np.any(mask):
                # Calculate mean label only for assigned points
                cluster_labels_mean[i] = np.mean(y_np[mask])
            # No else needed, remains NaN if empty

        # Check for NaN means (empty clusters) before argmax/argmin
        if np.isnan(cluster_labels_mean).all():
            raise ValueError(
                "All K-means clusters were empty or could not calculate mean labels."
            )
        elif np.isnan(cluster_labels_mean).any():
            print("Warning: One or more K-means clusters were empty.")
            # Fallback logic might be needed here if empty clusters are problematic

        # Find centroids most correlated with positive (1) and negative (0) labels
        # Use nanargmax/nanargmin to handle potential empty clusters gracefully
        try:
            pos_centroid_idx = np.nanargmax(cluster_labels_mean)
            neg_centroid_idx = np.nanargmin(cluster_labels_mean)
        except ValueError:
            # This happens if all means are NaN
            raise ValueError(
                "Could not determine positive/negative centroids due to all NaN means (likely all clusters empty)."
            )

        # Check if the same cluster was chosen for both (e.g., only one non-empty cluster)
        if pos_centroid_idx == neg_centroid_idx:
            print(
                "Warning: Could not distinguish positive/negative K-means centroids based on labels (e.g., only one cluster had points or means were equal)."
            )
            # Fallback: Use first two centroids if available and distinct?
            if self.config.n_clusters >= 2 and len(np.unique(cluster_assignments)) >= 2:
                # Find the two clusters with the most points maybe?
                unique_clusters, counts = np.unique(
                    cluster_assignments, return_counts=True
                )
                if len(unique_clusters) >= 2:
                    sorted_indices = np.argsort(counts)[::-1]
                    # Try using the two largest clusters
                    idx1, idx2 = (
                        unique_clusters[sorted_indices[0]],
                        unique_clusters[sorted_indices[1]],
                    )
                    # Arbitrarily assign pos/neg, or maybe based on their (potentially equal) means?
                    print(f"Fallback: Using clusters {idx1} and {idx2} based on size.")
                    # Re-calculate difference based on these indices
                    pos_centroid = centroids[idx1]
                    neg_centroid = centroids[idx2]
                    initial_direction_np = pos_centroid - neg_centroid
                else:  # Only one cluster actually had points
                    raise ValueError(
                        "Cannot compute difference: Only one K-means cluster contained data points."
                    )
            else:
                raise ValueError(
                    f"Cannot compute difference: Only {len(np.unique(cluster_assignments))} K-means cluster(s) found with data, or means were indistinguishable."
                )
        else:
            pos_centroid = centroids[pos_centroid_idx]
            neg_centroid = centroids[neg_centroid_idx]
            # Direction is from negative to positive centroid
            initial_direction_np = pos_centroid - neg_centroid

        # Return the initial direction tensor (trainer will handle unscaling and setting)
        initial_direction_tensor = torch.tensor(
            initial_direction_np, device=self.config.device, dtype=self.dtype
        )

        # Mark fit as having run (direction_vector buffer is set by trainer later)
        # self.has_fit = True # DO NOT SET here, trainer does after unscaling
        return initial_direction_tensor


class PCAProbe(DirectionalProbe[PCAProbeConfig]):
    """PCA-based probe."""

    def __init__(self, config: PCAProbeConfig):
        super().__init__(config)
        self.pca_model: Optional[PCA] = None  # Store sklearn model if needed later

    def fit(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Fit PCA and determine direction sign based on correlation with labels.
        Input x may be standardized by the trainer.
        Returns the computed direction tensor *before* potential unscaling.
        """
        # PCA works best with float32 or float64
        x_np = x.cpu().numpy().astype(np.float32)

        # Ensure n_components is valid
        n_samples, n_features = x_np.shape
        n_components = min(self.config.n_components, n_samples, n_features)
        if n_components != self.config.n_components:
            print(
                f"Warning: n_components reduced from {self.config.n_components} to {n_components} due to data shape {x_np.shape}"
            )

        if n_components == 0:
            raise ValueError(
                f"Cannot perform PCA with 0 components (data shape {x_np.shape})."
            )

        self.pca_model = PCA(n_components=n_components)

        # Fit PCA
        try:
            self.pca_model.fit(x_np)
            # Components are rows in pca_model.components_
            # Shape: [n_components, dim]
            components = self.pca_model.components_
        except Exception as e:
            raise RuntimeError(f"Sklearn PCA fitting failed: {e}")

        # Get the first principal component
        pc1 = components[0]  # Shape: [dim]

        # Determine sign based on correlation with labels if provided
        if y is not None:
            y_np = y.cpu().numpy()
            if y_np.ndim > 1:
                y_np = y_np.squeeze()

            # Ensure y_np has the same number of samples as x_np
            if len(y_np) != n_samples:
                raise ValueError(
                    f"PCA fit input x ({n_samples} samples) and labels y ({len(y_np)} samples) have different lengths."
                )

            # Project potentially standardized data onto the first component
            projections = np.dot(x_np, pc1.T)  # Shape: [batch]

            # Calculate correlation between projections and labels
            try:
                # Ensure labels are numeric for correlation
                correlation = np.corrcoef(projections, y_np.astype(float))[0, 1]
                # Flip component sign if correlation is negative
                sign = np.sign(correlation) if not np.isnan(correlation) else 1.0
                pc1 = pc1 * sign
            except ValueError as e:
                print(
                    f"Warning: Could not calculate correlation between PCA projections and labels: {e}. Using original PC sign."
                )
            except IndexError:
                print(
                    "Warning: Could not extract correlation coefficient. Using original PC sign."
                )

        # Return the initial direction (first PC, potentially sign-corrected)
        initial_direction_tensor = torch.tensor(
            pc1, device=self.config.device, dtype=self.dtype
        )

        # Mark fit as having run (direction_vector buffer is set by trainer later)
        # self.has_fit = True # DO NOT SET here
        return initial_direction_tensor


class MeanDifferenceProbe(DirectionalProbe[MeanDiffProbeConfig]):
    """Probe finding direction through mean difference between classes."""

    def fit(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Compute direction as difference between class means.
        Input x may be standardized by the trainer.
        Returns the computed direction tensor *before* potential unscaling.
        """
        if y is None:
            raise ValueError("MeanDifferenceProbe requires labels (y).")

        # Ensure consistent dtypes and device
        x = x.to(dtype=self.dtype, device=self.config.device)
        y = y.to(device=self.config.device)  # Let mask handle dtype comparison

        # Calculate means for positive (1) and negative (0) classes
        # Ensure y is boolean or integer {0, 1} for masking
        pos_mask = (y == 1).squeeze()
        neg_mask = (y == 0).squeeze()

        # Check if masks are valid
        if not torch.any(pos_mask):
            raise ValueError(
                "MeanDifferenceProbe requires data for the positive class (label 1)."
            )
        if not torch.any(neg_mask):
            raise ValueError(
                "MeanDifferenceProbe requires data for the negative class (label 0)."
            )

        # Filter data and compute means
        pos_data = x[pos_mask]
        neg_data = x[neg_mask]

        # Ensure data is not empty before computing mean
        if pos_data.shape[0] == 0 or neg_data.shape[0] == 0:
            # This should be caught by torch.any checks above, but double-check
            raise ValueError("One class has no data after masking.")

        pos_mean = pos_data.mean(dim=0)
        neg_mean = neg_data.mean(dim=0)

        # Direction from negative to positive mean
        # This initial direction is potentially in the standardized space
        initial_direction_tensor = pos_mean - neg_mean

        # Return the initial direction
        # self.has_fit = True # DO NOT SET here
        return initial_direction_tensor



================================================
FILE: probity/probes/inference.py
================================================
import torch
from typing import List, Optional, Union, Any
from transformer_lens import HookedTransformer

from probity.probes import BaseProbe, MultiClassLogisticProbe


class ProbeInference:
    """
    Primary interface for running trained probes on new text.

    This class handles BaseProbe instances in a consistent way.
    It manages the full pipeline of tokenization, model forward pass, activation extraction,
    and probe application with proper handling of standardization and transformations.
    """

    def __init__(
        self,
        model_name: str,
        hook_point: str,
        probe: BaseProbe,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
    ):
        self.device = device
        self.model_name = model_name
        self.hook_point = hook_point

        # Set up probe
        self.probe = probe.to(device)
        self.probe.eval()
        self.probe_type = "base_probe"
        self.probe_class = probe.__class__.__name__

        # Setup model
        self.model = HookedTransformer.from_pretrained_no_processing(model_name)
        self.model.to(device)

    def get_activations(self, text: Union[str, List[str]]) -> torch.Tensor:
        """Get model activations for text input.

        Args:
            text: Input text or list of texts

        Returns:
            Tensor of activations (batch_size, seq_len, hidden_size)
        """
        # Ensure text is a list
        if isinstance(text, str):
            text = [text]

        # Tokenize
        # Make sure the model has a tokenizer
        if not hasattr(self.model, "tokenizer") or self.model.tokenizer is None:
            raise AttributeError("Model does not have a valid tokenizer.")

        tokens = self.model.tokenizer(text, return_tensors="pt", padding=True)

        # Get activations
        with torch.no_grad():
            _, cache = self.model.run_with_cache(
                tokens["input_ids"].to(self.device),
                names_filter=[self.hook_point],
                return_cache_object=True,
            )

        return cache[self.hook_point]

    def __call__(self, text: Union[str, List[str]]) -> torch.Tensor:
        """
        Deprecated: Please use get_direction_activations() for raw activations or
        get_probabilities() for properly transformed outputs.

        Get probe outputs for text.

        Args:
            text: Input text or list of texts

        Returns:
            Tensor of probe outputs (batch_size, seq_len, [n_vectors for VectorSet])
        """
        import warnings

        warnings.warn(
            "Direct calling is deprecated. Use get_direction_activations() for "
            "raw activations or get_probabilities() for properly transformed outputs.",
            DeprecationWarning,
            stacklevel=2,
        )
        return self.get_direction_activations(text)

    def get_direction_activations(self, text: Union[str, List[str]]) -> torch.Tensor:
        """Get activations along the probe direction.

        This projects the activations onto the learned probe direction by taking
        the dot product between the activations and the direction vector.

        Args:
            text: Input text or list of texts

        Returns:
            Tensor of direction activations (batch_size, seq_len)
        """
        # Get activations
        activations = self.get_activations(text)
        batch_size, seq_len, hidden_size = activations.shape

        # Reshape for batch calculation
        flat_activations = activations.view(-1, hidden_size)

        # Standardization is handled by the probe itself now
        # #if hasattr(self.probe, '_apply_standardization'):
        # #     flat_activations = self.probe._apply_standardization(flat_activations)

        # Get the probe direction (always normalized for consistency)
        direction = self.probe.get_direction()

        # Calculate dot product with direction
        with torch.no_grad():
            # For multi-dimensional directions (rarely used)
            if direction.dim() > 1:
                # Assuming flat_activations [B*S, D], direction [O, D]
                # Result [B*S, O]
                outputs = torch.matmul(flat_activations, direction.t())
            else:
                # For single vector directions (common case)
                # flat_activations [B*S, D], direction [D]
                # Result [B*S]
                outputs = torch.matmul(flat_activations, direction)

        # Reshape back
        if outputs.dim() > 1 and outputs.size(1) > 1:
            # Multi-dimensional output [B*S, O] -> [B, S, O]
            return outputs.view(batch_size, seq_len, -1).cpu()
        else:
            # Single dimension output [B*S] -> [B, S]
            return outputs.view(batch_size, seq_len).cpu()

    def get_probe_outputs(self, text: Union[str, List[str]]) -> torch.Tensor:
        """Get outputs using the probe's forward method.

        This uses the probe's specific forward implementation which may include
        additional transformations beyond a simple dot product. The probe's forward
        method now handles standardization internally.

        Args:
            text: Input text or list of texts

        Returns:
            Tensor of probe outputs (batch_size, seq_len, [output_dim])
        """
        # Get activations
        activations = self.get_activations(text)
        batch_size, seq_len, hidden_size = activations.shape

        # Reshape for probe
        flat_activations = activations.view(-1, hidden_size)

        # Run probe - the probe's forward method handles all standardization internally
        with torch.no_grad():
            outputs = self.probe(flat_activations)

        # Reshape back
        if outputs.dim() > 1 and outputs.size(1) > 1:
            # Multi-dimensional output [B*S, O] -> [B, S, O]
            return outputs.view(batch_size, seq_len, -1).cpu()
        else:
            # Single dimension output [B*S] -> [B, S]
            return outputs.view(batch_size, seq_len).cpu()

    def get_probabilities(self, text: Union[str, List[str]]) -> torch.Tensor:
        """Get probabilities from the probe.

        Applies sigmoid for binary logistic probes, softmax for multi-class
        logistic probes, and returns raw outputs otherwise.

        Args:
            text: Input text or list of texts

        Returns:
            Tensor of probabilities/outputs (batch_size, seq_len, [num_classes])
        """
        outputs = self.get_probe_outputs(text)

        # Apply sigmoid for binary logistic probes
        logistic_probe_types = ["LogisticProbe", "SklearnLogisticProbe"]
        if self.probe_class in logistic_probe_types:
            return torch.sigmoid(outputs)
        # Apply softmax for multi-class logistic probes
        elif isinstance(self.probe, MultiClassLogisticProbe):
            # Apply softmax along the class dimension (last dimension after reshape)
            return torch.softmax(outputs, dim=-1)
        else:
            # For other probe types, return raw outputs
            return outputs

    @classmethod
    def from_saved_probe(
        cls,
        model_name: str,
        hook_point: str,
        probe_path: str,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        probe_class: Optional[Any] = None,
    ) -> "ProbeInference":
        """Create inference instance from saved probe.

        Args:
            model_name: Name of the model
            hook_point: Hook point to use
            probe_path: Path to the saved probe
            device: Device to run on
            probe_class: Optional probe class for custom probe types

        Returns:
            ProbeInference instance
        """
        # Load the probe based on the file extension
        if probe_path.endswith(".json"):
            # JSON format
            if probe_class is None:
                # Rely on BaseProbe.load_json to determine the class
                # from probity.probes import LogisticProbe # Example fallback removed
                # probe_class = LogisticProbe
                probe = BaseProbe.load_json(probe_path)
            else:
                probe = probe_class.load_json(probe_path)
        else:
            # PyTorch format
            if probe_class is None:
                # Rely on BaseProbe.load to determine the class
                # from probity.probes import LogisticProbe # Example fallback removed
                # probe_class = LogisticProbe
                probe = BaseProbe.load(probe_path)
            else:
                probe = probe_class.load(probe_path)

        return cls(model_name, hook_point, probe, device=device)



================================================
FILE: probity/probes/linear.py
================================================
import torch
import torch.nn as nn

from .base import BaseProbe
from .config import LinearProbeConfig


class LinearProbe(BaseProbe[LinearProbeConfig]):
    """Simple linear probe for regression or finding directions.

    Learns a linear transformation Wx + b. The direction is derived from W.
    Operates on original activation space.
    """

    def __init__(self, config: LinearProbeConfig):
        super().__init__(config)
        self.linear = nn.Linear(config.input_size, config.output_size, bias=config.bias)

        # Initialize weights (optional, can use default PyTorch init)
        nn.init.kaiming_uniform_(self.linear.weight, nonlinearity="linear")
        if config.bias and self.linear.bias is not None:
            nn.init.zeros_(self.linear.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass. Input x is expected in the original activation space."""
        # Standardization is handled by the trainer externally if needed
        return self.linear(x)

    def _get_raw_direction_representation(self) -> torch.Tensor:
        """Return the raw linear layer weights."""
        return self.linear.weight.data

    def _set_raw_direction_representation(self, vector: torch.Tensor) -> None:
        """Set the raw linear layer weights."""
        if self.linear.weight.shape != vector.shape:
            # Reshape if necessary (e.g., loaded [dim] but need [1, dim])
            if (
                self.linear.weight.dim() == 2
                and self.linear.weight.shape[0] == 1
                and vector.dim() == 1
            ):
                vector = vector.unsqueeze(0)
            elif (
                self.linear.weight.dim() == 1
                and vector.dim() == 2
                and vector.shape[0] == 1
            ):
                vector = vector.squeeze(0)
            else:
                raise ValueError(
                    f"Shape mismatch loading vector. Probe weight: {self.linear.weight.shape}, Loaded vector: {vector.shape}"
                )
        with torch.no_grad():
            self.linear.weight.copy_(vector)

    def get_direction(self, normalized: bool = True) -> torch.Tensor:
        """Get the learned probe direction, applying normalization."""
        # Start with raw weights (already in original activation space)
        direction = self._get_raw_direction_representation().clone()

        # Normalize if requested and configured
        should_normalize = normalized and self.config.normalize_weights
        if should_normalize:
            if self.config.output_size > 1:
                # Normalize each output direction independently
                norms = torch.norm(direction, dim=1, keepdim=True)
                direction = direction / (norms + 1e-8)
            else:
                # Normalize the single direction vector
                norm = torch.norm(direction)
                direction = direction / (norm + 1e-8)

        # Squeeze if single output dimension for convenience
        if self.config.output_size == 1:
            direction = direction.squeeze(0)

        return direction

    # get_loss_fn remains specific to LinearProbe, not moved to base
    def get_loss_fn(self) -> nn.Module:
        """Selects loss function based on config."""
        if self.config.loss_type == "mse":
            return nn.MSELoss()
        elif self.config.loss_type == "cosine":
            # CosineEmbeddingLoss expects targets y = 1 or -1
            # Input: (x1, x2, y) -> computes loss based on y * cos(x1, x2)
            # Here, pred is x1, target direction (implicit) is x2, label is y
            # We might need a wrapper if target vectors aren't directly available
            print(
                "Warning: Cosine loss in LinearProbe assumes target vectors are handled externally."
            )
            return nn.CosineEmbeddingLoss()
        elif self.config.loss_type == "l1":
            return nn.L1Loss()
        else:
            raise ValueError(f"Unknown loss type: {self.config.loss_type}")



================================================
FILE: probity/probes/logistic.py
================================================
import torch
import torch.nn as nn
from typing import Optional

from .base import BaseProbe
from .config import LogisticProbeConfig, MultiClassLogisticProbeConfig


class LogisticProbe(BaseProbe[LogisticProbeConfig]):
    """Logistic regression probe implemented using nn.Linear. Operates on original activation space."""

    def __init__(self, config: LogisticProbeConfig):
        super().__init__(config)
        # Logistic regression is essentially a linear layer followed by sigmoid (handled by loss)
        self.linear = nn.Linear(config.input_size, config.output_size, bias=config.bias)

        # Initialize weights (zeros often work well for logistic init)
        nn.init.zeros_(self.linear.weight)
        if config.bias and self.linear.bias is not None:
            nn.init.zeros_(self.linear.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass. Returns logits. Input x is expected in the original activation space."""
        # Standardization is handled by the trainer externally if needed
        return self.linear(x)

    def _get_raw_direction_representation(self) -> torch.Tensor:
        """Return the raw linear layer weights."""
        return self.linear.weight.data

    def _set_raw_direction_representation(self, vector: torch.Tensor) -> None:
        """Set the raw linear layer weights."""
        if self.linear.weight.shape != vector.shape:
            if (
                self.linear.weight.dim() == 2
                and self.linear.weight.shape[0] == 1
                and vector.dim() == 1
            ):
                vector = vector.unsqueeze(0)
            elif (
                self.linear.weight.dim() == 1
                and vector.dim() == 2
                and vector.shape[0] == 1
            ):
                vector = vector.squeeze(0)
            else:
                raise ValueError(
                    f"Shape mismatch loading vector. Probe weight: {self.linear.weight.shape}, Loaded vector: {vector.shape}"
                )
        with torch.no_grad():
            self.linear.weight.copy_(vector)

    def get_direction(self, normalized: bool = True) -> torch.Tensor:
        """Get the learned probe direction, applying normalization."""
        # Start with raw weights (already in original activation space)
        direction = self._get_raw_direction_representation().clone()

        # Normalize if requested and configured
        should_normalize = normalized and self.config.normalize_weights
        if should_normalize:
            if self.config.output_size > 1:
                norms = torch.norm(direction, dim=1, keepdim=True)
                direction = direction / (norms + 1e-8)
            else:
                norm = torch.norm(direction)
                direction = direction / (norm + 1e-8)

        # Squeeze if single output dimension
        if self.config.output_size == 1:
            direction = direction.squeeze(0)

        return direction

    def get_loss_fn(self, pos_weight: Optional[torch.Tensor] = None) -> nn.Module:
        """Get binary cross entropy loss with logits.

        Args:
            pos_weight: Optional weight for positive class (for class imbalance).
        """
        return nn.BCEWithLogitsLoss(pos_weight=pos_weight)


class MultiClassLogisticProbe(BaseProbe[MultiClassLogisticProbeConfig]):
    """Multi-class logistic regression probe (Softmax Regression)."""

    def __init__(self, config: MultiClassLogisticProbeConfig):
        super().__init__(config)
        if config.output_size <= 1:
            raise ValueError("MultiClassLogisticProbe requires output_size > 1.")
        self.linear = nn.Linear(config.input_size, config.output_size, bias=config.bias)

        # Initialize weights
        nn.init.zeros_(self.linear.weight)
        if config.bias and self.linear.bias is not None:
            nn.init.zeros_(self.linear.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass. Returns logits for each class."""
        return self.linear(x)

    def _get_raw_direction_representation(self) -> torch.Tensor:
        """Return the raw linear layer weights (weight matrix)."""
        return self.linear.weight.data

    def _set_raw_direction_representation(self, vector: torch.Tensor) -> None:
        """Set the raw linear layer weights (weight matrix)."""
        if self.linear.weight.shape != vector.shape:
            raise ValueError(
                f"Shape mismatch loading vector. Probe weight: "
                f"{self.linear.weight.shape}, Loaded vector: {vector.shape}"
            )
        with torch.no_grad():
            self.linear.weight.copy_(vector)

    def get_direction(self, normalized: bool = True) -> torch.Tensor:
        """Get the learned probe directions (weight matrix), applying normalization per class."""
        # Start with raw weights
        directions = self._get_raw_direction_representation().clone()

        # Normalize if requested and configured
        should_normalize = normalized and self.config.normalize_weights
        if should_normalize:
            # Normalize each class direction (row) independently
            norms = torch.norm(directions, dim=1, keepdim=True)
            directions = directions / (norms + 1e-8)

        return directions  # Shape [output_size, input_size]

    def get_loss_fn(self, class_weights: Optional[torch.Tensor] = None) -> nn.Module:
        """Get cross entropy loss.

        Args:
            class_weights: Optional weights for each class (for class imbalance).
                           Shape [output_size].
        """
        return nn.CrossEntropyLoss(weight=class_weights)



================================================
FILE: probity/probes/probe_set.py
================================================
import os
import json
import torch
from typing import List, Optional

from .base import BaseProbe


class ProbeSet:
    """A collection of probes, typically acting on the same activation space."""

    def __init__(self, probes: List[BaseProbe]):
        self.probes = probes

        # Validate compatibility and store common metadata
        if probes:
            first_probe = probes[0]
            self.input_dim = getattr(first_probe.config, "input_size", None)
            self.model_name = getattr(first_probe.config, "model_name", None)
            self.hook_point = getattr(first_probe.config, "hook_point", None)
            self.hook_layer = getattr(first_probe.config, "hook_layer", None)
            self.device = getattr(first_probe.config, "device", None)
            self.dtype = getattr(
                first_probe, "dtype", torch.float32
            )  # Get dtype from probe instance

            if self.input_dim is None:
                raise ValueError(
                    f"First probe '{first_probe.name}' must have input_size defined in its config."
                )

            for i, p in enumerate(probes[1:], 1):
                p_dim = getattr(p.config, "input_size", None)
                if p_dim != self.input_dim:
                    raise ValueError(
                        f"Probe {i} ('{p.name}') has input dimension {p_dim}, but expected {self.input_dim} based on the first probe."
                    )
                # Optional: Stricter checks for model_name, hook_point etc.
                p_model = getattr(p.config, "model_name", None)
                if p_model != self.model_name:
                    print(
                        f"Warning: Probe {i} ('{p.name}') model '{p_model}' differs from set model '{self.model_name}'."
                    )
                p_hook = getattr(p.config, "hook_point", None)
                if p_hook != self.hook_point:
                    print(
                        f"Warning: Probe {i} ('{p.name}') hook point '{p_hook}' differs from set hook point '{self.hook_point}'."
                    )

        else:
            self.input_dim = None
            self.model_name = None
            self.hook_point = None
            self.hook_layer = None
            self.device = None
            self.dtype = torch.float32  # Default dtype if empty set

    def encode(self, acts: torch.Tensor) -> torch.Tensor:
        """Compute dot products with all probes' normalized directions.

        Args:
            acts: Activations to project, shape [..., d_model]

        Returns:
            Projected values, shape [..., n_probes]
        """
        if not self.probes:
            return torch.empty(
                acts.shape[:-1] + (0,), device=acts.device, dtype=self.dtype
            )

        # Ensure activations are on the correct device and dtype
        target_device = self.device or acts.device
        acts = acts.to(device=target_device, dtype=self.dtype)

        # Get normalized directions for all probes
        # Ensure all directions are on the target device and correct dtype
        try:
            weight_matrix = torch.stack(
                [
                    p.get_direction(normalized=True).to(
                        device=target_device, dtype=self.dtype
                    )
                    for p in self.probes
                ]
            )  # Shape [n_probes, d_model]
        except Exception as e:
            probe_shapes = [
                (p.name, p.get_direction(normalized=True).shape) for p in self.probes
            ]
            raise RuntimeError(
                f"Error stacking probe directions during encode. Check probe consistency. Shapes: {probe_shapes}. Original error: {e}"
            )

        # Check for dimension mismatch before einsum
        if acts.shape[-1] != weight_matrix.shape[-1]:
            raise ValueError(
                f"Activation dimension ({acts.shape[-1]}) does not match probe direction dimension ({weight_matrix.shape[-1]})."
            )

        # Project all at once using einsum for flexibility with batch dimensions
        return torch.einsum("...d,nd->...n", acts, weight_matrix)

    def __getitem__(self, idx) -> BaseProbe:
        """Get a probe by index."""
        return self.probes[idx]

    def __len__(self) -> int:
        """Get number of probes."""
        return len(self.probes)

    def save(self, directory: str, use_json: bool = False) -> None:
        """Save all probes to a directory, including an index file.

        Args:
            directory: Directory to save the probes.
            use_json: If True, save probes in JSON format, otherwise use .pt.
        """
        os.makedirs(directory, exist_ok=True)

        # Save index file with common metadata and list of probe files
        index = {
            "model_name": self.model_name,
            "hook_point": self.hook_point,
            "hook_layer": self.hook_layer,
            "format": "json" if use_json else "pt",
            "probes": [],
        }

        # Save each probe individually
        for i, probe in enumerate(self.probes):
            # Sanitize probe name for filename (replace non-alphanumeric with underscore)
            safe_name = "".join(c if c.isalnum() else "_" for c in probe.name)
            # Ensure filename is not excessively long
            max_len = 60  # Max length for the name part
            safe_name = safe_name[:max_len]

            filename = f"probe_{i}_{safe_name}.{'json' if use_json else 'pt'}"
            filepath = os.path.join(directory, filename)

            try:
                if use_json:
                    probe.save_json(filepath)
                else:
                    probe.save(filepath)
            except Exception as e:
                print(f"Error saving probe {i} ('{probe.name}') to {filepath}: {e}")
                # Continue saving other probes
                continue

            # Add entry to index
            index["probes"].append(
                {
                    "name": probe.name,
                    "file": filename,
                    "probe_type": probe.__class__.__name__,
                }
            )

        # Save the index file
        index_path = os.path.join(directory, "index.json")
        try:
            with open(index_path, "w") as f:
                json.dump(index, f, indent=2)
        except Exception as e:
            print(f"Error saving index file to {index_path}: {e}")

    @classmethod
    def load(cls, directory: str, device: Optional[str] = None) -> "ProbeSet":
        """Load a ProbeSet from a directory containing probes and an index.json file.

        Args:
            directory: Directory containing the probes and index.json.
            device: Optional device override for loading probes. If None, uses device
                    specified during saving (from index/probe metadata) or default.

        Returns:
            ProbeSet instance
        """
        index_path = os.path.join(directory, "index.json")
        if not os.path.exists(index_path):
            raise FileNotFoundError(f"Index file not found in directory: {directory}")

        # Load index
        try:
            with open(index_path) as f:
                index = json.load(f)
        except Exception as e:
            raise IOError(f"Error reading index file {index_path}: {e}")

        # Load each probe listed in the index
        probes = []
        save_format = index.get(
            "format", "pt"
        )  # Default to .pt if format not specified

        for i, entry in enumerate(index.get("probes", [])):
            filename = entry.get("file")
            probe_type_name = entry.get("probe_type")
            probe_name = entry.get("name", f"probe_{i}")

            if not filename or not probe_type_name:
                print(
                    f"Warning: Skipping entry {i} in index due to missing 'file' or 'probe_type': {entry}"
                )
                continue

            filepath = os.path.join(directory, filename)
            if not os.path.exists(filepath):
                print(
                    f"Warning: File {filepath} listed in index for probe '{probe_name}' not found. Skipping probe."
                )
                continue

            # Dynamically get the probe class (delegated to BaseProbe.load/.load_json)
            # We need a starting point - BaseProbe itself
            try:
                if save_format == "json":
                    # BaseProbe.load_json will determine the correct class from metadata
                    probe = BaseProbe.load_json(filepath, device=device)
                else:
                    # BaseProbe.load will determine the correct class from saved .pt file
                    probe = BaseProbe.load(filepath, device=device)

                # Optional: Verify loaded probe type matches index entry
                if probe.__class__.__name__ != probe_type_name:
                    print(
                        f"Warning: Loaded probe type '{probe.__class__.__name__}' from {filename} does not match index type '{probe_type_name}'."
                    )

                probes.append(probe)

            except Exception as e:
                print(
                    f"Error loading probe '{probe_name}' from {filepath}: {e}. Skipping probe."
                )
                # Decide whether to raise an error or just skip the problematic probe
                # For now, skipping
                continue

        if not probes:
            print("Warning: Loaded an empty ProbeSet.")

        # Create the ProbeSet instance
        return cls(probes)



================================================
FILE: probity/probes/sklearn_logistic.py
================================================
import torch
import numpy as np
from typing import Optional, Literal
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

from .base import BaseProbe
from .config import SklearnLogisticProbeConfig

# Define the literal type for solver more precisely
SolverLiteral = Literal[
    "lbfgs", "liblinear", "newton-cg", "newton-cholesky", "sag", "saga"
]


class SklearnLogisticProbe(BaseProbe[SklearnLogisticProbeConfig]):
    """Logistic regression probe using scikit-learn. Handles its own standardization internally."""

    # Explicitly type hint buffers and internal models
    unscaled_coef_: Optional[torch.Tensor]
    intercept_: Optional[torch.Tensor]
    scaler: Optional[StandardScaler]
    model: LogisticRegression

    def __init__(self, config: SklearnLogisticProbeConfig):
        super().__init__(config)
        # Store scaler and model internally
        self.scaler = StandardScaler() if config.standardize else None
        # Validate solver type
        solver: SolverLiteral = (
            config.solver if config.solver in SolverLiteral.__args__ else "lbfgs"
        )
        if solver != config.solver:
            print(
                f"Warning: Invalid solver '{config.solver}' specified. Using default 'lbfgs'. Valid options: {SolverLiteral.__args__}"
            )

        self.model = LogisticRegression(
            max_iter=config.max_iter,
            random_state=config.random_state,
            fit_intercept=config.bias,
            solver=solver,  # Use validated solver
        )
        # Store the final, unscaled coefficients and intercept as tensors
        # Initialize buffers as None; they will be populated by fit() or load()
        self.register_buffer("unscaled_coef_", None, persistent=True)
        self.register_buffer("intercept_", None, persistent=True)

    def fit(self, x: torch.Tensor, y: torch.Tensor) -> None:
        """Fit the probe using sklearn's LogisticRegression.
        Stores unscaled coefficients internally.
        Input x is expected to be in the original activation space for this fit method.
        """
        x_np = x.cpu().numpy().astype(np.float32)
        y_np = y.cpu().numpy()
        if y_np.ndim > 1:
            y_np = y_np.squeeze()  # Ensure y is 1D

        # Apply internal standardization if requested
        if self.scaler is not None:
            try:
                x_np_scaled = self.scaler.fit_transform(x_np)
            except ValueError as e:
                # Handle cases where scaler fails (e.g., constant features)
                print(
                    f"Warning: StandardScaler failed during fit: {e}. Proceeding without scaling."
                )
                x_np_scaled = x_np
                self.scaler = None  # Disable scaler if it failed
        else:
            x_np_scaled = x_np

        # Fit logistic regression on potentially scaled data
        try:
            self.model.fit(x_np_scaled, y_np)
        except Exception as e:
            raise RuntimeError(f"Sklearn LogisticRegression fitting failed: {e}")

        # Get coefficients (potentially scaled) and intercept
        coef_ = (
            self.model.coef_.squeeze()
            if self.model.coef_.shape[0] == 1
            else self.model.coef_
        )
        intercept_ = self.model.intercept_

        # Unscale coefficients if internal standardization was used *and successful*
        if (
            self.scaler is not None
            and hasattr(self.scaler, "scale_")
            and self.scaler.scale_ is not None
        ):
            # Ensure scaler has 'scale_' attribute before accessing
            scale_ = self.scaler.scale_
            # Add epsilon to avoid division by zero or very small numbers
            coef_unscaled = coef_ / (scale_ + 1e-8)
        else:
            coef_unscaled = coef_

        # Store unscaled coefficients and intercept as tensors (buffers)
        # Convert numpy arrays to tensors on the correct device and dtype
        coef_tensor = torch.tensor(
            coef_unscaled, dtype=self.dtype, device=self.config.device
        )
        intercept_tensor = torch.tensor(
            intercept_, dtype=self.dtype, device=self.config.device
        )

        # Update buffers using _set_raw... or directly via setattr/copy_
        self._set_raw_direction_representation(coef_tensor)
        # Update intercept buffer separately
        if self.intercept_ is None:
            self.register_buffer("intercept_", intercept_tensor.clone())
        else:
            with torch.no_grad():
                self.intercept_.copy_(intercept_tensor)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Predict logits using the learned coefficients. Input x is in original activation space."""
        if self.unscaled_coef_ is None:
            raise RuntimeError(
                "SklearnLogisticProbe must be fitted or loaded before calling forward."
            )

        # Forward pass uses the stored unscaled coefficients and intercept
        x = x.to(dtype=self.dtype)
        # Ensure coef and intercept are tensors before use
        coef = self.unscaled_coef_
        intercept = self.intercept_

        if not isinstance(coef, torch.Tensor):
            raise RuntimeError("unscaled_coef_ is not a valid tensor.")

        # Calculate logits: (x @ coef^T) + intercept
        # Need to handle potential shape mismatch between x and coef (e.g., [B, D] vs [D] or [C, D])
        if coef.dim() == 1:
            # Binary classification case: coef is [D]
            logits = torch.matmul(x, coef)
        elif coef.dim() == 2 and coef.shape[0] == 1:
            # Binary classification case (sklearn default): coef is [1, D]
            logits = torch.matmul(x, coef.squeeze(0))
        elif coef.dim() == 2 and x.dim() >= 2:
            # Multi-class classification: coef is [C, D], x is [..., D]
            # Result should be [..., C]
            logits = torch.matmul(x, coef.t())
        else:
            raise RuntimeError(
                f"Unexpected shape combination for forward pass: x {x.shape}, coef {coef.shape}"
            )

        if intercept is not None:
            if isinstance(intercept, torch.Tensor):
                # Ensure intercept shape aligns with logits shape for broadcasting
                # Logits: [B] or [..., C]. Intercept: [1] or [C]
                try:
                    logits += intercept
                except RuntimeError as e:
                    raise RuntimeError(
                        f"Error adding intercept during forward pass. Logits shape: {logits.shape}, Intercept shape: {intercept.shape}. Original error: {e}"
                    )
            else:
                raise RuntimeError("intercept_ is not a valid tensor.")

        return logits

    def _get_raw_direction_representation(self) -> torch.Tensor:
        """Return the stored unscaled coefficients."""
        if self.unscaled_coef_ is None:
            # Return zero vector if not fitted/loaded, consistent with DirectionalProbe
            print(
                f"Warning: Accessing direction for probe {self.name} before fit/load. Returning zero vector."
            )
            input_size = getattr(self.config, "input_size", None)
            device = getattr(self.config, "device", "cpu")
            if input_size is None:
                raise ValueError("Cannot determine input_size to create zero vector.")
            return torch.zeros(input_size, dtype=self.dtype, device=device)
        # Ensure it's a tensor before returning
        if not isinstance(self.unscaled_coef_, torch.Tensor):
            raise RuntimeError(
                "_get_raw_direction_representation: unscaled_coef_ is not a Tensor"
            )
        return self.unscaled_coef_

    def _set_raw_direction_representation(self, vector: torch.Tensor) -> None:
        """Set the unscaled coefficients. Used primarily for loading."""
        # vector should already be unscaled when loading

        # Get target device and dtype
        device = getattr(self.config, "device", "cpu")
        dtype = self.dtype

        # Ensure vector is on correct device and dtype
        vector = vector.to(device=device, dtype=dtype)

        # Check shape if buffer already exists
        if hasattr(self, "unscaled_coef_") and self.unscaled_coef_ is not None:
            if self.unscaled_coef_.shape != vector.shape:
                # Attempt common reshape scenarios (e.g., [1, dim] vs [dim])
                try:
                    target_shape = self.unscaled_coef_.shape
                    print(
                        f"Warning: Reshaping loaded vector from {vector.shape} to match existing buffer shape {target_shape}"
                    )
                    vector = vector.reshape(target_shape)
                except Exception as e:
                    raise ValueError(
                        f"Shape mismatch loading vector for {self.name}. Buffer shape: {self.unscaled_coef_.shape}, Loaded vector shape: {vector.shape}. Reshape failed: {e}"
                    )

        # Update or register the buffer
        if not hasattr(self, "unscaled_coef_") or self.unscaled_coef_ is None:
            self.register_buffer("unscaled_coef_", vector.clone())
        else:
            with torch.no_grad():
                self.unscaled_coef_.copy_(vector)

    def get_direction(self, normalized: bool = True) -> torch.Tensor:
        """Get the probe direction (unscaled coefficients), applying normalization."""
        # Direction is already unscaled
        direction = self._get_raw_direction_representation().clone()

        # Handle potential multi-class case (coef shape [C, D])
        # For multi-class, we might return the matrix or require selecting a class direction
        # Current implementation normalizes each row independently if normalize_weights=True
        is_multiclass = direction.dim() == 2 and direction.shape[0] > 1

        # Normalize if requested and configured
        should_normalize = normalized and self.config.normalize_weights
        if should_normalize:
            if is_multiclass:
                # Normalize each class direction (row) independently
                norms = torch.norm(direction, dim=1, keepdim=True)
                direction = direction / (norms + 1e-8)
            else:
                # Normalize the single direction vector (potentially squeezing [1,D] to [D])
                if direction.dim() == 2 and direction.shape[0] == 1:
                    direction = direction.squeeze(0)
                norm = torch.norm(direction)
                if norm > 1e-8:
                    direction = direction / norm
                else:
                    direction = torch.zeros_like(direction)
        else:
            # If not normalizing, still squeeze the [1, D] case for consistency?
            # Let's keep it as [C, D] or [1, D] or [D] as determined by _get_raw...
            pass

        return direction



================================================
FILE: probity/training/__init__.py
================================================



================================================
FILE: probity/training/trainer.py
================================================
from abc import ABC
from dataclasses import dataclass

# from pathlib import Path # Unused
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch import optim
from typing import Optional, Dict, List, Tuple, Literal  # Removed Any, Type
from tqdm.auto import tqdm
import math

from probity.collection.activation_store import ActivationStore

# Probes now expect non-standardized data for forward pass and store unscaled directions
from probity.probes import (
    BaseProbe,
    DirectionalProbe,
    MultiClassLogisticProbe,  # Add import
    LogisticProbe,  # Added back for instanceof check
    LinearProbe,  # Added for instanceof check
)


@dataclass
class BaseTrainerConfig:
    """Enhanced base configuration shared by all trainers."""

    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    save_dir: Optional[str] = None
    batch_size: int = 32
    learning_rate: float = 1e-3
    end_learning_rate: float = 1e-5  # For LR scheduling
    weight_decay: float = 0.01
    num_epochs: int = 10
    show_progress: bool = True
    optimizer_type: Literal["Adam", "SGD", "AdamW"] = "Adam"
    handle_class_imbalance: bool = True
    standardize_activations: bool = False  # Option to standardize *during* training


class BaseProbeTrainer(ABC):
    """Enhanced abstract base class for all probe trainers. Handles standardization during training."""

    def __init__(self, config: BaseTrainerConfig):
        self.config = config
        # Store standardization stats if standardization is enabled
        self.feature_mean: Optional[torch.Tensor] = None
        self.feature_std: Optional[torch.Tensor] = None

    def _get_lr_scheduler(
        self, optimizer: optim.Optimizer, start_lr: float, end_lr: float, num_steps: int
    ) -> optim.lr_scheduler.LRScheduler:
        """Create exponential learning rate scheduler."""
        if start_lr <= 0 or end_lr <= 0:
            # Handle cases where LR might be zero or negative, default to constant LR
            print("Warning: start_lr or end_lr <= 0, using constant LR scheduler.")
            return optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)

        # Ensure num_steps is positive
        if num_steps <= 0:
            print("Warning: num_steps <= 0 for LR scheduler, using constant LR.")
            return optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)

        # Avoid log(0) or division by zero
        if start_lr == end_lr:
            gamma = 1.0
        else:
            # Ensure end_lr / start_lr is positive
            ratio = end_lr / start_lr
            if ratio <= 0:
                print(
                    f"Warning: Invalid learning rate range ({start_lr} -> {end_lr}). "
                    f"Using constant LR."
                )
                return optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)
            gamma = math.exp(math.log(ratio) / num_steps)

        return optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)

    def _calculate_pos_weights(self, y: torch.Tensor) -> torch.Tensor:
        """Calculate positive weights for handling class imbalance.

        Handles both single-output (y shape: [N, 1]) and
        multi-output (y shape: [N, C]) cases.
        """
        if y.dim() == 1:
            y = y.unsqueeze(1)

        # Calculate weights for each output dimension
        num_pos = y.sum(dim=0)
        num_neg = len(y) - num_pos
        weights = num_neg / (num_pos + 1e-8)  # Add epsilon to prevent division by zero

        return weights

    def _calculate_class_weights(
        self, y: torch.Tensor, num_classes: int
    ) -> Optional[torch.Tensor]:
        """Calculate class weights for multi-class CrossEntropyLoss."""
        if y.dim() == 2 and y.shape[1] == 1:
            y = y.squeeze(1)  # Convert [N, 1] to [N]
        if y.dim() != 1:
            print("Warning: Cannot calculate class weights for non-1D target tensor.")
            return None

        # Check dtype before attempting conversion or calculations
        if y.dtype != torch.long:
            print(
                f"Warning: Cannot calculate class weights for non-Long target tensor "
                f"(dtype: {y.dtype})."
            )
            return None
            # Note: We no longer attempt conversion for non-long types.
            # If conversion is desired, the calling code should handle it.

        counts = torch.bincount(y, minlength=num_classes)
        # Avoid division by zero for classes with zero samples
        total_samples = counts.sum()
        if total_samples == 0:
            return None  # No samples to calculate weights from

        # Calculate weights: (total_samples / (num_classes * count_for_class))
        # This gives higher weight to less frequent classes.
        weights = total_samples / (num_classes * (counts + 1e-8))

        # Handle cases where a class might have 0 samples (weight will be large but finite due to epsilon)
        # Clamp weights to avoid extreme values? Optional.
        # weights = torch.clamp(weights, min=0.1, max=10.0)

        return weights

    def _create_optimizer(self, model: torch.nn.Module) -> optim.Optimizer:
        """Create optimizer based on config."""
        optimizer_class = getattr(optim, self.config.optimizer_type, None)
        if optimizer_class is None:
            raise ValueError(f"Unknown optimizer type: {self.config.optimizer_type}")

        # Filter out parameters that do not require gradients
        params_to_optimize = filter(lambda p: p.requires_grad, model.parameters())

        if self.config.optimizer_type in ["Adam", "AdamW"]:
            optimizer = optimizer_class(
                params_to_optimize,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
            )
        elif self.config.optimizer_type == "SGD":
            optimizer = optimizer_class(
                params_to_optimize,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
                # Add momentum? momentum=0.9 might be a good default
            )
        else:
            # Should be caught by getattr check, but belts and braces
            raise ValueError(
                f"Unsupported optimizer type: {self.config.optimizer_type}"
            )

        return optimizer

    def prepare_data(
        self,
        activation_store: ActivationStore,
        position_key: str,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Prepare data for training, optionally applying standardization.

        Computes standardization statistics if standardize_activations is True.
        Returns:
            X_train: Activations potentially standardized for training.
            y: Labels.
            X_orig: Original, non-standardized activations.
        """
        X_orig, y = activation_store.get_probe_data(position_key)
        X_train = X_orig  # Default to original if no standardization

        # Apply standardization only if configured
        if self.config.standardize_activations:
            # Compute statistics if not already done (e.g., first call)
            if self.feature_mean is None or self.feature_std is None:
                self.feature_mean = X_orig.mean(dim=0, keepdim=True)
                # Add epsilon for numerical stability
                self.feature_std = X_orig.std(dim=0, keepdim=True) + 1e-8

                # Move statistics to the correct device
                if self.config.device:
                    target_device = torch.device(self.config.device)
                    self.feature_mean = self.feature_mean.to(target_device)
                    self.feature_std = self.feature_std.to(target_device)

            # Apply standardization to create X_train
            if self.feature_mean is not None and self.feature_std is not None:
                # Ensure X_orig is on the same device as stats before operation
                if hasattr(self.feature_mean, "device"):
                    X_orig_dev = X_orig.to(self.feature_mean.device)
                    X_train = (X_orig_dev - self.feature_mean) / self.feature_std
                else:
                    # Fallback if stats don't have device info (shouldn't happen)
                    X_train = (X_orig - self.feature_mean) / self.feature_std

        return X_train, y, X_orig  # Return both training and original activations

    # Removed transfer_stats_to_model


@dataclass
class SupervisedTrainerConfig(BaseTrainerConfig):
    """Enhanced config for supervised training methods."""

    train_ratio: float = 0.8
    patience: int = 5
    min_delta: float = 1e-4


class SupervisedProbeTrainer(BaseProbeTrainer):
    """Enhanced trainer for supervised probes with progress tracking and LR scheduling."""

    def __init__(self, config: SupervisedTrainerConfig):
        super().__init__(config)
        # Ensure self.config has the more specific type
        if not isinstance(config, SupervisedTrainerConfig):
            raise TypeError("SupervisedProbeTrainer requires a SupervisedTrainerConfig")
        self.config: SupervisedTrainerConfig = config

    def prepare_supervised_data(
        self,
        activation_store: ActivationStore,
        position_key: str,
    ) -> Tuple[DataLoader, DataLoader]:
        """Prepare train/val splits with DataLoader creation.
        DataLoaders yield batches of (X_train, y, X_orig).
        """
        X_train_all, y_all, X_orig_all = self.prepare_data(
            activation_store, position_key
        )

        # Split data
        n_total = len(X_orig_all)
        n_train = int(n_total * self.config.train_ratio)
        # Ensure validation set is not empty
        if n_train == n_total:
            n_train = max(
                0, n_total - 1
            )  # Keep at least one sample for validation if possible
            if n_train == 0 and n_total > 0:
                print(
                    "Warning: Only one data point available. Using it for training and validation."
                )
                n_train = 1
            elif n_total > 0:
                print(
                    "Warning: train_ratio resulted in no validation data. "
                    "Adjusting to keep one sample for validation."
                )

        # Ensure train set is not empty if n_total > 0
        if n_train == 0 and n_total > 0:
            print(
                "Warning: train_ratio resulted in no training data. Using all data for training."
            )
            n_train = n_total

        # Generate random permutation
        indices = torch.randperm(n_total)
        train_indices = indices[:n_train]
        val_indices = indices[n_train:]

        # Handle edge case: If either split is empty (can happen if n_total is very small)
        if len(train_indices) == 0 and n_total > 0:
            train_indices = indices  # Use all data for training
            val_indices = indices  # Use all data for validation too (less ideal)
            print(
                "Warning: No training samples after split. Using all data for training and validation."
            )
        elif len(val_indices) == 0 and n_total > 0:
            val_indices = (
                train_indices  # Use training data for validation if val split is empty
            )
            print(
                "Warning: No validation samples after split. "
                "Using training data for validation."
            )

        X_train_split, X_val_split = (
            X_train_all[train_indices],
            X_train_all[val_indices],
        )
        X_orig_train_split, X_orig_val_split = (
            X_orig_all[train_indices],
            X_orig_all[val_indices],
        )
        y_train_split, y_val_split = y_all[train_indices], y_all[val_indices]

        # Create dataloaders
        # Handle both single and multi-dimensional labels
        y_train_split = (
            y_train_split if y_train_split.dim() > 1 else y_train_split.unsqueeze(1)
        )
        y_val_split = y_val_split if y_val_split.dim() > 1 else y_val_split.unsqueeze(1)

        # Note: We don't move tensors to device here because DataLoader will
        # create copies that could waste memory. Instead, we move tensors to
        # device in training loop just before using them.
        train_dataset = TensorDataset(X_train_split, y_train_split, X_orig_train_split)
        val_dataset = TensorDataset(X_val_split, y_val_split, X_orig_val_split)

        train_loader = DataLoader(
            train_dataset, batch_size=self.config.batch_size, shuffle=True
        )
        val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size)

        return train_loader, val_loader

    def train_epoch(
        self,
        model: torch.nn.Module,  # Use base Module type here, checked in train
        train_loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        loss_fn: torch.nn.Module,
        epoch: int,
        num_epochs: int,
        is_multi_class: bool = False,  # Flag for multi-class loss
    ) -> float:
        """Run one epoch of training with progress tracking. Uses X_train for training."""
        model.train()
        total_loss = 0

        # Create progress bar for batches
        batch_pbar = tqdm(
            train_loader,
            desc=f"Epoch {epoch+1}/{num_epochs}",
            disable=not self.config.show_progress,
            leave=False,
        )

        for (
            batch_x_train,
            batch_y,
            _,
        ) in batch_pbar:  # Ignore X_orig during training pass
            optimizer.zero_grad()
            batch_x_train = batch_x_train.to(self.config.device)
            batch_y = batch_y.to(self.config.device)

            # --- Adjust target shape/type for loss ---
            if is_multi_class:
                # CrossEntropyLoss expects Long targets of shape [N]
                if batch_y.dim() == 2 and batch_y.shape[1] == 1:
                    batch_y = batch_y.squeeze(1)
                batch_y = batch_y.long()  # Ensure Long type
            else:  # BCEWithLogitsLoss expects Float targets
                batch_y = batch_y.float()  # Ensure Float type
            # -----------------------------------------

            # Model forward pass uses the potentially standardized data
            outputs = model(batch_x_train)
            loss = loss_fn(outputs, batch_y)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            batch_pbar.set_postfix({"Loss": f"{loss.item():.6f}"})

        return total_loss / len(train_loader)

    def train(
        self,
        model: BaseProbe,  # Expect a BaseProbe instance
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
    ) -> Dict[str, List[float]]:
        """Train model, potentially unscale direction if standardization was used."""
        # Ensure model is a BaseProbe instance
        if not isinstance(model, BaseProbe):
            raise TypeError(
                "SupervisedProbeTrainer expects model to be an instance of BaseProbe"
            )

        # Ensure model is on the correct device
        target_device = torch.device(self.config.device)
        model.to(target_device)

        # Determine if this is a multi-class probe
        is_multi_class = isinstance(model, MultiClassLogisticProbe)
        # num_classes = model.config.output_size if hasattr(model.config, 'output_size') else 1 # Removed

        # Standardization stats are managed by the trainer, not transferred

        optimizer = self._create_optimizer(model)
        scheduler = self._get_lr_scheduler(
            optimizer,
            self.config.learning_rate,
            self.config.end_learning_rate,
            self.config.num_epochs,
        )

        # --- Set up loss function ---
        loss_fn: nn.Module
        all_train_y: Optional[torch.Tensor] = None
        if self.config.handle_class_imbalance:
            # Calculate all labels only once if needed for any weight calculation
            all_train_y = torch.cat([y for _, y, _ in train_loader])

        if isinstance(model, MultiClassLogisticProbe):
            weights_arg: Optional[torch.Tensor] = None
            if self.config.handle_class_imbalance and all_train_y is not None:
                num_classes = model.config.output_size
                class_weights = self._calculate_class_weights(all_train_y, num_classes)
                if class_weights is not None:
                    weights_arg = class_weights.to(target_device)
            # Pass `class_weights` keyword arg
            loss_fn = model.get_loss_fn(class_weights=weights_arg)

        elif isinstance(model, LogisticProbe):
            weights_arg: Optional[torch.Tensor] = None
            if self.config.handle_class_imbalance and all_train_y is not None:
                pos_weight = self._calculate_pos_weights(all_train_y)
                if pos_weight is not None:
                    weights_arg = pos_weight.to(target_device)
            # Pass `pos_weight` keyword arg
            loss_fn = model.get_loss_fn(pos_weight=weights_arg)

        elif isinstance(model, LinearProbe):
            # LinearProbe loss (MSE, L1, Cosine) doesn't use these weights
            loss_fn = model.get_loss_fn()
            if self.config.handle_class_imbalance:
                print(
                    f"Warning: Class imbalance handling enabled, but may not be effective "
                    f"for LinearProbe with loss type '{model.config.loss_type}'."
                )
        else:
            # Fallback for other BaseProbe types that might be supervised
            probe_type_name = type(model).__name__
            print(
                f"Warning: Unknown supervised probe type '{probe_type_name}'. "
                f"Attempting default BCEWithLogitsLoss. Ensure this is appropriate."
            )
            weights_arg: Optional[torch.Tensor] = None
            if self.config.handle_class_imbalance and all_train_y is not None:
                pos_weight = self._calculate_pos_weights(all_train_y)
                if pos_weight is not None:
                    weights_arg = pos_weight.to(target_device)
            # Assume BCE loss for unknown types
            loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights_arg)
        # --- End Loss Function Setup ---

        # Training history
        history: Dict[str, List[float]] = {
            "train_loss": [],
            "val_loss": [],
            "learning_rate": [],
        }

        # Create progress bar for epochs
        epoch_pbar = tqdm(
            range(self.config.num_epochs),
            desc="Training",
            disable=not self.config.show_progress,
        )

        best_val_loss = float("inf")
        patience_counter = 0

        for epoch in epoch_pbar:
            # Train epoch uses X_train
            train_loss = self.train_epoch(
                model,
                train_loader,
                optimizer,
                loss_fn,
                epoch,
                self.config.num_epochs,
                is_multi_class=is_multi_class,  # Pass flag
            )
            history["train_loss"].append(train_loss)

            # Validation uses X_orig
            if val_loader is not None:
                val_loss = self.validate(
                    model, val_loader, loss_fn, is_multi_class=is_multi_class
                )
                history["val_loss"].append(val_loss)

                # Early stopping check
                if val_loss < best_val_loss - self.config.min_delta:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # Save best model state? Optional
                else:
                    patience_counter += 1

                if patience_counter >= self.config.patience:
                    print(f"\nEarly stopping triggered after {epoch + 1} epochs")
                    break

                epoch_pbar.set_postfix(
                    {
                        "Train Loss": f"{train_loss:.6f}",
                        "Val Loss": f"{val_loss:.6f}",
                        "LR": f"{scheduler.get_last_lr()[0]:.2e}",
                    }
                )
            else:
                epoch_pbar.set_postfix(
                    {
                        "Train Loss": f"{train_loss:.6f}",
                        "LR": f"{scheduler.get_last_lr()[0]:.2e}",
                    }
                )

            history["learning_rate"].append(scheduler.get_last_lr()[0])
            scheduler.step()

        # --- Post-Training Direction Unscaling ---
        if self.config.standardize_activations and self.feature_std is not None:
            print("Unscaling probe direction...")
            with torch.no_grad():
                # Get the direction learned on standardized data
                learned_direction = model._get_raw_direction_representation()

                # Unscale the direction
                # Ensure std dev matches direction dims for division
                std_dev = self.feature_std.squeeze().to(learned_direction.device)

                # Handle potential shape mismatches (e.g., [1, dim] vs [dim])
                if (
                    learned_direction.dim() == 2
                    and learned_direction.shape[0] == 1
                    and std_dev.dim() == 1
                ):
                    # Common case for Linear/Logistic: weights are [1, dim], std_dev is [dim]
                    unscaled_direction = learned_direction / std_dev.unsqueeze(0)
                elif learned_direction.shape == std_dev.shape:
                    unscaled_direction = learned_direction / std_dev
                elif (
                    learned_direction.dim() == 1
                    and std_dev.dim() == 1
                    and learned_direction.shape[0] == std_dev.shape[0]
                ):
                    # Case for single vector directions (like maybe directional probes before squeeze?)
                    unscaled_direction = learned_direction / std_dev
                else:
                    print(
                        f"Warning: Shape mismatch during final unscaling. Direction: {learned_direction.shape}, StdDev: {std_dev.shape}. Skipping unscaling."
                    )
                    unscaled_direction = (
                        learned_direction  # Keep original if shapes mismatch
                    )

                # Update the probe's internal representation with the unscaled direction
                model._set_raw_direction_representation(unscaled_direction)

        return history

    def validate(
        self,
        model: torch.nn.Module,  # Use base Module type here
        val_loader: DataLoader,
        loss_fn: torch.nn.Module,
        is_multi_class: bool = False,  # Flag for multi-class loss
    ) -> float:
        """Run validation with progress tracking. Uses X_orig for validation."""
        model.eval()
        total_loss = 0

        with torch.no_grad():
            for _, batch_y, batch_x_orig in val_loader:  # Use X_orig for validation
                batch_x_orig = batch_x_orig.to(self.config.device)
                batch_y = batch_y.to(self.config.device)

                # --- Adjust target shape/type for loss ---
                if is_multi_class:
                    # CrossEntropyLoss expects Long targets of shape [N]
                    if batch_y.dim() == 2 and batch_y.shape[1] == 1:
                        batch_y = batch_y.squeeze(1)
                    batch_y = batch_y.long()  # Ensure Long type
                else:  # BCEWithLogitsLoss expects Float targets
                    batch_y = batch_y.float()  # Ensure Float type
                # -----------------------------------------

                # Model forward pass uses original (non-standardized) data
                # Assumes the probe direction has been unscaled if needed after training
                outputs = model(batch_x_orig)
                loss = loss_fn(outputs, batch_y)
                total_loss += loss.item()

        return total_loss / len(val_loader)


@dataclass
class DirectionalTrainerConfig(BaseTrainerConfig):
    """Configuration for training direction-finding probes."""

    pass  # Uses base config settings


class DirectionalProbeTrainer(BaseProbeTrainer):
    """Trainer for probes that find directions through direct computation (KMeans, PCA, MeanDiff)."""

    def __init__(self, config: DirectionalTrainerConfig):
        super().__init__(config)
        # Ensure self.config has the more specific type
        if not isinstance(config, DirectionalTrainerConfig):
            raise TypeError(
                "DirectionalProbeTrainer requires a DirectionalTrainerConfig"
            )
        self.config: DirectionalTrainerConfig = config

    def prepare_supervised_data(
        self,
        activation_store: ActivationStore,
        position_key: str,
    ) -> Tuple[DataLoader, DataLoader]:
        """Prepare data for directional probe computation.
        Returns two identical DataLoaders yielding (X_train, y, X_orig).
        """
        X_train_all, y_all, X_orig_all = self.prepare_data(
            activation_store, position_key
        )

        # Create a dataset with all data
        # Handle single/multi-dimensional labels
        y_all = y_all if y_all.dim() > 1 else y_all.unsqueeze(1)
        dataset = TensorDataset(X_train_all, y_all, X_orig_all)  # Include X_orig

        # Create loader for all data
        # Shuffle False might be better if order matters, but usually doesn't for these methods
        # Use full dataset length as batch size for single fit step
        batch_size_fit = len(dataset)
        if batch_size_fit == 0:
            print("Warning: Dataset is empty for DirectionalProbeTrainer")
            # Return empty loaders to avoid errors
            return DataLoader([]), DataLoader([])

        all_data_loader = DataLoader(
            dataset,
            batch_size=batch_size_fit,
            shuffle=False,  # No need to shuffle if using full batch
        )

        # Return the same loader twice to maintain the trainer interface
        return all_data_loader, all_data_loader

    def train(
        self,
        model: DirectionalProbe,  # Expect DirectionalProbe instance
        train_loader: DataLoader,
        val_loader: Optional[
            DataLoader
        ] = None,  # val_loader is ignored but kept for API consistency
    ) -> Dict[str, List[float]]:
        """Train method for directional probes.
        1. Accumulate all data (X_train, y, X_orig)
        2. Call probe's fit method with X_train, which returns the initial direction
        3. Unscale the initial direction if standardization was used
        4. Set the probe's final direction buffer
        5. Compute metrics using X_orig and the final probe state
        """
        # Ensure model is a DirectionalProbe instance
        if not isinstance(model, DirectionalProbe):
            raise TypeError(
                "DirectionalProbeTrainer expects model to be an instance of DirectionalProbe"
            )

        # Ensure model is on the correct device
        target_device = torch.device(self.config.device)
        model.to(target_device)

        # Standardization stats are managed by trainer

        # Accumulate all data (loader should have batch_size=len(dataset))
        try:
            x_train_tensor, y_train_tensor, x_orig_tensor = next(iter(train_loader))
        except StopIteration:
            print("Warning: Training loader is empty.")
            return {"train_loss": [], "val_loss": []}  # Return empty history

        x_train_tensor = x_train_tensor.to(target_device)
        y_train_tensor = y_train_tensor.to(target_device)
        x_orig_tensor = x_orig_tensor.to(target_device)  # Keep original data too

        # Fit the probe using potentially standardized data (X_train)
        # The fit method now returns the initial direction (potentially scaled)
        initial_direction = model.fit(x_train_tensor, y_train_tensor)

        # --- Unscale Direction ---
        final_direction = initial_direction  # Default if no standardization
        if self.config.standardize_activations and self.feature_std is not None:
            print("Unscaling directional probe direction...")
            std_dev = self.feature_std.squeeze().to(initial_direction.device)
            # Assume initial_direction is [dim]
            if initial_direction.shape == std_dev.shape:
                final_direction = initial_direction / std_dev
            # Add case for [1, dim] direction and [dim] std_dev
            elif (
                initial_direction.dim() == 2
                and initial_direction.shape[0] == 1
                and std_dev.dim() == 1
            ):
                final_direction = initial_direction / std_dev.unsqueeze(0)
            else:
                print(
                    f"Warning: Shape mismatch during directional probe unscaling. "
                    f"Direction: {initial_direction.shape}, StdDev: {std_dev.shape}. "
                    f"Skipping unscaling."
                )

        # Set the final, unscaled direction in the probe's buffer
        model._set_raw_direction_representation(final_direction)

        # --- Compute Metrics ---
        # Metrics should be computed using the *original* data and the *final* probe state
        history: Dict[str, List[float]] = {
            "train_loss": [],
            # Validation loss is same as train loss since we use all data
            # and val_loader is ignored
            "val_loss": [],
        }

        # Calculate loss using original data and final probe state
        with torch.no_grad():
            # Probe forward uses the final (unscaled) direction set above
            preds = model(x_orig_tensor)
            # Ensure y_train_tensor matches expected shape for loss
            y_target = y_train_tensor.float()
            if preds.dim() == 1 and y_target.dim() == 2 and y_target.shape[1] == 1:
                y_target = y_target.squeeze(1)
            elif preds.shape != y_target.shape:
                # Attempt to align shapes if possible (e.g., [N] vs [N, 1])
                try:
                    y_target = y_target.view_as(preds)
                except RuntimeError:
                    print(
                        f"Warning: Could not align prediction ({preds.shape}) and "
                        f"target ({y_target.shape}) shapes for loss calculation."
                    )
                    # Fallback or skip loss calculation
                    history["train_loss"].append(float("nan"))
                    history["val_loss"].append(float("nan"))
                    return history

            # Use BCEWithLogitsLoss as it's common for binary classification probes
            # Use original y labels
            try:
                # Use the appropriate loss based on the probe type
                if isinstance(model, MultiClassLogisticProbe):
                    loss_fn = nn.CrossEntropyLoss()
                    y_target = y_target.long().squeeze()  # Ensure long and [N] shape
                else:
                    loss_fn = nn.BCEWithLogitsLoss()
                    # y_target is already float

                loss = loss_fn(preds, y_target)
                loss_item = loss.item()
            except Exception as e:
                print(f"Error during loss calculation: {e}")

        history["train_loss"].append(loss_item)
        history["val_loss"].append(loss_item)  # Use same loss for val

        return history



================================================
FILE: probity/utils/dataset_loading.py
================================================



================================================
FILE: tests/test_probe_save_load.py
================================================
import torch
import numpy as np
import os
import pytest
from pathlib import Path
from typing import Dict, List

from probity.datasets.templated import TemplatedDataset
from probity.datasets.tokenized import TokenizedProbingDataset
from transformers import AutoTokenizer
from probity.probes.linear_probe import LogisticProbe, LogisticProbeConfig
from probity.training.trainer import SupervisedProbeTrainer, SupervisedTrainerConfig
from probity.probes.inference import ProbeInference

# Create a temporary directory for test artifacts
@pytest.fixture
def temp_dir():
    path = Path("test_artifacts")
    if not path.exists():
        path.mkdir()
    yield path
    # Uncomment to clean up after tests
    # import shutil
    # shutil.rmtree(path)

def set_seed(seed=42):
    """Set random seeds for reproducibility."""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    import random
    random.seed(seed)

def create_movie_dataset():
    """Create a simple movie sentiment dataset identical to the tutorial."""
    adjectives = {
        "positive": ["incredible", "amazing", "fantastic", "awesome", "beautiful"],
        "negative": ["terrible", "awful", "horrible", "bad", "disappointing"]
    }
    verbs = {
        "positive": ["loved", "enjoyed", "adored"],
        "negative": ["hated", "disliked", "detested"]
    }

    # Create dataset using factory method
    movie_dataset = TemplatedDataset.from_movie_sentiment_template(
        adjectives=adjectives,
        verbs=verbs
    )

    # Convert to probing dataset
    probing_dataset = movie_dataset.to_probing_dataset(
        label_from_metadata="sentiment",
        label_map={"positive": 1, "negative": 0},
        auto_add_positions=True
    )

    # Convert to tokenized dataset
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=probing_dataset,
        tokenizer=tokenizer,
        padding=True,
        max_length=128,
        add_special_tokens=True
    )
    
    return tokenized_dataset

def train_probe(dataset, model_name="gpt2", hook_point="blocks.7.hook_resid_pre", device="cpu"):
    """Train a logistic probe on the dataset."""
    from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig
    
    # Set up logistic probe configuration
    probe_config = LogisticProbeConfig(
        input_size=768,
        normalize_weights=True,
        bias=True,
        model_name=model_name,
        hook_point=hook_point,
        hook_layer=7,
        name="test_sentiment_probe"
    )

    # Set up trainer configuration
    trainer_config = SupervisedTrainerConfig(
        batch_size=32,
        learning_rate=1e-3,
        num_epochs=3,  # Reduced epochs for faster testing
        weight_decay=0.01,
        train_ratio=0.8,
        handle_class_imbalance=True,
        show_progress=True,
        device=device
    )

    pipeline_config = ProbePipelineConfig(
        dataset=dataset,
        probe_cls=LogisticProbe,
        probe_config=probe_config,
        trainer_cls=SupervisedProbeTrainer,
        trainer_config=trainer_config,
        position_key="ADJ",  # Probe at the adjective position
        model_name=model_name,
        hook_points=[hook_point],
        cache_dir="./test_artifacts/probe_cache",
        device=device
    )

    # Create and run pipeline
    pipeline = ProbePipeline(pipeline_config)
    probe, training_history = pipeline.run()
    
    return probe

def compare_probes(original_probe, loaded_probe, test_examples, device="cpu"):
    """Compare two probes to check if they produce the same outputs."""
    
    model_name = "gpt2"
    hook_point = "blocks.7.hook_resid_pre"
    
    # Create inference objects
    original_inference = ProbeInference(
        model_name=model_name,
        hook_point=hook_point,
        probe=original_probe,
        device=device
    )
    
    loaded_inference = ProbeInference(
        model_name=model_name, 
        hook_point=hook_point,
        probe=loaded_probe,
        device=device
    )
    
    # Compare direction vectors
    original_direction = original_probe.get_direction(normalized=True)
    loaded_direction = loaded_probe.get_direction(normalized=True)
    
    # Compare raw activations
    original_raw_scores = original_inference.get_direction_activations(test_examples)
    loaded_raw_scores = loaded_inference.get_direction_activations(test_examples)
    
    # Compare probabilities
    original_probs = original_inference.get_probabilities(test_examples)
    loaded_probs = loaded_inference.get_probabilities(test_examples)
    
    # Return all comparison data
    return {
        "direction_cosine_sim": torch.nn.functional.cosine_similarity(
            original_direction, loaded_direction, dim=0).item(),
        "direction_diff": torch.abs(original_direction - loaded_direction).max().item(),
        "raw_scores_diff": torch.abs(original_raw_scores - loaded_raw_scores).max().item(),
        "probabilities_diff": torch.abs(original_probs - loaded_probs).max().item(),
        "original_direction": original_direction,
        "loaded_direction": loaded_direction,
        "original_raw_scores": original_raw_scores,
        "loaded_raw_scores": loaded_raw_scores,
        "original_probs": original_probs,
        "loaded_probs": loaded_probs
    }

def test_probe_save_load_pt(temp_dir):
    """Test that a probe saved and loaded in PyTorch format has consistent behavior."""
    set_seed(42)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Create dataset
    dataset = create_movie_dataset()
    
    # Train probe
    original_probe = train_probe(dataset, device=device)
    
    # Save probe in PyTorch format
    probe_path = temp_dir / "test_probe.pt"
    original_probe.save(str(probe_path))
    
    # Load probe back
    loaded_probe = LogisticProbe.load(str(probe_path))
    loaded_probe.to(device)
    
    # Test examples
    test_examples = [
        "The movie was incredible and I loved every minute of it.",
        "That film was absolutely terrible and I hated it."
    ]
    
    # Compare probes
    results = compare_probes(original_probe, loaded_probe, test_examples, device)
    
    # Print detailed comparison for debugging
    print("\n=== PyTorch Format Test Results ===")
    print(f"Direction cosine similarity: {results['direction_cosine_sim']:.6f}")
    print(f"Max direction difference: {results['direction_diff']:.6f}")
    print(f"Max raw scores difference: {results['raw_scores_diff']:.6f}")
    print(f"Max probabilities difference: {results['probabilities_diff']:.6f}")
    
    # Original probe details
    print("\nOriginal probe - feature stats:")
    if hasattr(original_probe, 'feature_mean'):
        print(f"  feature_mean exists: {original_probe.feature_mean is not None}")
        if original_probe.feature_mean is not None:
            print(f"  feature_mean shape: {original_probe.feature_mean.shape}")
    if hasattr(original_probe, 'feature_std'):
        print(f"  feature_std exists: {original_probe.feature_std is not None}")
        if original_probe.feature_std is not None:
            print(f"  feature_std shape: {original_probe.feature_std.shape}")
    
    # Loaded probe details
    print("\nLoaded probe - feature stats:")
    if hasattr(loaded_probe, 'feature_mean'):
        print(f"  feature_mean exists: {loaded_probe.feature_mean is not None}")
        if loaded_probe.feature_mean is not None:
            print(f"  feature_mean shape: {loaded_probe.feature_mean.shape}")
    if hasattr(loaded_probe, 'feature_std'):
        print(f"  feature_std exists: {loaded_probe.feature_std is not None}")
        if loaded_probe.feature_std is not None:
            print(f"  feature_std shape: {loaded_probe.feature_std.shape}")
    
    # Assertions (with tolerance)
    assert results['direction_cosine_sim'] > 0.999, "Direction vectors should be almost identical"
    assert results['direction_diff'] < 1e-4, "Direction vectors should be almost identical"
    assert results['raw_scores_diff'] < 1e-2, "Raw scores should be almost identical"
    assert results['probabilities_diff'] < 1e-2, "Probabilities should be almost identical"

def test_probe_save_load_json(temp_dir):
    """Test that a probe saved and loaded in JSON format has consistent behavior."""
    set_seed(42)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Create dataset
    dataset = create_movie_dataset()
    
    # Train probe
    original_probe = train_probe(dataset, device=device)
    
    # Get the original standardization buffers and weights
    original_mean = original_probe.feature_mean
    original_std = original_probe.feature_std
    original_weight = original_probe.linear.weight.data.clone()
    
    # Get the original direction vector 
    original_direction = original_probe.get_direction(normalized=True)
    original_unnorm_direction = original_probe.get_direction(normalized=False)
    
    # Save probe in JSON format
    json_path = temp_dir / "test_probe.json"
    original_probe.save_json(str(json_path))
    
    # Examine the saved JSON content
    import json
    with open(json_path, 'r') as f:
        saved_data = json.load(f)
        
    print("\n=== JSON Format Detailed Info ===")
    print("JSON Metadata keys:", list(saved_data["metadata"].keys()))
    print(f"is_standardized: {saved_data['metadata'].get('is_standardized')}")
    print(f"is_normalized: {saved_data['metadata'].get('is_normalized')}")
    print(f"is_unscaled: {saved_data['metadata'].get('is_unscaled')}")
    print(f"has_bias: {saved_data['metadata'].get('has_bias')}")
    
    # Check if feature_mean and feature_std are in the metadata
    if 'feature_mean' in saved_data['metadata']:
        print("feature_mean in JSON:", torch.tensor(saved_data['metadata']['feature_mean']).shape)
    if 'feature_std' in saved_data['metadata']:
        print("feature_std in JSON:", torch.tensor(saved_data['metadata']['feature_std']).shape)
    
    # Load probe back
    loaded_probe = LogisticProbe.load_json(str(json_path))
    loaded_probe.to(device)
    
    # Get the loaded standardization buffers and weights
    loaded_mean = loaded_probe.feature_mean
    loaded_std = loaded_probe.feature_std  
    loaded_weight = loaded_probe.linear.weight.data.clone()
    
    # Get the loaded direction vector
    loaded_direction = loaded_probe.get_direction(normalized=True)
    loaded_unnorm_direction = loaded_probe.get_direction(normalized=False)
    
    # Compare weights and standardization buffers
    print("\nWeight comparison:")
    print(f"Original weight shape: {original_weight.shape}")
    print(f"Loaded weight shape: {loaded_weight.shape}")
    print(f"Weights equal: {torch.allclose(original_weight, loaded_weight)}")
    print(f"Weight difference max: {torch.abs(original_weight - loaded_weight).max().item()}")
    
    # Compare standardization buffers
    print("\nStandardization buffers comparison:")
    if original_mean is not None and loaded_mean is not None:
        print(f"Mean equal: {torch.allclose(original_mean, loaded_mean)}")
        print(f"Mean difference max: {torch.abs(original_mean - loaded_mean).max().item()}")
    else:
        print("Mean comparison unavailable - one or both is None")
        
    if original_std is not None and loaded_std is not None:
        print(f"Std equal: {torch.allclose(original_std, loaded_std)}")
        print(f"Std difference max: {torch.abs(original_std - loaded_std).max().item()}")
    else:
        print("Std comparison unavailable - one or both is None")
        
    # Compare direction vectors
    print("\nDirection vector comparison:")
    print(f"Normalized original direction shape: {original_direction.shape}")
    print(f"Normalized loaded direction shape: {loaded_direction.shape}")
    print(f"Direction cosine similarity: {torch.nn.functional.cosine_similarity(original_direction, loaded_direction, dim=0).item():.6f}")
    print(f"Direction difference max: {torch.abs(original_direction - loaded_direction).max().item():.6f}")
    
    # Compare unnormalized direction vectors
    print("\nUnnormalized direction vector comparison:")
    print(f"Unnorm original direction shape: {original_unnorm_direction.shape}")
    print(f"Unnorm loaded direction shape: {loaded_unnorm_direction.shape}")
    cos_sim_unnorm = torch.nn.functional.cosine_similarity(original_unnorm_direction, loaded_unnorm_direction, dim=0).item()
    print(f"Unnorm direction cosine similarity: {cos_sim_unnorm:.6f}")
    
    # Compare computation steps
    print("\nGet_direction computation comparison:")
    
    # Get config's additional_info
    original_info = getattr(original_probe.config, 'additional_info', {})
    loaded_info = getattr(loaded_probe.config, 'additional_info', {})
    
    print(f"Original additional_info: {original_info}")
    print(f"Loaded additional_info: {loaded_info}")
    
    # Test the impact of standardization on get_direction
    test_input = torch.randn(3, 768, device=device)
    
    original_standardized = None
    if hasattr(original_probe, '_apply_standardization'):
        original_standardized = original_probe._apply_standardization(test_input)
        
    loaded_standardized = None
    if hasattr(loaded_probe, '_apply_standardization'):
        loaded_standardized = loaded_probe._apply_standardization(test_input)
    
    if original_standardized is not None and loaded_standardized is not None:
        print(f"Standardized inputs equal: {torch.allclose(original_standardized, loaded_standardized)}")
        print(f"Standardized inputs diff max: {torch.abs(original_standardized - loaded_standardized).max().item()}")
    
    # Test examples
    test_examples = [
        "The movie was incredible and I loved every minute of it.",
        "That film was absolutely terrible and I hated it."
    ]
    
    # Compare probes
    results = compare_probes(original_probe, loaded_probe, test_examples, device)
    
    # Print detailed comparison for debugging
    print("\n=== JSON Format Test Results ===")
    print(f"Direction cosine similarity: {results['direction_cosine_sim']:.6f}")
    print(f"Max direction difference: {results['direction_diff']:.6f}")
    print(f"Max raw scores difference: {results['raw_scores_diff']:.6f}")
    print(f"Max probabilities difference: {results['probabilities_diff']:.6f}")
    
    # Original probe details
    print("\nOriginal probe - feature stats:")
    if hasattr(original_probe, 'feature_mean'):
        print(f"  feature_mean exists: {original_probe.feature_mean is not None}")
        if original_probe.feature_mean is not None:
            print(f"  feature_mean shape: {original_probe.feature_mean.shape}")
    if hasattr(original_probe, 'feature_std'):
        print(f"  feature_std exists: {original_probe.feature_std is not None}")
        if original_probe.feature_std is not None:
            print(f"  feature_std shape: {original_probe.feature_std.shape}")
    
    # Loaded probe details
    print("\nLoaded probe - feature stats:")
    if hasattr(loaded_probe, 'feature_mean'):
        print(f"  feature_mean exists: {loaded_probe.feature_mean is not None}")
        if loaded_probe.feature_mean is not None:
            print(f"  feature_mean shape: {loaded_probe.feature_mean.shape}")
    if hasattr(loaded_probe, 'feature_std'):
        print(f"  feature_std exists: {loaded_probe.feature_std is not None}")
        if loaded_probe.feature_std is not None:
            print(f"  feature_std shape: {loaded_probe.feature_std.shape}")
            
    # FIXME: Because of the inconsistency in JSON format, we're temporarily relaxing the assertions
    if cos_sim_unnorm > 0.7:
        print("PASS: Unnormalized direction vectors have reasonable similarity")
    else:
        print("FAIL: Unnormalized direction vectors differ significantly")
        
    # Assertions (with tolerance)
    assert results['direction_cosine_sim'] > 0.999, "Direction vectors should be almost identical"
    assert results['direction_diff'] < 1e-4, "Direction vectors should be almost identical"
    assert results['raw_scores_diff'] < 1e-2, "Raw scores should be almost identical"
    assert results['probabilities_diff'] < 1e-2, "Probabilities should be almost identical"

def test_standardization_application():
    """Test that standardization is properly saved and applied during inference."""
    set_seed(42)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Create fixed test tensors
    mean = torch.tensor([1.0, 2.0, 3.0], device=device).reshape(1, -1)
    std = torch.tensor([0.5, 1.0, 1.5], device=device).reshape(1, -1)
    
    # Create a test probe with standardization
    probe_config = LogisticProbeConfig(input_size=3, device=device)
    probe = LogisticProbe(probe_config)
    
    # Register standardization buffers
    probe.register_buffer('feature_mean', mean)
    probe.register_buffer('feature_std', std)
    
    # Set fixed weights for testing
    with torch.no_grad():
        probe.linear.weight.data = torch.tensor([[1.0, 1.0, 1.0]], device=device)
        if probe.linear.bias is not None:
            probe.linear.bias.data = torch.tensor([0.0], device=device)
    
    # Save and load the probe
    temp_path = Path("test_artifacts")
    if not temp_path.exists():
        temp_path.mkdir()
    
    probe_path = temp_path / "test_standardization.pt"
    probe.save(str(probe_path))
    
    loaded_probe = LogisticProbe.load(str(probe_path))
    loaded_probe.to(device)
    
    # Test input
    test_input = torch.tensor([[2.0, 4.0, 6.0]], device=device)
    
    # Apply standardization manually
    expected_standardized = (test_input - mean) / std
    expected_output = torch.sum(expected_standardized, dim=1)
    
    # Get output from original probe
    original_output = probe(test_input)
    
    # Get output from loaded probe
    loaded_output = loaded_probe(test_input)
    
    # Print results for debugging
    print("\n=== Standardization Test Results ===")
    print(f"Test input: {test_input}")
    print(f"Expected standardized: {expected_standardized}")
    print(f"Expected output: {expected_output}")
    print(f"Original probe output: {original_output}")
    print(f"Loaded probe output: {loaded_output}")
    
    # Check if original probe applies standardization
    if hasattr(probe, '_apply_standardization'):
        standardized_input = probe._apply_standardization(test_input)
        print(f"Original probe standardized input: {standardized_input}")
        print(f"Matches expected standardized: {torch.allclose(standardized_input, expected_standardized)}")
    
    # Check if loaded probe applies standardization
    if hasattr(loaded_probe, '_apply_standardization'):
        standardized_input = loaded_probe._apply_standardization(test_input)
        print(f"Loaded probe standardized input: {standardized_input}")
        print(f"Matches expected standardized: {torch.allclose(standardized_input, expected_standardized)}")
    
    # Assertions
    assert torch.allclose(original_output, expected_output), "Original probe should apply standardization correctly"
    assert torch.allclose(loaded_output, expected_output), "Loaded probe should apply standardization correctly"
    assert torch.allclose(original_output, loaded_output), "Original and loaded probes should give same output"

def test_inference_standardization():
    """Test if ProbeInference correctly applies standardization."""
    set_seed(42)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Create fixed test tensors
    mean = torch.tensor([1.0, 2.0, 3.0], device=device).reshape(1, -1)
    std = torch.tensor([0.5, 1.0, 1.5], device=device).reshape(1, -1)
    
    # Create a test probe with standardization
    probe_config = LogisticProbeConfig(input_size=3, device=device)
    probe = LogisticProbe(probe_config)
    
    # Register standardization buffers
    probe.register_buffer('feature_mean', mean)
    probe.register_buffer('feature_std', std)
    
    # Set fixed weights for testing
    with torch.no_grad():
        probe.linear.weight.data = torch.tensor([[1.0, 1.0, 1.0]], device=device)
        if probe.linear.bias is not None:
            probe.linear.bias.data = torch.tensor([0.0], device=device)
    
    # Save and load the probe
    temp_path = Path("test_artifacts")
    if not temp_path.exists():
        temp_path.mkdir()
    
    # Test both formats
    for fmt in ["pt", "json"]:
        probe_path = temp_path / f"test_inference_{fmt}.{fmt}"
        if fmt == "pt":
            probe.save(str(probe_path))
        else:
            probe.save_json(str(probe_path))
        
        # Create inference object with loaded probe
        inference = ProbeInference.from_saved_probe(
            model_name="gpt2",
            hook_point="blocks.7.hook_resid_pre",
            probe_path=str(probe_path),
            device=device
        )
        
        # Check if the probe in the inference object has standardization buffers
        inference_probe = inference.probe
        
        print(f"\n=== Inference Test Results ({fmt} format) ===")
        if hasattr(inference_probe, 'feature_mean'):
            print(f"feature_mean exists: {inference_probe.feature_mean is not None}")
            if inference_probe.feature_mean is not None:
                print(f"feature_mean shape: {inference_probe.feature_mean.shape}")
                print(f"feature_mean: {inference_probe.feature_mean}")
        if hasattr(inference_probe, 'feature_std'):
            print(f"feature_std exists: {inference_probe.feature_std is not None}")
            if inference_probe.feature_std is not None:
                print(f"feature_std shape: {inference_probe.feature_std.shape}")
                print(f"feature_std: {inference_probe.feature_std}")
                
        # Test if _apply_standardization works
        if hasattr(inference_probe, '_apply_standardization'):
            test_input = torch.tensor([[2.0, 4.0, 6.0]], device=device)
            standardized_input = inference_probe._apply_standardization(test_input)
            expected_standardized = (test_input - mean) / std
            
            print(f"Test input: {test_input}")
            print(f"Standardized input: {standardized_input}")
            print(f"Expected standardized: {expected_standardized}")
            print(f"Matches expected: {torch.allclose(standardized_input, expected_standardized)}")
            
            assert torch.allclose(standardized_input, expected_standardized), \
                "Standardization in inference should match expected values"

def investigate_inference_pipeline():
    """Investigate how ProbeInference uses the probe object."""
    set_seed(42)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Create a simple probe
    probe_config = LogisticProbeConfig(input_size=3, device=device)
    probe = LogisticProbe(probe_config)
    
    # Add standardization for testing
    mean = torch.tensor([1.0, 2.0, 3.0], device=device).reshape(1, -1)
    std = torch.tensor([0.5, 1.0, 1.5], device=device).reshape(1, -1)
    probe.register_buffer('feature_mean', mean)
    probe.register_buffer('feature_std', std)
    
    # Create an inference object
    inference = ProbeInference(
        model_name="gpt2",
        hook_point="blocks.7.hook_resid_pre",
        probe=probe,
        device=device
    )
    
    # Test fixed activations 
    test_activations = torch.tensor([
        [[2.0, 4.0, 6.0]],  # batch 0, token 0
    ], device=device)
    
    # Mock the get_activations method to return our test activations
    original_get_activations = inference.get_activations
    inference.get_activations = lambda text: test_activations
    
    # Test the inference methods
    raw_scores = inference.get_direction_activations("dummy text")
    probs = inference.get_probabilities("dummy text")
    probe_outputs = inference.get_probe_outputs("dummy text")
    
    # Restore the original method
    inference.get_activations = original_get_activations
    
    # Compute expected values manually
    # For raw scores - no sigmoid
    standardized = (test_activations.view(-1, 3) - mean) / std
    direction = probe.get_direction(normalized=True)
    expected_raw = torch.matmul(standardized, direction).view(test_activations.shape[0], -1)
    
    # For probe outputs - direct probe output, no sigmoid yet
    expected_probe_output = probe(test_activations.view(-1, 3)).view(test_activations.shape[0], -1)
    
    # For probabilities - apply sigmoid to probe output
    expected_probs = torch.sigmoid(expected_probe_output)
    
    print("\n=== Inference Pipeline Investigation ===")
    print(f"Test activations: {test_activations}")
    print(f"Standardized: {standardized}")
    print(f"Direction: {direction}")
    print(f"Expected raw scores: {expected_raw}")
    print(f"Actual raw scores: {raw_scores}")
    print(f"Raw scores match: {torch.allclose(raw_scores, expected_raw)}")
    
    print(f"\nExpected probe outputs: {expected_probe_output}")
    print(f"Actual probe outputs: {probe_outputs}")
    print(f"Probe outputs match: {torch.allclose(probe_outputs, expected_probe_output)}")
    
    print(f"\nExpected probabilities: {expected_probs}")
    print(f"Actual probabilities: {probs}")
    print(f"Probabilities match: {torch.allclose(probs, expected_probs)}")
    
    # Check the methods used in get_direction_activations
    print("\nStandardization in get_direction_activations:")
    activations = test_activations.view(-1, 3)
    if hasattr(probe, '_apply_standardization'):
        standardized_act = probe._apply_standardization(activations)
        print(f"  Standardized activations: {standardized_act}")
        print(f"  Matches expected: {torch.allclose(standardized_act, standardized)}")
    
    # Return information about which methods were called and their results
    return {
        "raw_scores": raw_scores,
        "expected_raw": expected_raw,
        "match_raw": torch.allclose(raw_scores, expected_raw),
        "probs": probs,
        "expected_probs": expected_probs,
        "match_probs": torch.allclose(probs, expected_probs),
        "probe_outputs": probe_outputs,
        "expected_probe_output": expected_probe_output,
        "match_probe_output": torch.allclose(probe_outputs, expected_probe_output)
    } 


================================================
FILE: tests/unit/datasets/test_base.py
================================================
import pytest
import os
import json
import shutil
from datasets import Dataset

from probity.datasets.base import (
    ProbingDataset,
    ProbingExample,
    CharacterPositions,
    Position,
)


# Fixtures
@pytest.fixture
def sample_position():
    return Position(start=5, end=10)


@pytest.fixture
def sample_multi_position():
    return [Position(start=5, end=10), Position(start=20, end=25)]


@pytest.fixture
def sample_char_positions(sample_position, sample_multi_position):
    return CharacterPositions(
        positions={"word": sample_position, "phrase": sample_multi_position}
    )


@pytest.fixture
def sample_probing_example(sample_char_positions):
    return ProbingExample(
        text="This is a sample text.",
        label=1,
        label_text="positive",
        character_positions=sample_char_positions,
        group_id="group1",
        attributes={"source": "test"},
    )


@pytest.fixture
def sample_probing_example_no_pos():
    return ProbingExample(
        text="Another sample text.",
        label=0,
        label_text="negative",
        group_id="group2",
        attributes={"source": "test"},
    )


@pytest.fixture
def sample_probing_examples(sample_probing_example, sample_probing_example_no_pos):
    return [sample_probing_example, sample_probing_example_no_pos]


@pytest.fixture
def sample_probing_dataset(sample_probing_examples):
    return ProbingDataset(
        examples=sample_probing_examples,
        task_type="classification",
        valid_layers=["layer1"],
        label_mapping={"positive": 1, "negative": 0},
        dataset_attributes={"name": "test_dataset"},
    )


@pytest.fixture
def sample_hf_dataset(sample_probing_dataset):
    return sample_probing_dataset._to_hf_dataset()


@pytest.fixture
def temp_save_dir():
    path = "./temp_test_dataset"
    os.makedirs(path, exist_ok=True)
    yield path
    shutil.rmtree(path)


# Tests for CharacterPositions
def test_character_positions_getitem(sample_char_positions, sample_position):
    assert sample_char_positions["word"] == sample_position


def test_character_positions_keys(sample_char_positions):
    assert sample_char_positions.keys() == {"word", "phrase"}


# Tests for ProbingExample
def test_probing_example_creation(sample_probing_example):
    assert sample_probing_example.text == "This is a sample text."
    assert sample_probing_example.label == 1
    assert sample_probing_example.label_text == "positive"
    assert sample_probing_example.group_id == "group1"
    assert sample_probing_example.attributes == {"source": "test"}
    assert isinstance(sample_probing_example.character_positions, CharacterPositions)


# Tests for ProbingDataset
def test_probing_dataset_initialization(
    sample_probing_dataset, sample_probing_examples
):
    assert sample_probing_dataset.examples == sample_probing_examples
    assert sample_probing_dataset.task_type == "classification"
    assert sample_probing_dataset.valid_layers == ["layer1"]
    assert sample_probing_dataset.label_mapping == {"positive": 1, "negative": 0}
    assert sample_probing_dataset.dataset_attributes == {"name": "test_dataset"}
    assert sample_probing_dataset.position_types == {"word", "phrase"}


def test_probing_dataset_initialization_no_pos(sample_probing_example_no_pos):
    dataset = ProbingDataset([sample_probing_example_no_pos])
    assert dataset.position_types == set()


def test_probing_dataset_add_target_positions(
    sample_probing_dataset, sample_probing_example, sample_probing_example_no_pos
):
    def find_sample(text):
        if "sample" in text:
            return Position(start=text.find("sample"), end=text.find("sample") + 6)
        return None  # Test case where finder returns None

    sample_probing_dataset.add_target_positions("sample_word", find_sample)

    # Check example with positions
    example1 = sample_probing_dataset.examples[0]
    assert "sample_word" in example1.character_positions.keys()
    pos1 = example1.character_positions["sample_word"]
    assert isinstance(pos1, Position)
    assert pos1.start == 10
    assert pos1.end == 16

    # Check example without initial positions
    example2 = sample_probing_dataset.examples[1]
    assert example2.character_positions is not None  # Should be initialized
    assert "sample_word" in example2.character_positions.keys()
    pos2 = example2.character_positions["sample_word"]
    assert isinstance(pos2, Position)
    assert pos2.start == 8
    assert pos2.end == 14

    assert "sample_word" in sample_probing_dataset.position_types


def test_probing_dataset_to_hf_dataset(sample_probing_dataset, sample_hf_dataset):
    assert isinstance(sample_hf_dataset, Dataset)
    assert len(sample_hf_dataset) == 2
    assert set(sample_hf_dataset.column_names) == {
        "text",
        "label",
        "label_text",
        "group_id",
        "char_pos_word_start",
        "char_pos_word_end",
        "char_pos_word_multi",
        "char_pos_phrase_start",
        "char_pos_phrase_end",
        "char_pos_phrase_multi",
    }

    # Check first example (with positions)
    assert sample_hf_dataset[0]["text"] == "This is a sample text."
    assert sample_hf_dataset[0]["label"] == 1.0  # HF converts labels to float
    assert sample_hf_dataset[0]["char_pos_word_start"] == 5
    assert sample_hf_dataset[0]["char_pos_word_end"] == 10
    assert sample_hf_dataset[0]["char_pos_word_multi"] == []
    assert sample_hf_dataset[0]["char_pos_phrase_start"] == 5  # First of multi
    assert sample_hf_dataset[0]["char_pos_phrase_end"] == 10  # First of multi
    assert sample_hf_dataset[0]["char_pos_phrase_multi"] == [[5, 10], [20, 25]]

    # Check second example (without positions)
    assert sample_hf_dataset[1]["text"] == "Another sample text."
    assert sample_hf_dataset[1]["label"] == 0.0
    assert sample_hf_dataset[1]["char_pos_word_start"] is None
    assert sample_hf_dataset[1]["char_pos_word_end"] is None
    assert sample_hf_dataset[1]["char_pos_word_multi"] == []
    assert sample_hf_dataset[1]["char_pos_phrase_start"] is None
    assert sample_hf_dataset[1]["char_pos_phrase_end"] is None
    assert sample_hf_dataset[1]["char_pos_phrase_multi"] == []


def test_probing_dataset_from_hf_dataset(sample_hf_dataset, sample_probing_dataset):
    loaded_dataset = ProbingDataset.from_hf_dataset(
        dataset=sample_hf_dataset,
        position_types=["word", "phrase"],
        label_mapping=sample_probing_dataset.label_mapping,
        task_type=sample_probing_dataset.task_type,
        valid_layers=sample_probing_dataset.valid_layers,
        dataset_attributes=sample_probing_dataset.dataset_attributes,
    )

    assert len(loaded_dataset.examples) == len(sample_probing_dataset.examples)
    assert loaded_dataset.task_type == sample_probing_dataset.task_type
    assert loaded_dataset.label_mapping == sample_probing_dataset.label_mapping
    assert (
        loaded_dataset.dataset_attributes == sample_probing_dataset.dataset_attributes
    )
    assert loaded_dataset.position_types == sample_probing_dataset.position_types

    # Deep comparison of examples (need to handle float label conversion)
    for loaded_ex, orig_ex in zip(
        loaded_dataset.examples, sample_probing_dataset.examples
    ):
        assert loaded_ex.text == orig_ex.text
        assert float(loaded_ex.label) == float(orig_ex.label)  # Compare as float
        assert loaded_ex.label_text == orig_ex.label_text
        assert loaded_ex.group_id == orig_ex.group_id
        # Attributes are not stored/loaded per example in HF dataset
        # Handle None vs empty CharacterPositions due to HF conversion
        if orig_ex.character_positions is None:
            assert loaded_ex.character_positions == CharacterPositions(
                positions={}
            ), f"Expected empty CharacterPositions for None input, got {loaded_ex.character_positions}"
        else:
            assert loaded_ex.character_positions == orig_ex.character_positions


def test_probing_dataset_save_load(sample_probing_dataset, temp_save_dir):
    # Save the dataset
    sample_probing_dataset.save(temp_save_dir)

    # Check if files exist
    assert os.path.exists(f"{temp_save_dir}/hf_dataset")
    assert os.path.exists(f"{temp_save_dir}/dataset_attributes.json")

    # Load the dataset
    loaded_dataset = ProbingDataset.load(temp_save_dir)

    # Perform comparisons (similar to from_hf_dataset test)
    assert len(loaded_dataset.examples) == len(sample_probing_dataset.examples)
    assert loaded_dataset.task_type == sample_probing_dataset.task_type
    assert loaded_dataset.label_mapping == sample_probing_dataset.label_mapping
    assert (
        loaded_dataset.dataset_attributes == sample_probing_dataset.dataset_attributes
    )
    assert loaded_dataset.position_types == sample_probing_dataset.position_types

    # Compare examples
    for loaded_ex, orig_ex in zip(
        loaded_dataset.examples, sample_probing_dataset.examples
    ):
        assert loaded_ex.text == orig_ex.text
        assert float(loaded_ex.label) == float(orig_ex.label)
        assert loaded_ex.label_text == orig_ex.label_text
        assert loaded_ex.group_id == orig_ex.group_id
        # Handle None vs empty CharacterPositions due to HF conversion
        if orig_ex.character_positions is None:
            assert loaded_ex.character_positions == CharacterPositions(
                positions={}
            ), f"Expected empty CharacterPositions for None input, got {loaded_ex.character_positions}"
        else:
            assert loaded_ex.character_positions == orig_ex.character_positions


def test_probing_dataset_save_load_backward_compatibility(
    sample_probing_dataset, temp_save_dir
):
    # Simulate saving with the old metadata format
    sample_probing_dataset.dataset.save_to_disk(f"{temp_save_dir}/hf_dataset")
    old_metadata = {
        "task_type": sample_probing_dataset.task_type,
        "valid_layers": sample_probing_dataset.valid_layers,
        "label_mapping": sample_probing_dataset.label_mapping,
        "metadata": sample_probing_dataset.dataset_attributes,  # Old key name
        "position_types": list(sample_probing_dataset.position_types),
    }
    with open(f"{temp_save_dir}/metadata.json", "w") as f:  # Old filename
        json.dump(old_metadata, f)

    # Load the dataset - should work with the old format
    loaded_dataset = ProbingDataset.load(temp_save_dir)

    # Verify loaded data
    assert (
        loaded_dataset.dataset_attributes == sample_probing_dataset.dataset_attributes
    )
    assert loaded_dataset.position_types == sample_probing_dataset.position_types


def test_probing_dataset_train_test_split(sample_probing_dataset):
    # Create a slightly larger dataset for splitting
    examples = [
        ProbingExample(f"text {i}", i % 2, str(i % 2), None, f"group{i}")
        for i in range(10)
    ]
    dataset = ProbingDataset(
        examples=examples, dataset_attributes=sample_probing_dataset.dataset_attributes
    )

    train_ds, test_ds = dataset.train_test_split(test_size=0.3, seed=42)

    assert isinstance(train_ds, ProbingDataset)
    assert isinstance(test_ds, ProbingDataset)
    assert len(train_ds.examples) == 7
    assert len(test_ds.examples) == 3

    # Check that attributes are preserved
    assert train_ds.dataset_attributes == dataset.dataset_attributes
    assert test_ds.dataset_attributes == dataset.dataset_attributes

    # Check that position types are correctly inferred (should be empty here)
    assert train_ds.position_types == set()
    assert test_ds.position_types == set()

    # Check content consistency: sizes and non-overlapping union = original
    original_texts = {ex.text for ex in examples}
    train_texts = {ex.text for ex in train_ds.examples}
    test_texts = {ex.text for ex in test_ds.examples}
    assert len(train_texts) == 7
    assert len(test_texts) == 3
    assert train_texts.union(test_texts) == original_texts
    assert train_texts.intersection(test_texts) == set()


def test_probing_dataset_split_with_positions(sample_probing_dataset):
    # Create a dataset with positions for splitting
    examples = [
        ProbingExample(
            f"text {i}",
            i % 2,
            str(i % 2),
            CharacterPositions({"num": Position(0, 1)}),
            f"group{i}",
        )
        for i in range(10)
    ]
    dataset = ProbingDataset(
        examples=examples, dataset_attributes=sample_probing_dataset.dataset_attributes
    )
    assert dataset.position_types == {"num"}

    train_ds, test_ds = dataset.train_test_split(test_size=0.3, seed=42)

    assert train_ds.position_types == {"num"}
    assert test_ds.position_types == {"num"}

    # The essential checks are that position_types are preserved.
    # Checking specific examples in the split is brittle.
    # test_example_texts = {ex.text for ex in test_ds.examples}
    # assert "text 1" in test_example_texts
    # test_ex_1 = next(ex for ex in test_ds.examples if ex.text == "text 1")
    # assert (
    #     test_ex_1.character_positions is not None
    # ), f"Character positions are None for example: {test_ex_1}"
    # assert test_ex_1.character_positions["num"] == Position(0, 1)



================================================
FILE: tests/unit/datasets/test_position_finder.py
================================================
import pytest
from transformers import AutoTokenizer

# Assuming Position and PositionFinder are in probity.datasets.position_finder
from probity.datasets.position_finder import Position, PositionFinder

# --- Fixtures ---


@pytest.fixture(scope="module")
def tokenizer():
    """Provides a tokenizer instance for tests."""
    # Using a simple tokenizer like gpt2 for testing purposes
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token  # Standard practice
    return tokenizer


# --- Test Position Class ---


def test_position_initialization():
    """Test valid Position initialization."""
    pos1 = Position(start=5)
    assert pos1.start == 5
    assert pos1.end is None

    pos2 = Position(start=10, end=15)
    assert pos2.start == 10
    assert pos2.end == 15


def test_position_invalid_start():
    """Test Position validation for negative start."""
    with pytest.raises(ValueError, match="Start position must be non-negative"):
        Position(start=-1)


def test_position_invalid_end():
    """Test Position validation for end < start."""
    with pytest.raises(
        ValueError, match="End position must be greater than start position"
    ):
        Position(start=10, end=5)


# --- Test PositionFinder Static Methods ---


# Test from_template
def test_positionfinder_from_template_simple():
    """Test finding position from a simple template."""
    template = "The color is {COLOR}."
    marker = "{COLOR}"
    finder = PositionFinder.from_template(template, marker)
    text = "The color is blue."
    position = finder(text)
    assert isinstance(position, Position)
    assert position.start == 13  # Start of 'blue'
    assert position.end == 17  # End of 'blue'
    assert text[position.start : position.end] == "blue"


def test_positionfinder_from_template_complex():
    """Test finding position from a template with multiple variables."""
    template = "{PERSON} likes {FOOD}."
    marker = "{PERSON}"
    finder = PositionFinder.from_template(template, marker)
    text = "Alice likes pizza."
    position = finder(text)
    assert position.start == 0  # Start of 'Alice'
    assert position.end == 5  # End of 'Alice'
    assert text[position.start : position.end] == "Alice"

    marker_food = "{FOOD}"
    finder_food = PositionFinder.from_template(template, marker_food)
    position_food = finder_food(text)
    assert position_food.start == 12  # Start of 'pizza'
    assert position_food.end == 17  # End of 'pizza'
    assert text[position_food.start : position_food.end] == "pizza"


def test_positionfinder_from_template_no_match():
    """Test template finder when text doesn't match."""
    template = "Value: {VALUE}"
    marker = "{VALUE}"
    finder = PositionFinder.from_template(template, marker)
    text = "This is different"
    with pytest.raises(ValueError, match="Text does not match template"):
        finder(text)


# Test from_regex
def test_positionfinder_from_regex_single_match():
    """Test finding a single regex match."""
    pattern = r"\b\d{3}\b"  # Find 3-digit numbers
    finder = PositionFinder.from_regex(pattern)
    text = "Agent 007 is here."
    positions = finder(text)
    assert len(positions) == 1
    assert positions[0].start == 6
    assert positions[0].end == 9
    assert text[positions[0].start : positions[0].end] == "007"


def test_positionfinder_from_regex_multiple_matches():
    """Test finding multiple regex matches."""
    pattern = r"cat"
    finder = PositionFinder.from_regex(pattern)
    text = "The cat sat on the catalog."
    positions = finder(text)
    assert len(positions) == 2
    assert positions[0].start == 4
    assert positions[0].end == 7
    assert text[positions[0].start : positions[0].end] == "cat"
    assert positions[1].start == 19
    assert positions[1].end == 22
    assert text[positions[1].start : positions[1].end] == "cat"


def test_positionfinder_from_regex_capture_group():
    """Test finding a specific capture group with regex."""
    pattern = r"Name: (\w+)"
    finder = PositionFinder.from_regex(pattern, group=1)  # Capture group 1
    text = "ID: 123 Name: Bob Age: 30"
    positions = finder(text)
    assert len(positions) == 1
    assert positions[0].start == 14  # Start of 'Bob'
    assert positions[0].end == 17  # End of 'Bob'
    assert text[positions[0].start : positions[0].end] == "Bob"


def test_positionfinder_from_regex_no_match():
    """Test regex finder when pattern doesn't match."""
    pattern = r"xyz"
    finder = PositionFinder.from_regex(pattern)
    text = "abc def ghi"
    positions = finder(text)
    assert len(positions) == 0


# Test from_char_position
def test_positionfinder_from_char_position_valid():
    """Test finding a valid fixed character position."""
    finder = PositionFinder.from_char_position(5)
    text = "0123456789"
    position = finder(text)
    assert isinstance(position, Position)
    assert position.start == 5
    assert position.end is None  # Single character position


def test_positionfinder_from_char_position_invalid():
    """Test finding an out-of-bounds character position."""
    finder = PositionFinder.from_char_position(10)
    text = "0123456789"  # Length 10
    with pytest.raises(ValueError, match="Position 10 is beyond text length 10"):
        finder(text)

    finder_long = PositionFinder.from_char_position(15)
    with pytest.raises(ValueError, match="Position 15 is beyond text length 10"):
        finder_long(text)


# Test convert_to_token_position
def test_convert_to_token_position_no_special(tokenizer):
    """Test converting char pos to token pos without special tokens."""
    text = "This is a test sentence."
    # Tokenization: ['This', 'Ġis', 'Ġa', 'Ġtest', 'Ġsentence', '.']
    # Offsets:    [(0, 4), (4, 7), (7, 9), (9, 14), (14, 23), (23, 24)]

    # Position within "test"
    char_pos = Position(start=10)  # Character 'e' in 'test'
    token_idx = PositionFinder.convert_to_token_position(
        char_pos, text, tokenizer, add_special_tokens=False
    )
    assert token_idx == 3  # Index of 'Ġtest'

    # Position at the start of "is"
    char_pos_start = Position(start=5)  # Character 'i' in 'is'
    token_idx_start = PositionFinder.convert_to_token_position(
        char_pos_start, text, tokenizer, add_special_tokens=False
    )
    assert token_idx_start == 1  # Index of 'Ġis'


def test_convert_to_token_position_with_special_bos(tokenizer):
    """Test converting char pos to token pos with BOS special token."""
    text = "Hello world."
    # Assuming GPT2 adds BOS: ['<|endoftext|>', 'Hello', 'Ġworld', '.']
    # Clean Tokens: ['Hello', 'Ġworld', '.']
    # Clean Offsets: [(0, 5), (5, 11), (11, 12)]

    # Position within "world"
    char_pos = Position(start=7)  # Character 'o' in 'world'

    # Find clean index first (should be 1 for 'Ġworld')
    clean_encoding = tokenizer(
        text, return_offsets_mapping=True, add_special_tokens=False
    )
    clean_token_idx_check = -1
    for idx, (start, end) in enumerate(clean_encoding.offset_mapping):
        if start <= char_pos.start < end:
            clean_token_idx_check = idx
            break
    assert clean_token_idx_check == 1

    # Now convert with special tokens
    token_idx = PositionFinder.convert_to_token_position(
        char_pos, text, tokenizer, add_special_tokens=True
    )

    # Expecting index 2: ['<|endoftext|>', 'Hello', 'Ġworld', '.']
    # Check if tokenizer actually adds BOS/EOS. GPT2 uses EOS as BOS if not specified.
    prefix_tokens = 0
    if getattr(tokenizer, "bos_token_id", None) is not None:
        prefix_tokens += 1
    elif getattr(tokenizer, "cls_token_id", None) is not None:
        prefix_tokens += 1
    # GPT2 uses eos_token as bos_token if bos_token is not set
    elif (
        getattr(tokenizer, "eos_token_id", None) is not None
        and getattr(tokenizer, "bos_token_id", None) is None
    ):
        prefix_tokens += 1

    expected_token_idx = clean_token_idx_check + prefix_tokens
    assert token_idx == expected_token_idx


def test_convert_to_token_position_char_not_aligned(tokenizer):
    """Test conversion when char pos doesn't align with token boundaries."""
    text = "Example sentence."
    # Example: Sometimes a space might not be part of a token offset map
    # This test is slightly contrived as tokenizers usually cover all chars.
    # We simulate by choosing a position *between* tokens if possible,
    # although `char_offset_mapping` usually prevents this exact scenario.
    # Let's test the error raising if the loop completes without finding a match.

    # Use a position clearly outside any valid token span
    # Mock the offset mapping to ensure no match
    class MockTokenizer:
        def __call__(
            self, text, return_offsets_mapping=False, add_special_tokens=False
        ):
            if add_special_tokens is False and return_offsets_mapping:
                # Simulate offsets that *don't* cover position 5
                return {
                    "input_ids": [0, 1, 2],
                    "offset_mapping": [(0, 2), (3, 5), (6, 8)],
                }
            elif add_special_tokens is True:
                # Simulate adding a BOS token
                return {"input_ids": [100, 0, 1, 2]}
            else:
                return {"input_ids": [0, 1, 2]}

        bos_token_id = 100
        padding_side = "right"

    mock_tokenizer = MockTokenizer()
    char_pos = Position(
        start=5
    )  # Position 5 is not covered by [(0, 2), (3, 5), (6, 8)]

    with pytest.raises(
        ValueError, match="Character position 5 not aligned with any token offset."
    ):
        PositionFinder.convert_to_token_position(
            char_pos, text, mock_tokenizer, add_special_tokens=False  # type: ignore
        )

    with pytest.raises(
        ValueError, match="Character position 5 not aligned with any token offset."
    ):
        PositionFinder.convert_to_token_position(
            char_pos, text, mock_tokenizer, add_special_tokens=True  # type: ignore
        )


# Test validate_token_position
def test_validate_token_position_valid():
    """Test validating valid token positions."""
    tokens = [101, 1000, 2000, 3000, 102]  # Length 5
    assert PositionFinder.validate_token_position(0, tokens) is True  # Start
    assert PositionFinder.validate_token_position(2, tokens) is True  # Middle
    assert PositionFinder.validate_token_position(4, tokens) is True  # End - 1


def test_validate_token_position_invalid():
    """Test validating invalid token positions."""
    tokens = [101, 1000, 2000, 3000, 102]  # Length 5
    assert PositionFinder.validate_token_position(-1, tokens) is False  # Negative
    assert PositionFinder.validate_token_position(5, tokens) is False  # Equal to length
    assert (
        PositionFinder.validate_token_position(6, tokens) is False
    )  # Greater than length
    assert PositionFinder.validate_token_position(0, []) is False  # Empty list



================================================
FILE: tests/unit/datasets/test_templated.py
================================================
import pytest
from probity.datasets.templated import (
    TemplateVariable,
    Template,
    TemplatedDataset,
)
from probity.datasets.base import ProbingDataset, ProbingExample
from probity.datasets.position_finder import Position


# Sample data for testing
adj_var = TemplateVariable(
    name="ADJ",
    values=["good", "bad", "great", "terrible"],
    attributes={"sentiment": ["positive", "negative", "positive", "negative"]},
    class_bound=True,
    class_key="sentiment",
)
verb_var = TemplateVariable(
    name="VERB",
    values=["loved", "hated", "enjoyed", "disliked"],
    attributes={"sentiment": ["positive", "negative", "positive", "negative"]},
    class_bound=True,
    class_key="sentiment",
)
noun_var = TemplateVariable(name="NOUN", values=["movie", "film"], attributes=None)

template1 = Template(
    template="I thought this {NOUN} was {ADJ}, I {VERB} it.",
    variables={"ADJ": adj_var, "VERB": verb_var, "NOUN": noun_var},
    attributes={"task": "sentiment"},
)

template_no_bound = Template(
    template="{ADJ} {NOUN}",
    variables={"ADJ": adj_var, "NOUN": noun_var},
)


# --- Test TemplateVariable ---
def test_template_variable_init():
    var = TemplateVariable(name="TEST", values=["a", "b"])
    assert var.name == "TEST"
    assert var.values == ["a", "b"]
    assert var.attributes is None
    assert not var.class_bound
    assert var.class_key is None


def test_template_variable_init_with_attrs():
    var = TemplateVariable(
        name="TEST",
        values=["a", "b"],
        attributes={"type": ["X", "Y"]},
        class_bound=True,
        class_key="type",
    )
    assert var.name == "TEST"
    assert var.attributes == {"type": ["X", "Y"]}
    assert var.class_bound
    assert var.class_key == "type"


# --- Test Template ---
def test_template_init():
    t = Template(template="{VAR}", variables={"VAR": adj_var})
    assert t.template == "{VAR}"
    assert t.variables == {"VAR": adj_var}
    assert t.attributes is None


def test_template_get_marker():
    assert template1.get_marker("ADJ") == "{ADJ}"


def test_template_get_all_markers():
    assert template1.get_all_markers() == {
        "ADJ": "{ADJ}",
        "VERB": "{VERB}",
        "NOUN": "{NOUN}",
    }


def test_template_validate_valid():
    assert template1.validate()


def test_template_validate_invalid_missing_var():
    invalid_template = Template(template="{ADJ} {MISSING}", variables={"ADJ": adj_var})
    assert not invalid_template.validate()


def test_template_validate_invalid_extra_var():
    invalid_template = Template(
        template="{ADJ}", variables={"ADJ": adj_var, "EXTRA": noun_var}
    )
    assert not invalid_template.validate()


# --- Test TemplatedDataset ---
def test_templated_dataset_init_valid():
    ds = TemplatedDataset(templates=[template1])
    assert ds.templates == [template1]
    assert ds.attributes == {}


def test_templated_dataset_init_invalid_template():
    invalid_template = Template(template="{ADJ} {MISSING}", variables={"ADJ": adj_var})
    with pytest.raises(ValueError):
        TemplatedDataset(templates=[invalid_template])


def test_to_probing_dataset_basic():
    ds = TemplatedDataset(templates=[template1])
    probing_ds = ds.to_probing_dataset(auto_add_positions=False)
    assert isinstance(probing_ds, ProbingDataset)
    # Expected examples: 2 (NOUN) * (2 pos * 2 pos + 2 neg * 2 neg) = 2 * (4 + 4) = 16
    assert len(probing_ds.examples) == 16


def test_to_probing_dataset_with_labels():
    ds = TemplatedDataset(templates=[template1])
    probing_ds = ds.to_probing_dataset(
        label_from_attributes="sentiment",
        label_map={"positive": 1, "negative": 0},
        auto_add_positions=False,
    )
    assert len(probing_ds.examples) == 16
    positive_count = sum(1 for ex in probing_ds.examples if ex.label == 1)
    negative_count = sum(1 for ex in probing_ds.examples if ex.label == 0)
    # Each noun (movie, film) combines with (pos_adj, pos_verb) and (neg_adj, neg_verb)
    # pos combinations = 2 adj * 2 verb = 4 per noun -> 8 total positive
    # neg combinations = 2 adj * 2 verb = 4 per noun -> 8 total negative
    assert positive_count == 8
    assert negative_count == 8
    assert probing_ds.examples[0].label_text in ["positive", "negative"]


def test_to_probing_dataset_attribute_slicing():
    """Verify that variable attributes are correctly sliced per example."""
    ds = TemplatedDataset(templates=[template1])
    probing_ds = ds.to_probing_dataset(
        label_from_attributes="sentiment",
        label_map={"positive": 1, "negative": 0},
        auto_add_positions=False,
    )
    # Check an example
    example = next(
        ex for ex in probing_ds.examples if "I thought this movie was good" in ex.text
    )  # Should be positive
    assert example is not None
    assert example.label == 1
    assert example.attributes["class"] == "positive"
    assert example.attributes["variables"]["ADJ"] == {"sentiment": "positive"}
    assert example.attributes["variables"]["VERB"] == {"sentiment": "positive"}
    assert (
        example.attributes["variables"]["NOUN"] is None
    )  # No attributes defined for NOUN


def test_to_probing_dataset_auto_positions():
    ds = TemplatedDataset(templates=[template1])
    probing_ds = ds.to_probing_dataset(auto_add_positions=True)
    assert len(probing_ds.examples) == 16
    assert probing_ds.position_types == {"ADJ", "VERB", "NOUN"}
    example = probing_ds.examples[0]
    # Explicitly check keys to avoid potential issues with `in` on custom class
    assert example.character_positions is not None
    assert "ADJ" in example.character_positions.keys()
    assert "VERB" in example.character_positions.keys()
    assert "NOUN" in example.character_positions.keys()
    # Check position calculation for a specific example
    # Example: "I thought this movie was good, I loved it."
    example = next(
        ex
        for ex in probing_ds.examples
        if ex.text == "I thought this movie was good, I loved it."
    )
    adj_pos = example.character_positions["ADJ"]
    verb_pos = example.character_positions["VERB"]
    noun_pos = example.character_positions["NOUN"]
    assert isinstance(adj_pos, Position)
    assert isinstance(verb_pos, Position)
    assert isinstance(noun_pos, Position)
    assert example.text[adj_pos.start : adj_pos.end] == "good"
    assert example.text[verb_pos.start : verb_pos.end] == "loved"
    assert example.text[noun_pos.start : noun_pos.end] == "movie"


def test_to_probing_dataset_no_bound_vars():
    # Create a template without class-bound variables
    adj_no_bound = TemplateVariable(name="ADJ", values=["hot", "cold"])
    noun_no_bound = TemplateVariable(name="NOUN", values=["day", "night"])
    template_nb = Template(
        template="It is a {ADJ} {NOUN}.",
        variables={"ADJ": adj_no_bound, "NOUN": noun_no_bound},
    )
    ds = TemplatedDataset(templates=[template_nb])
    probing_ds = ds.to_probing_dataset(auto_add_positions=False)
    # Expected: 2 adj * 2 noun = 4 examples
    assert len(probing_ds.examples) == 4
    texts = {ex.text for ex in probing_ds.examples}
    assert texts == {
        "It is a hot day.",
        "It is a hot night.",
        "It is a cold day.",
        "It is a cold night.",
    }
    # No class should be assigned if no class_bound vars
    assert all(ex.attributes["class"] is None for ex in probing_ds.examples)


def test_from_movie_sentiment_template():
    adjectives = {"positive": ["great", "excellent"], "negative": ["bad", "awful"]}
    verbs = {"positive": ["loved", "adored"], "negative": ["hated", "despised"]}
    ds = TemplatedDataset.from_movie_sentiment_template(adjectives, verbs)
    assert len(ds.templates) == 1
    template = ds.templates[0]
    assert template.template == "I thought this movie was {ADJ}, I {VERB} it."
    assert "ADJ" in template.variables
    assert "VERB" in template.variables
    assert template.variables["ADJ"].class_bound
    assert template.variables["VERB"].class_bound
    probing_ds = ds.to_probing_dataset(
        label_from_attributes="sentiment",
        label_map={"positive": 1, "negative": 0},
        auto_add_positions=False,
    )
    # Expected: 2 pos_adj * 2 pos_verb + 2 neg_adj * 2 neg_verb = 4 + 4 = 8
    assert len(probing_ds.examples) == 8
    positive_count = sum(1 for ex in probing_ds.examples if ex.label == 1)
    negative_count = sum(1 for ex in probing_ds.examples if ex.label == 0)
    assert positive_count == 4
    assert negative_count == 4


def test_from_mood_story_template():
    names = ["Alice", "Bob"]
    verbs = {"positive": ["loves", "enjoys"], "negative": ["hates", "dislikes"]}
    ds = TemplatedDataset.from_mood_story_template(names, verbs)
    assert len(ds.templates) == 1
    template = ds.templates[0]
    assert template.template == "{NAME} {VERB} parties, and does so whenever possible."
    assert "NAME" in template.variables
    assert "VERB" in template.variables
    assert not template.variables["NAME"].class_bound
    assert not template.variables[
        "VERB"
    ].class_bound  # NOTE: In this factory, VERB isn't class-bound in the *template* definition itself
    probing_ds = ds.to_probing_dataset(auto_add_positions=False)
    # Expected: 2 names * 4 verbs = 8 examples
    assert len(probing_ds.examples) == 8
    texts = {ex.text for ex in probing_ds.examples}
    expected_texts = {
        f"{name} {verb} parties, and does so whenever possible."
        for name in names
        for verb_list in verbs.values()
        for verb in verb_list
    }
    assert texts == expected_texts
    # Verify verb attributes are stored correctly in the example
    example_alice_loves = next(
        ex for ex in probing_ds.examples if "Alice loves" in ex.text
    )
    assert (
        example_alice_loves.attributes["variables"]["VERB"]["sentiment"] == "positive"
    )
    example_bob_hates = next(ex for ex in probing_ds.examples if "Bob hates" in ex.text)
    assert example_bob_hates.attributes["variables"]["VERB"]["sentiment"] == "negative"



================================================
FILE: tests/unit/datasets/test_tokenized.py
================================================
import pytest
import torch
from transformers import AutoTokenizer, PreTrainedTokenizerFast
from probity.datasets.base import ProbingDataset, ProbingExample, CharacterPositions
from probity.datasets.position_finder import Position, PositionFinder
from probity.datasets.tokenized import (
    TokenizedProbingDataset,
    TokenizedProbingExample,
    TokenizationConfig,
    TokenPositions,
)
import dataclasses
import tempfile
import os
import shutil
import datasets  # Added import for datasets.Dataset
from typing import cast


# Fixtures
@pytest.fixture(
    scope="module"
)  # Use module scope for tokenizer fixtures to load only once
def tokenizer_right_pad() -> PreTrainedTokenizerFast:
    """Fixture for a right-padding gpt2 tokenizer."""
    tokenizer = cast(
        PreTrainedTokenizerFast,
        AutoTokenizer.from_pretrained("gpt2", use_fast=True),
    )
    tokenizer.padding_side = "right"
    # GPT2 uses EOS token for padding if pad_token is not set
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


@pytest.fixture(scope="module")
def tokenizer_left_pad() -> PreTrainedTokenizerFast:
    """Fixture for a left-padding gpt2 tokenizer."""
    tokenizer = cast(
        PreTrainedTokenizerFast,
        AutoTokenizer.from_pretrained("gpt2", use_fast=True),
    )
    # Important: Must explicitly set padding_side on instance
    tokenizer.padding_side = "left"
    # GPT2 uses EOS token for padding if pad_token is not set
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


@pytest.fixture
def simple_probing_dataset() -> ProbingDataset:
    """Fixture for a simple ProbingDataset."""
    examples = [
        ProbingExample(
            text="This is the first example.",  # len=27
            label=0,
            label_text="class_0",
            character_positions=CharacterPositions(
                {"FIRST_WORD": Position(0, 4)}
            ),  # "This"
            attributes={"id": "ex1"},
        ),
        ProbingExample(
            text="A second, longer example here.",  # len=30
            label=1,
            label_text="class_1",
            character_positions=CharacterPositions(
                {"FIRST_WORD": Position(0, 1)}
            ),  # "A"
            attributes={"id": "ex2"},
        ),
        ProbingExample(
            text="Third.",  # len=6
            label=0,
            label_text="class_0",
            # No character positions
            attributes={"id": "ex3"},
        ),
    ]
    return ProbingDataset(
        examples=examples,
        task_type="classification",
        label_mapping={"class_0": 0, "class_1": 1},
        dataset_attributes={"name": "simple_test_set"},
    )


@pytest.fixture
def dataset_with_list_pos() -> ProbingDataset:
    """Fixture for a ProbingDataset with list positions."""
    examples = [
        ProbingExample(
            text="Multiple targets here and here.",  # len=31
            label=1,
            label_text="multi",
            character_positions=CharacterPositions(
                {"TARGET": [Position(9, 16), Position(26, 30)]}  # "targets", "here"
            ),
        ),
    ]
    return ProbingDataset(
        examples=examples, task_type="classification", label_mapping={"multi": 1}
    )


# Helper to get padding length
def get_pad_length(
    text: str,
    tokenizer: PreTrainedTokenizerFast,
    max_length: int,
    add_special_tokens: bool,
) -> int:
    """Calculates padding length for a given text."""
    tokens = tokenizer.encode(text, add_special_tokens=add_special_tokens)
    return max(0, max_length - len(tokens))


# Test Cases
def test_initialization():
    config = TokenizationConfig(
        tokenizer_name="test-tokenizer",
        tokenizer_kwargs={},
        vocab_size=100,
        pad_token_id=0,
        eos_token_id=1,
        bos_token_id=None,
        padding_side="right",
    )
    # Ensure CharacterPositions takes dict[str, Union[Position, List[Position]]]
    char_pos = CharacterPositions({"test": Position(0, 1)})
    examples = [
        TokenizedProbingExample(
            text="test",
            tokens=[10, 20],
            label=0,
            label_text="test_label",
            character_positions=char_pos,
        )
    ]
    dataset = TokenizedProbingDataset(
        examples=examples, tokenization_config=config, task_type="classification"
    )
    assert len(dataset) == 1
    assert isinstance(dataset.examples[0], TokenizedProbingExample)
    assert dataset.tokenization_config == config


def test_initialization_validation_fail():
    config = TokenizationConfig(
        tokenizer_name="test-tokenizer",
        tokenizer_kwargs={},
        vocab_size=100,
        pad_token_id=0,
        eos_token_id=1,
        bos_token_id=None,
        padding_side="right",
    )
    # Example with invalid token ID (150 >= vocab_size 100)
    examples = [
        TokenizedProbingExample(
            text="test", tokens=[10, 150], label=0, label_text="fail_label"
        )
    ]
    with pytest.raises(ValueError, match="Invalid token ID found"):
        TokenizedProbingDataset(
            examples=examples, tokenization_config=config, task_type="classification"
        )


def test_from_probing_dataset_right_pad(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted max_length for GPT2 tokenization if needed
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,  # Ensure tensors aren't returned by tokenizer directly
    )

    assert len(tokenized_dataset) == 3
    assert isinstance(tokenized_dataset, TokenizedProbingDataset)
    assert tokenized_dataset.position_types == {"FIRST_WORD"}
    assert (
        tokenized_dataset.tokenization_config.tokenizer_name
        == tokenizer_right_pad.name_or_path
    )
    assert tokenized_dataset.tokenization_config.padding_side == "right"
    assert (
        tokenized_dataset.tokenization_config.pad_token_id
        == tokenizer_right_pad.pad_token_id
    )
    assert "max_length" in tokenized_dataset.tokenization_config.tokenizer_kwargs
    assert (
        tokenized_dataset.tokenization_config.tokenizer_kwargs["max_length"]
        == max_length
    )
    assert (
        tokenized_dataset.tokenization_config.tokenizer_kwargs["add_special_tokens"]
        is add_special
    )

    # Check example 1: "This is the first example."
    ex1 = cast(TokenizedProbingExample, tokenized_dataset.examples[0])
    assert isinstance(ex1, TokenizedProbingExample)
    assert len(ex1.tokens) == max_length
    assert ex1.attention_mask is not None
    assert len(ex1.attention_mask) == max_length
    assert ex1.attention_mask[-1] == 0  # Check padding
    # GPT2 adds BOS token when add_special_tokens=True - Check if this holds
    # assert ex1.tokens[0] == tokenizer_right_pad.bos_token_id # Commenting out BOS check
    assert ex1.token_positions is not None
    assert "FIRST_WORD" in ex1.token_positions.keys()
    # Character pos (0, 4) -> "This" -> token position 0 (assuming no BOS)
    assert ex1.token_positions["FIRST_WORD"] == 0

    # Check example 2: "A second, longer example here."
    ex2 = cast(TokenizedProbingExample, tokenized_dataset.examples[1])
    assert len(ex2.tokens) == max_length
    assert ex2.attention_mask is not None
    assert ex2.token_positions is not None
    assert "FIRST_WORD" in ex2.token_positions.keys()
    # Character pos (0, 1) -> " A" -> token position 0 (assuming no BOS)
    assert ex2.token_positions["FIRST_WORD"] == 0

    # Check example 3: "Third." (no character positions)
    ex3 = cast(TokenizedProbingExample, tokenized_dataset.examples[2])
    assert len(ex3.tokens) == max_length
    assert ex3.attention_mask is not None
    # If positions were defined in the original dataset, TokenPositions should be created
    # If NO positions were defined for this example, token_positions should be None
    assert ex3.token_positions is None


def test_from_probing_dataset_left_pad(simple_probing_dataset, tokenizer_left_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_left_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    assert len(tokenized_dataset) == 3
    assert tokenized_dataset.tokenization_config.padding_side == "left"
    assert (
        tokenized_dataset.tokenization_config.pad_token_id
        == tokenizer_left_pad.pad_token_id
    )

    # Check example 1: "This is the first example."
    ex1 = cast(TokenizedProbingExample, tokenized_dataset.examples[0])
    assert len(ex1.tokens) == max_length
    assert ex1.attention_mask is not None
    assert ex1.tokens[0] == tokenizer_left_pad.pad_token_id  # Check padding
    # Find first non-pad token
    first_real_token_index = next(
        (i for i, t in enumerate(ex1.tokens) if t != tokenizer_left_pad.pad_token_id),
        -1,
    )
    assert first_real_token_index != -1
    # First real token should be BOS for GPT2 - Check if this holds
    # assert ex1.tokens[first_real_token_index] == tokenizer_left_pad.bos_token_id # Commenting out BOS check
    assert ex1.attention_mask[0] == 0  # Check attention mask padding
    assert ex1.token_positions is not None
    assert "FIRST_WORD" in ex1.token_positions.keys()
    # "This" is token 1 in the unpadded sequence [BOS] This is the first example . [EOS]
    pad_len = get_pad_length(ex1.text, tokenizer_left_pad, max_length, add_special)

    # Dynamically calculate expected position based on whether BOS was actually added
    unpadded_pos = 0  # Assuming position 0 if no BOS
    unpadded_tokens_no_special = tokenizer_left_pad.encode(
        ex1.text, add_special_tokens=False
    )
    unpadded_tokens_with_special = tokenizer_left_pad.encode(
        ex1.text, add_special_tokens=True
    )
    if (
        len(unpadded_tokens_with_special) > len(unpadded_tokens_no_special)
        and unpadded_tokens_with_special[0] == tokenizer_left_pad.bos_token_id
    ):
        unpadded_pos = 1  # Position is 1 if BOS was added

    # Expected position = unpadded_pos + pad_len
    expected_pos = unpadded_pos + pad_len
    assert ex1.token_positions["FIRST_WORD"] == expected_pos

    # Check example 3: "Third." (shortest)
    ex3 = cast(TokenizedProbingExample, tokenized_dataset.examples[2])
    assert len(ex3.tokens) == max_length
    assert ex3.attention_mask is not None
    assert ex3.tokens[0] == tokenizer_left_pad.pad_token_id
    assert ex3.token_positions is None  # No positions defined


def test_from_probing_dataset_list_pos(dataset_with_list_pos, tokenizer_right_pad):
    max_length = 20
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=dataset_with_list_pos,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )
    assert len(tokenized_dataset) == 1
    ex = cast(TokenizedProbingExample, tokenized_dataset.examples[0])
    assert ex.token_positions is not None
    assert "TARGET" in ex.token_positions.keys()
    assert isinstance(ex.token_positions["TARGET"], list)
    # Text: "Multiple targets here and here."
    # GPT2 Tokens (approx): [BOS] Multiple targets here and here . [EOS] [PAD] ...
    # "targets" -> char (9, 16) -> might be multiple tokens, need dynamic check
    # "here" -> char (26, 30) -> might be one token

    # Dynamic check for positions
    pos_targets = PositionFinder.convert_to_token_position(
        Position(9, 16), ex.text, tokenizer_right_pad, add_special_tokens=add_special
    )
    pos_here = PositionFinder.convert_to_token_position(
        Position(26, 30), ex.text, tokenizer_right_pad, add_special_tokens=add_special
    )
    # Assuming convert_to_token_position returns the first token index if span covers multiple
    expected_positions = [pos_targets, pos_here]

    assert ex.token_positions["TARGET"] == expected_positions


def test_get_batch_tensors_right_pad(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    batch = tokenized_dataset.get_batch_tensors(indices=[0, 1], pad=True)

    assert "input_ids" in batch
    assert "attention_mask" in batch
    assert "positions" in batch
    # Cast to Tensor for shape check
    input_ids_tensor = cast(torch.Tensor, batch["input_ids"])
    attention_mask_tensor = cast(torch.Tensor, batch["attention_mask"])
    assert isinstance(input_ids_tensor, torch.Tensor)
    assert isinstance(attention_mask_tensor, torch.Tensor)
    assert isinstance(batch["positions"], dict)

    # Shape check: (batch_size, sequence_length)
    assert input_ids_tensor.shape == (2, max_length)
    assert attention_mask_tensor.shape == (2, max_length)

    # Position check
    assert "FIRST_WORD" in batch["positions"]
    assert isinstance(batch["positions"]["FIRST_WORD"], torch.Tensor)
    # Expected positions [0, 0] for "This" and " A" (token index 0)
    assert torch.equal(batch["positions"]["FIRST_WORD"], torch.tensor([0, 0]))

    # Padding check (right padding)
    assert input_ids_tensor[0, -1] == tokenizer_right_pad.pad_token_id
    assert attention_mask_tensor[0, -1] == 0


def test_get_batch_tensors_left_pad(simple_probing_dataset, tokenizer_left_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_left_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    # Indices [0, 2] -> "This is the first example." and "Third."
    indices = [0, 2]
    batch = tokenized_dataset.get_batch_tensors(indices=indices, pad=True)

    assert "input_ids" in batch
    assert "attention_mask" in batch
    assert "positions" in batch

    # Cast to Tensor for shape and indexing checks
    input_ids_tensor = cast(torch.Tensor, batch["input_ids"])
    attention_mask_tensor = cast(torch.Tensor, batch["attention_mask"])

    # Shape check
    assert input_ids_tensor.shape == (2, max_length)
    assert attention_mask_tensor.shape == (2, max_length)

    # Padding check (left padding)
    assert (
        input_ids_tensor[0, 0] == tokenizer_left_pad.pad_token_id
    )  # Example 0 should have padding
    assert attention_mask_tensor[0, 0] == 0
    assert (
        input_ids_tensor[1, 0] == tokenizer_left_pad.pad_token_id
    )  # Example 2 (shorter) should have more padding
    assert attention_mask_tensor[1, 0] == 0

    # Position check (needs adjustment for left padding)
    assert "FIRST_WORD" in batch["positions"]
    # Example 0: "This" is token 0 in unpadded.
    ex0_text = simple_probing_dataset.examples[0].text
    assert ex0_text is not None  # Ensure text is not None for type checker
    ex0_pad_len = get_pad_length(ex0_text, tokenizer_left_pad, max_length, add_special)
    ex0_expected_pos = 0 + ex0_pad_len  # unpadded_pos = 0

    # Example 2: Has no "FIRST_WORD" position. Should default to 0.
    ex2_expected_pos = 0

    # Check implementation handles missing keys by defaulting to 0
    assert torch.equal(
        batch["positions"]["FIRST_WORD"],
        torch.tensor([ex0_expected_pos, ex2_expected_pos]),
    )


def test_save_load_cycle(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted
    add_special = True
    original_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    with tempfile.TemporaryDirectory() as tmpdir:
        save_path = os.path.join(tmpdir, "test_dataset")
        original_dataset.save(save_path)

        assert os.path.exists(save_path)
        assert os.path.exists(os.path.join(save_path, "tokenization_config.json"))
        assert os.path.exists(os.path.join(save_path, "dataset_attributes.json"))
        assert os.path.exists(os.path.join(save_path, "hf_dataset"))

        loaded_dataset = TokenizedProbingDataset.load(save_path)

        assert isinstance(loaded_dataset, TokenizedProbingDataset)
        assert len(loaded_dataset) == len(original_dataset)
        assert (
            loaded_dataset.tokenization_config == original_dataset.tokenization_config
        )
        assert loaded_dataset.task_type == original_dataset.task_type
        assert loaded_dataset.label_mapping == original_dataset.label_mapping
        assert loaded_dataset.position_types == original_dataset.position_types

        # Deep comparison of examples
        for i in range(len(original_dataset)):
            orig_ex = cast(TokenizedProbingExample, original_dataset.examples[i])
            load_ex = cast(TokenizedProbingExample, loaded_dataset.examples[i])

            assert isinstance(load_ex, TokenizedProbingExample)
            # Compare all relevant fields
            assert load_ex.text == orig_ex.text
            assert load_ex.label == orig_ex.label
            assert load_ex.label_text == orig_ex.label_text
            assert load_ex.attributes == orig_ex.attributes
            assert load_ex.tokens == orig_ex.tokens
            assert load_ex.attention_mask == orig_ex.attention_mask

            # Compare token positions carefully (handle None vs TokenPositions({}) potentially)
            if orig_ex.token_positions is None:
                assert load_ex.token_positions is None
            else:
                assert load_ex.token_positions is not None
                assert (
                    load_ex.token_positions.positions
                    == orig_ex.token_positions.positions
                )

        # Test if loaded dataset can still produce batches
        batch = loaded_dataset.get_batch_tensors(indices=[0, 1], pad=True)
        input_ids_tensor = cast(torch.Tensor, batch["input_ids"])
        assert input_ids_tensor.shape == (2, max_length)


def test_validate_positions(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )
    assert tokenized_dataset.validate_positions()

    # Manually create an invalid position (index >= seq_len)
    ex0 = cast(TokenizedProbingExample, tokenized_dataset.examples[0])
    invalid_pos_val = len(ex0.tokens)  # Index must be < length
    invalid_ex = dataclasses.replace(
        ex0,
        token_positions=TokenPositions(
            {"FIRST_WORD": invalid_pos_val}
        ),  # Invalid position
    )
    tokenized_dataset.examples[0] = invalid_ex  # type: ignore  # Overwrite with invalid example
    assert not tokenized_dataset.validate_positions()

    # Test negative position
    ex1 = cast(TokenizedProbingExample, tokenized_dataset.examples[1])
    invalid_ex_neg = dataclasses.replace(
        ex1,
        token_positions=TokenPositions({"FIRST_WORD": -1}),  # Invalid position
    )
    tokenized_dataset.examples[1] = invalid_ex_neg  # type: ignore  # Overwrite with invalid example

    # Reset example 0 to be valid for the next check
    # Re-tokenize example 0 to ensure it's a valid TokenizedProbingExample
    valid_ex0_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=ProbingDataset(examples=[simple_probing_dataset.examples[0]]),
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        add_special_tokens=add_special,
    )
    tokenized_dataset.examples[0] = valid_ex0_dataset.examples[0]

    assert not tokenized_dataset.validate_positions()  # Example 1 is still invalid


def test_verify_position_tokens(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    verification = tokenized_dataset.verify_position_tokens(
        tokenizer=tokenizer_right_pad, position_key="FIRST_WORD"
    )

    # verification maps example index to results dict
    assert isinstance(verification, dict)
    assert 0 in verification  # Example 0 has FIRST_WORD
    assert 1 in verification  # Example 1 has FIRST_WORD
    assert 2 not in verification  # Example 2 does not have FIRST_WORD

    # Check example 0
    verification_0 = cast(dict, verification.get(0, {}))
    assert "FIRST_WORD" in verification_0
    res0 = verification_0["FIRST_WORD"]
    assert res0["position"] == 0  # Corrected position
    assert res0["token_text"] == "This"  # Corrected expected token
    assert "error" not in res0

    # Check example 1
    verification_1 = cast(dict, verification.get(1, {}))
    assert "FIRST_WORD" in verification_1
    res1 = verification_1["FIRST_WORD"]
    assert res1["position"] == 0  # Corrected position
    assert res1["token_text"] == "A"  # Corrected expected token for " A"
    assert "error" not in res1


def test_show_token_context_right_pad(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    context = tokenized_dataset.show_token_context(
        example_idx=0,
        position_key="FIRST_WORD",
        tokenizer=tokenizer_right_pad,
        context_size=2,
    )

    assert context["example_idx"] == 0
    assert context["position_key"] == "FIRST_WORD"
    assert context["token_count"] == max_length
    assert context["padding_side"] == "right"
    assert "results" in context

    res = context["results"]  # Assuming single position
    assert res["position"] == 0  # Corrected position (0)
    assert res["original_position"] == 0  # Corrected original position (0)
    assert res["token_info"]["text"] == "This"  # Corrected expected token
    assert res["context_window"]["start"] == 0
    assert res["context_window"]["end"] == 3  # pos 0, context 2 -> indices 0, 1, 2
    # GPT2: This is the ... (No BOS)
    ex0_text = simple_probing_dataset.examples[0].text
    assert ex0_text is not None
    # Get first 3 tokens after tokenization (no special tokens assumed at start)
    expected_context_tokens = tokenizer_right_pad.encode(
        ex0_text, add_special_tokens=False, padding=False, truncation=False
    )[:3]
    expected_texts = [tokenizer_right_pad.decode([t]) for t in expected_context_tokens]
    assert res["context_window"]["tokens"] == expected_context_tokens
    assert res["context_window"]["texts"] == expected_texts
    # Check marked context correctly identifies "This"
    marked_token_text = expected_texts[0]  # Position 0
    assert f"[[ {marked_token_text} ]]" in res["context_window"]["marked_context"]


def test_show_token_context_left_pad(simple_probing_dataset, tokenizer_left_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_left_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    context = tokenized_dataset.show_token_context(
        example_idx=0,  # "This is the first example."
        position_key="FIRST_WORD",
        tokenizer=tokenizer_left_pad,
        context_size=2,
    )

    assert context["padding_side"] == "left"
    res = context["results"]

    # Calculate expected padded position
    ex0_text = simple_probing_dataset.examples[0].text
    assert ex0_text is not None
    pad_len = get_pad_length(ex0_text, tokenizer_left_pad, max_length, add_special)
    expected_padded_pos = 0 + pad_len  # Unpadded position 0 ("This") + padding

    assert res["position"] == expected_padded_pos  # Position within the padded sequence
    assert (
        res["original_position"] == 0
    )  # Position relative to unpadded real tokens (position 0)
    assert res["token_info"]["text"] == "This"  # Corrected expected token

    # Check context window indices are correct relative to padded sequence
    expected_start = max(0, expected_padded_pos - 2)
    expected_end = min(max_length, expected_padded_pos + 3)  # pos + context + 1
    assert res["context_window"]["start"] == expected_start
    assert res["context_window"]["end"] == expected_end
    assert "[[ This ]]" in res["context_window"]["marked_context"]


def test_verify_padding_left(simple_probing_dataset, tokenizer_left_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_left_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    verification = tokenized_dataset.verify_padding(
        tokenizer=tokenizer_left_pad,
        max_length=max_length,
        examples_to_check=2,  # Check first 2 examples
    )

    assert verification["padding_side"] == "left"
    assert verification["max_length"] == max_length
    assert 0 in verification["examples"]
    assert 1 in verification["examples"]

    # Example 0: "This is the first example." -> pos 1 ("This")
    ex0_text = simple_probing_dataset.examples[0].text
    assert ex0_text is not None
    ex0_res = verification["examples"][0]["positions"]["FIRST_WORD"]
    assert ex0_res["position_matches"] is True
    assert ex0_res["token_text"] == "This"
    ex0_pad_len = get_pad_length(ex0_text, tokenizer_left_pad, max_length, add_special)
    # Calculate expected position (unpadded pos 0 + padding)
    ex0_expected_calc = 0 + ex0_pad_len
    assert ex0_res["expected_position"] == ex0_expected_calc
    # Check padded vs expected
    assert ex0_res["padded_position"] == ex0_res["expected_position"]

    # Example 1: "A second, longer example here." -> pos 0 ("A")
    ex1_text = simple_probing_dataset.examples[1].text
    assert ex1_text is not None
    ex1_res = verification["examples"][1]["positions"]["FIRST_WORD"]
    assert ex1_res["position_matches"] is True
    assert ex1_res["token_text"] == "A"
    ex1_pad_len = get_pad_length(ex1_text, tokenizer_left_pad, max_length, add_special)
    ex1_expected_calc = 0 + ex1_pad_len
    assert ex1_res["expected_position"] == ex1_expected_calc
    assert ex1_res["padded_position"] == ex1_res["expected_position"]


def test_verify_padding_right(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    verification = tokenized_dataset.verify_padding(
        tokenizer=tokenizer_right_pad, max_length=max_length, examples_to_check=2
    )

    assert verification["padding_side"] == "right"

    # Example 0: "This" -> pos 1
    ex0_res = verification["examples"][0]["positions"]["FIRST_WORD"]
    assert ex0_res["position_matches"] is True
    assert ex0_res["token_text"] == "This"
    assert (
        ex0_res["expected_position"] == 0
    )  # Corrected expected position (unpadded pos = 0)
    assert ex0_res["padded_position"] == 0

    # Example 1: "A" -> pos 0
    ex1_res = verification["examples"][1]["positions"]["FIRST_WORD"]
    assert ex1_res["position_matches"] is True
    assert ex1_res["token_text"] == "A"
    assert ex1_res["expected_position"] == 0  # Corrected expected position
    assert ex1_res["padded_position"] == 0


def test_to_hf_dataset(simple_probing_dataset, tokenizer_right_pad):
    max_length = 16  # Adjusted
    add_special = True
    tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
        dataset=simple_probing_dataset,
        tokenizer=tokenizer_right_pad,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        add_special_tokens=add_special,
        return_tensors=None,
    )

    hf_dataset = tokenized_dataset._to_hf_dataset()
    assert isinstance(hf_dataset, datasets.Dataset)

    # Check basic columns from ProbingDataset
    assert "text" in hf_dataset.column_names
    assert "label" in hf_dataset.column_names
    assert "label_text" in hf_dataset.column_names
    # Check if 'id' attribute was flattened (depends on ProbingDataset._to_hf_dataset impl.)
    # assert "attribute_id" in hf_dataset.column_names # Example if flattened

    # Check columns added by TokenizedProbingDataset
    assert "tokens" in hf_dataset.column_names
    assert "attention_mask" in hf_dataset.column_names
    assert "token_pos_FIRST_WORD" in hf_dataset.column_names  # Specific to this dataset

    # Verify content
    assert len(hf_dataset) == 3
    assert hf_dataset[0]["text"] == simple_probing_dataset.examples[0].text
    assert hf_dataset[0]["label"] == simple_probing_dataset.examples[0].label
    assert len(hf_dataset[0]["tokens"]) == max_length
    assert hf_dataset[0]["token_pos_FIRST_WORD"] == 0  # Corrected position
    assert hf_dataset[1]["token_pos_FIRST_WORD"] == 0  # Corrected position
    assert (
        hf_dataset[2]["token_pos_FIRST_WORD"] is None
    )  # Example 2, no position defined


# TODO: Add tests for edge cases like empty datasets, examples with no text, etc.



================================================
FILE: tests/unit/pipeline/test_pipeline.py
================================================
import pytest
from unittest.mock import patch, MagicMock, Mock
from pathlib import Path
import torch
import shutil
from typing import Type

from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig
from probity.collection.activation_store import ActivationStore
from probity.datasets.tokenized import TokenizedProbingDataset
from probity.probes.linear_probe import BaseProbe, ProbeConfig
from probity.training.trainer import BaseProbeTrainer, BaseTrainerConfig


# --- Mock Classes ---


@pytest.fixture
def mock_probe_config():
    config = MagicMock()
    config.input_size = 10  # Example dimension
    return config


@pytest.fixture
def mock_trainer_config():
    config = MagicMock(spec=BaseTrainerConfig)
    config.device = "cpu"
    config.batch_size = 4
    return config


@pytest.fixture
def mock_dataset():
    dataset = MagicMock(spec=TokenizedProbingDataset)
    dataset.examples = [Mock() for _ in range(10)]  # Mock examples
    dataset.tokenization_config = MagicMock(tokenizer_name="mock_tokenizer")
    dataset.position_types = {"POS1", "POS2"}
    dataset.__len__.return_value = 10
    return dataset


@pytest.fixture
def mock_probe_cls():
    cls = MagicMock(spec=Type[BaseProbe])
    instance = MagicMock(spec=BaseProbe)
    instance.to = MagicMock(return_value=instance)  # Simulate moving to device
    instance.load = MagicMock(return_value=instance)
    instance.load_json = MagicMock(return_value=instance)  # For ProbeVector loading
    cls.return_value = instance  # Mock constructor call
    cls.load = MagicMock(return_value=instance)  # Mock classmethod load
    cls.load_json = MagicMock(return_value=instance)  # Mock classmethod load_json
    return cls


@pytest.fixture
def mock_trainer_cls():
    mock_instance = MagicMock(spec=BaseProbeTrainer)

    mock_instance.prepare_supervised_data = MagicMock(
        return_value=(MagicMock(), MagicMock())
    )
    mock_instance.train = MagicMock(
        return_value={"train_loss": [0.1], "val_loss": [0.1]}
    )

    mock_cls = MagicMock(spec=Type[BaseProbeTrainer])
    mock_cls.return_value = mock_instance

    return mock_cls


@pytest.fixture
def mock_activation_store():
    store = MagicMock(spec=ActivationStore)
    mock_ds = MagicMock(spec=TokenizedProbingDataset)
    mock_ds.examples = [Mock()] * 10  # Mock examples list
    mock_ds.tokenization_config = MagicMock(tokenizer_name="mock_tokenizer")
    mock_ds.position_types = {"POS1", "POS2"}
    store.dataset = mock_ds  # Assign the more detailed dataset mock

    store.model_name = "mock_model"  # Add model_name attribute
    store.get_probe_data = MagicMock(
        return_value=(torch.randn(10, 10), torch.randint(0, 2, (10, 1)))
    )
    store.save = MagicMock()
    store.load = MagicMock(return_value=store)  # Class method returns instance
    ActivationStore.load = MagicMock(return_value=store)  # Mock classmethod load
    return store


@pytest.fixture
def base_pipeline_config(
    mock_dataset,
    mock_probe_cls,
    mock_probe_config,
    mock_trainer_cls,
    mock_trainer_config,
):
    return ProbePipelineConfig(
        dataset=mock_dataset,
        probe_cls=mock_probe_cls,
        probe_config=mock_probe_config,
        trainer_cls=mock_trainer_cls,
        trainer_config=mock_trainer_config,
        position_key="POS1",
        cache_dir=None,  # No cache by default
        model_name="mock_model",
        hook_points=["hp1"],
        activation_batch_size=4,
        device="cpu",
    )


@pytest.fixture
def pipeline_with_cache(base_pipeline_config, tmp_path):
    config = base_pipeline_config
    config.cache_dir = str(tmp_path / "test_cache")
    # Create the cache dir structure manually for the test
    # cache_base = Path(config.cache_dir)
    # cache_key = "mock_hash" # Assume a fixed hash for simplicity in setup
    # config_hash_path = cache_base / cache_key
    # config_hash_path.mkdir(parents=True, exist_ok=True)
    # (config_hash_path / "hp1").mkdir(exist_ok=True) # Store path for hookpoint
    return ProbePipeline(config)


@pytest.fixture
def pipeline_no_cache(base_pipeline_config):
    base_pipeline_config.cache_dir = None
    return ProbePipeline(base_pipeline_config)


# --- Test Cases ---


def test_pipeline_init_device_sync(
    mock_probe_config,
    mock_trainer_config,
    mock_dataset,
    mock_probe_cls,
    mock_trainer_cls,
):
    """Test device synchronization during initialization."""
    # Case 1: Trainer has device, pipeline adopts it
    trainer_config_cuda = MagicMock(spec=BaseTrainerConfig, device="cuda")
    probe_config_cpu = MagicMock(spec=ProbeConfig, device="cpu")
    config1 = ProbePipelineConfig(
        dataset=mock_dataset,
        probe_cls=mock_probe_cls,
        probe_config=probe_config_cpu,
        trainer_cls=mock_trainer_cls,
        trainer_config=trainer_config_cuda,
        position_key="POS1",
        model_name="m",
        hook_points=["hp1"],
        device="cpu",  # Pipeline device differs initially
    )
    pipeline1 = ProbePipeline(config1)
    assert pipeline1.config.device == "cuda"
    assert pipeline1.config.probe_config.device == "cuda"
    assert pipeline1.config.trainer_config.device == "cuda"

    # Case 2: Pipeline has device, trainer and probe adopt it
    trainer_config_nodev = MagicMock(spec=BaseTrainerConfig)
    # Remove device attribute if it exists to simulate it not being set
    if hasattr(trainer_config_nodev, "device"):
        del trainer_config_nodev.device
    # Use a basic mock but ensure 'device' exists for hasattr checks
    probe_config_nodev = MagicMock()
    probe_config_nodev.device = None  # Add placeholder for hasattr check
    if (
        hasattr(probe_config_nodev, "device") and probe_config_nodev.device is not None
    ):  # Defensive check
        del probe_config_nodev.device

    config2 = ProbePipelineConfig(
        dataset=mock_dataset,
        probe_cls=mock_probe_cls,
        probe_config=probe_config_nodev,
        trainer_cls=mock_trainer_cls,
        trainer_config=trainer_config_nodev,
        position_key="POS1",
        model_name="m",
        hook_points=["hp1"],
        device="mps",  # Pipeline sets device
    )
    pipeline2 = ProbePipeline(config2)
    assert pipeline2.config.device == "mps"
    assert pipeline2.config.probe_config.device == "mps"
    assert pipeline2.config.trainer_config.device == "mps"


@patch("probity.pipeline.pipeline.TransformerLensCollector")
def test_pipeline_run_no_cache(
    mock_collector_cls, pipeline_no_cache, mock_activation_store
):
    """Test pipeline run without cache."""
    # Setup mock collector
    mock_collector_instance = MagicMock()
    mock_collector_instance.collect.return_value = {"hp1": mock_activation_store}
    mock_collector_cls.return_value = mock_collector_instance

    # Mock trainer components
    mock_trainer_instance = pipeline_no_cache.config.trainer_cls.return_value

    # Run
    probe, history = pipeline_no_cache.run(hook_point="hp1")

    # Assertions
    mock_collector_instance.collect.assert_called_once_with(
        pipeline_no_cache.config.dataset
    )
    pipeline_no_cache.config.probe_cls.assert_called_once_with(
        pipeline_no_cache.config.probe_config
    )
    probe.to.assert_called_once_with("cpu")  # Check probe moved to device
    pipeline_no_cache.config.trainer_cls.assert_called_once_with(
        pipeline_no_cache.config.trainer_config
    )
    mock_trainer_instance.prepare_supervised_data.assert_called_once_with(
        mock_activation_store, pipeline_no_cache.config.position_key
    )
    mock_trainer_instance.train.assert_called_once()
    assert pipeline_no_cache.probe is probe
    assert history == {"train_loss": [0.1], "val_loss": [0.1]}


@patch("probity.pipeline.pipeline.Path")
@patch("probity.pipeline.pipeline.ActivationStore")
@patch("probity.pipeline.pipeline.TransformerLensCollector")
def test_pipeline_run_with_valid_cache(
    mock_collector_cls,
    mock_as_cls,
    mock_path_cls,
    pipeline_with_cache,
    mock_activation_store,
):
    """Test pipeline run with a valid cache found."""
    # --- Mock Cache Loading ---
    mock_base_path = MagicMock()
    mock_store_path = MagicMock()

    # Simulate Path(cache_dir) -> mock_base_path
    mock_path_cls.side_effect = lambda x: (
        mock_base_path if x == pipeline_with_cache.config.cache_dir else MagicMock()
    )

    # mock_base_path.exists() -> True
    mock_base_path.exists.return_value = True
    # mock_base_path / hook_point -> mock_store_path
    mock_base_path.__truediv__.return_value = mock_store_path
    # mock_store_path.exists() -> True
    mock_store_path.exists.return_value = True

    # mock_activation_store.load() -> mock_activation_store
    mock_as_cls.load.return_value = mock_activation_store

    # Make validation pass
    pipeline_with_cache._validate_cache_compatibility = MagicMock(return_value=True)
    # --- End Mock Cache Loading ---

    mock_collector_instance = MagicMock()  # Should not be called
    mock_collector_cls.return_value = mock_collector_instance
    mock_trainer_instance = pipeline_with_cache.config.trainer_cls.return_value

    # Run
    probe, history = pipeline_with_cache.run(hook_point="hp1")

    # Assertions
    mock_path_cls.assert_any_call(
        pipeline_with_cache.config.cache_dir
    )  # Check cache dir access
    mock_base_path.exists.assert_called_once()
    mock_base_path.__truediv__.assert_called_once_with(
        "hp1"
    )  # Check hook point subdir access
    mock_store_path.exists.assert_called_once()
    mock_as_cls.load.assert_called_once_with(str(mock_store_path))
    pipeline_with_cache._validate_cache_compatibility.assert_called_once_with(
        mock_activation_store, "hp1"
    )
    mock_collector_instance.collect.assert_not_called()  # Ensure collector was skipped
    pipeline_with_cache.config.probe_cls.assert_called_once()
    pipeline_with_cache.config.trainer_cls.assert_called_once()
    mock_trainer_instance.prepare_supervised_data.assert_called_once()
    mock_trainer_instance.train.assert_called_once()
    assert pipeline_with_cache.activation_stores["hp1"] is mock_activation_store


@patch("shutil.rmtree")
@patch("probity.pipeline.pipeline.Path")
@patch("probity.pipeline.pipeline.ActivationStore")
@patch("probity.pipeline.pipeline.TransformerLensCollector")
def test_pipeline_run_with_invalid_cache(
    mock_collector_cls,
    mock_as_cls,
    mock_path_cls,
    mock_rmtree,
    pipeline_with_cache,
    mock_activation_store,
):
    """Test pipeline run with an invalid cache (triggering collection)."""
    # --- Mock Cache Loading ---
    mock_base_path = MagicMock(spec=Path)
    mock_store_path = MagicMock(spec=Path)

    # Simulate Path(cache_dir) -> mock_base_path
    mock_path_cls.side_effect = lambda x: (
        mock_base_path
        if x == pipeline_with_cache.config.cache_dir
        else MagicMock(spec=Path)
    )

    mock_base_path.exists.return_value = True
    mock_base_path.__truediv__.return_value = mock_store_path
    mock_store_path.exists.return_value = True

    mock_as_cls.load.return_value = mock_activation_store

    # Make validation fail
    pipeline_with_cache._validate_cache_compatibility = MagicMock(return_value=False)
    # --- End Mock Cache Loading ---

    # Setup mock collector (will be called this time)
    collected_stores = {"hp1": mock_activation_store}
    mock_collector_instance = MagicMock()
    mock_collector_instance.collect.return_value = collected_stores
    mock_collector_cls.return_value = mock_collector_instance

    mock_trainer_instance = pipeline_with_cache.config.trainer_cls.return_value

    # Run
    probe, history = pipeline_with_cache.run(hook_point="hp1")

    # Assertions
    mock_path_cls.assert_any_call(pipeline_with_cache.config.cache_dir)
    mock_base_path.exists.assert_called_once()
    mock_base_path.__truediv__.assert_any_call("hp1")
    mock_store_path.exists.assert_called_once()
    mock_as_cls.load.assert_called_once_with(str(mock_store_path))
    pipeline_with_cache._validate_cache_compatibility.assert_called_once_with(
        mock_activation_store, "hp1"
    )
    mock_rmtree.assert_called_once_with(mock_base_path)  # Check cache cleared
    mock_collector_instance.collect.assert_called_once_with(
        pipeline_with_cache.config.dataset
    )  # Collector was called
    pipeline_with_cache.config.probe_cls.assert_called_once()
    pipeline_with_cache.config.trainer_cls.assert_called_once()
    mock_trainer_instance.prepare_supervised_data.assert_called_once()
    mock_trainer_instance.train.assert_called_once()
    assert pipeline_with_cache.activation_stores["hp1"] is collected_stores["hp1"]


@patch("probity.pipeline.pipeline.Path")
@patch("probity.pipeline.pipeline.ActivationStore")
@patch("probity.pipeline.pipeline.TransformerLensCollector")
def test_pipeline_run_cache_save(
    mock_collector_cls,
    mock_as_cls,
    mock_path_cls,
    pipeline_with_cache,
    mock_activation_store,
):
    """Test if activations are saved when cache_dir is set and collection happens."""
    # --- Mock Cache Loading (make it seem empty/invalid initially) ---
    mock_base_path = MagicMock(spec=Path)
    mock_store_path = MagicMock(spec=Path)
    mock_path_cls.side_effect = lambda x: (
        mock_base_path
        if x == pipeline_with_cache.config.cache_dir
        else MagicMock(spec=Path)
    )

    # Simulate cache dir DOES NOT exist initially to force collection & save
    mock_base_path.exists.return_value = False
    mock_base_path.mkdir = MagicMock()  # Mock mkdir call

    # --- End Mock Cache Loading ---

    # Setup mock collector
    collected_stores = {"hp1": mock_activation_store}
    mock_collector_instance = MagicMock()
    mock_collector_instance.collect.return_value = collected_stores
    mock_collector_cls.return_value = mock_collector_instance

    # mock_trainer_instance = pipeline_with_cache.config.trainer_cls.return_value # Unused

    # Need to mock the __truediv__ call that happens during the save loop
    mock_base_path.__truediv__.return_value = mock_store_path

    # Run
    probe, history = pipeline_with_cache.run(hook_point="hp1")

    # Assertions
    mock_path_cls.assert_any_call(pipeline_with_cache.config.cache_dir)
    mock_base_path.exists.assert_called_once()  # Check existence
    # Store path existence shouldn't be checked initially
    # mock_store_path.exists.assert_not_called()
    # Check base dir creation
    mock_base_path.mkdir.assert_called_once_with(parents=True, exist_ok=True)
    mock_collector_instance.collect.assert_called_once()
    # Check __truediv__ was called to construct the save path
    mock_base_path.__truediv__.assert_called_with("hp1")
    # Check store was saved
    mock_activation_store.save.assert_called_once_with(str(mock_store_path))


@patch(
    "probity.pipeline.pipeline.ProbePipelineConfig"
)  # We need to mock the config used internally
@patch("probity.pipeline.pipeline.torch")
@patch("probity.pipeline.pipeline.Path")
def test_pipeline_load(
    mock_path_cls,
    mock_torch,
    MockPipelineConfig,
    mock_probe_cls,
    mock_trainer_cls,
    tmp_path,
):
    """Test loading a pipeline from a saved state."""
    load_dir = tmp_path / "saved_pipeline"
    load_dir.mkdir()
    probe_path = load_dir / "probe.pt"
    config_path = load_dir / "config.pt"

    # Mock Path objects
    mock_load_path = MagicMock(spec=Path)
    mock_probe_path = MagicMock(spec=Path)
    mock_config_path = MagicMock(spec=Path)
    mock_vector_path = MagicMock(spec=Path)  # For probe_vector.json

    mock_path_cls.side_effect = lambda x: {
        str(load_dir): mock_load_path,
        str(probe_path): mock_probe_path,
        str(config_path): mock_config_path,
        str(load_dir / "probe_vector.json"): mock_vector_path,
    }.get(
        str(x), MagicMock(spec=Path)
    )  # Return specific mocks or generic one

    mock_load_path.__truediv__.side_effect = lambda x: {
        "config.pt": mock_config_path,
        "probe.pt": mock_probe_path,
        "probe_vector.json": mock_vector_path,
    }.get(x)

    # Mock loaded config
    mock_loaded_config = MagicMock(spec=ProbePipelineConfig)
    mock_loaded_config.probe_cls = mock_probe_cls  # The class itself
    mock_loaded_config.trainer_config = MagicMock(
        device="cuda"
    )  # Config object has device
    mock_loaded_config.probe_config = MagicMock()  # Needs to exist
    mock_torch.load.return_value = mock_loaded_config
    MockPipelineConfig.return_value = (
        mock_loaded_config  # Ensure constructor uses mocked config
    )

    # Mock probe loading
    mock_probe_instance = (
        mock_probe_cls.return_value
    )  # Get the instance created by the fixture
    # Simulate only probe.pt exists
    mock_probe_path.exists.return_value = True
    mock_vector_path.exists.return_value = False

    # Call the class method
    pipeline = ProbePipeline.load(str(load_dir))

    # Assertions
    mock_path_cls.assert_any_call(str(load_dir))
    mock_load_path.__truediv__.assert_any_call("config.pt")
    mock_torch.load.assert_called_once_with(str(mock_config_path))
    # Ensure probe_cls (which is a mock) was called to create the probe instance
    # It's called inside the load method, not necessarily inside the constructor
    mock_probe_cls.load.assert_called_once_with(str(mock_probe_path))
    mock_probe_instance.to.assert_called_once_with(
        "cuda"
    )  # Loaded probe moved to device from config
    assert pipeline.probe is mock_probe_instance
    assert pipeline.config is mock_loaded_config


def test_validate_cache_compatibility(
    pipeline_no_cache, mock_activation_store, mock_dataset
):
    """Test cache validation logic."""
    pipeline = pipeline_no_cache  # Use a pipeline instance for access to config

    # Case 1: Compatible
    store_compatible = MagicMock(spec=ActivationStore)
    comp_ds = MagicMock(spec=TokenizedProbingDataset)
    comp_ds.examples = [Mock()] * 10
    comp_ds.tokenization_config = MagicMock(tokenizer_name="mock_tokenizer")
    comp_ds.position_types = {"POS1", "POS2"}
    store_compatible.dataset = comp_ds
    store_compatible.model_name = "mock_model"  # Match model name

    assert pipeline._validate_cache_compatibility(store_compatible, "hp1") is True

    # Case 2: Dataset size mismatch
    store_diff_size = MagicMock(spec=ActivationStore)
    diff_size_ds = MagicMock(spec=TokenizedProbingDataset)
    diff_size_ds.examples = [Mock()] * 5  # Different size
    diff_size_ds.tokenization_config = MagicMock(tokenizer_name="mock_tokenizer")
    diff_size_ds.position_types = {"POS1", "POS2"}
    store_diff_size.dataset = diff_size_ds
    store_diff_size.model_name = "mock_model"
    assert pipeline._validate_cache_compatibility(store_diff_size, "hp1") is False

    # Case 3: Tokenizer mismatch
    store_diff_tokenizer = MagicMock(spec=ActivationStore)
    diff_tok_ds = MagicMock(spec=TokenizedProbingDataset)
    diff_tok_ds.examples = [Mock()] * 10
    diff_tok_ds.tokenization_config = MagicMock(
        tokenizer_name="other_tokenizer"
    )  # Diff tokenizer
    diff_tok_ds.position_types = {"POS1", "POS2"}
    store_diff_tokenizer.dataset = diff_tok_ds
    store_diff_tokenizer.model_name = "mock_model"
    assert pipeline._validate_cache_compatibility(store_diff_tokenizer, "hp1") is False

    # Case 4: Position types mismatch
    store_diff_pos = MagicMock(spec=ActivationStore)
    diff_pos_ds = MagicMock(spec=TokenizedProbingDataset)
    diff_pos_ds.examples = [Mock()] * 10
    diff_pos_ds.tokenization_config = MagicMock(tokenizer_name="mock_tokenizer")
    diff_pos_ds.position_types = {"POS_X"}  # Different position types
    store_diff_pos.dataset = diff_pos_ds
    store_diff_pos.model_name = "mock_model"
    assert pipeline._validate_cache_compatibility(store_diff_pos, "hp1") is False

    # Case 5: Model name mismatch
    store_diff_model = MagicMock(spec=ActivationStore)
    diff_model_ds = MagicMock(spec=TokenizedProbingDataset)
    diff_model_ds.examples = [Mock()] * 10
    diff_model_ds.tokenization_config = MagicMock(tokenizer_name="mock_tokenizer")
    diff_model_ds.position_types = {"POS1", "POS2"}
    store_diff_model.dataset = diff_model_ds
    store_diff_model.model_name = "other_model"  # Different model name
    assert pipeline._validate_cache_compatibility(store_diff_model, "hp1") is False


@patch("hashlib.md5")
@patch("json.dumps")
def test_get_cache_key(mock_json_dumps, mock_md5, pipeline_no_cache):
    """Test cache key generation."""
    # Mock hashlib and json.dumps
    mock_hash_obj = MagicMock()
    mock_hash_obj.hexdigest.return_value = "mock_hash_value"
    mock_md5.return_value = mock_hash_obj
    mock_json_dumps.return_value = '{"json_string": true}'

    # Call the method
    key = pipeline_no_cache._get_cache_key()

    # Assertions
    expected_dict = {
        "model_name": "mock_model",
        "hook_points": ["hp1"],
        "position_key": "POS1",
        "tokenizer_name": "mock_tokenizer",
        "dataset_size": 10,
        "position_types": sorted(list({"POS1", "POS2"})),  # Ensure order consistency
    }
    # Check json.dumps called with correct dict structure and sort_keys=True
    mock_json_dumps.assert_called_once()
    call_args, call_kwargs = mock_json_dumps.call_args
    assert (
        call_args[0] == expected_dict
    )  # This should now pass due to sorted list in source
    assert call_kwargs.get("sort_keys") is True

    # Check hashlib.md5 was called with the encoded json string
    mock_md5.assert_called_once_with('{"json_string": true}'.encode())
    mock_hash_obj.hexdigest.assert_called_once()
    assert key == "mock_hash_value"


@patch("probity.pipeline.pipeline.Path")
def test_get_cache_path(mock_path_cls, pipeline_with_cache):
    """Test cache path generation."""
    # Mock Path and _get_cache_key
    mock_base_path = MagicMock(spec=Path)
    mock_final_path = MagicMock(spec=Path)
    mock_path_cls.return_value = mock_base_path
    mock_base_path.__truediv__.return_value = mock_final_path
    pipeline_with_cache._get_cache_key = MagicMock(return_value="mock_config_hash")

    # Call the method
    cache_path = pipeline_with_cache._get_cache_path()

    # Assertions
    mock_path_cls.assert_called_once_with(pipeline_with_cache.config.cache_dir)
    pipeline_with_cache._get_cache_key.assert_called_once()
    mock_base_path.__truediv__.assert_called_once_with("mock_config_hash")
    assert cache_path is mock_final_path


def test_run_without_hook_point_specified(pipeline_no_cache):
    """Test run uses the first hook point if none is specified."""
    pipeline_no_cache.config.hook_points = ["hp_first", "hp_second"]

    # Mock activation stores loading
    mock_store1 = MagicMock(spec=ActivationStore)
    mock_store2 = MagicMock(spec=ActivationStore)
    pipeline_no_cache._load_or_collect_activations = MagicMock(
        return_value={"hp_first": mock_store1, "hp_second": mock_store2}
    )

    mock_trainer_instance = pipeline_no_cache.config.trainer_cls.return_value

    # Run without specifying hook_point
    pipeline_no_cache.run()

    # Assert that prepare_supervised_data was called with the first store
    mock_trainer_instance.prepare_supervised_data.assert_called_once_with(
        mock_store1, pipeline_no_cache.config.position_key
    )


def test_run_with_invalid_hook_point(pipeline_no_cache):
    """Test run raises ValueError for an unknown hook point."""
    pipeline_no_cache.config.hook_points = ["hp_known"]
    # Mock activation stores loading
    mock_store = MagicMock(spec=ActivationStore)
    pipeline_no_cache._load_or_collect_activations = MagicMock(
        return_value={"hp_known": mock_store}
    )

    with pytest.raises(ValueError, match="Hook point hp_unknown not found"):
        pipeline_no_cache.run(hook_point="hp_unknown")


# Clean up cache directory if it exists (safety measure)
@pytest.fixture(autouse=True)
def cleanup_cache(tmp_path):
    cache_dir = tmp_path / "test_cache"
    yield
    if cache_dir.exists():
        shutil.rmtree(cache_dir)



================================================
FILE: tests/unit/trainer/test_trainer.py
================================================
import pytest
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from unittest.mock import MagicMock, patch
import math

from probity.collection.activation_store import ActivationStore
from probity.probes.linear_probe import (
    BaseProbe,
    LogisticProbe,
    MultiClassLogisticProbe,
    LinearProbe,
    DirectionalProbe,
    LogisticProbeConfig,
)
from probity.training.trainer import (
    BaseTrainerConfig,
    SupervisedTrainerConfig,
    DirectionalTrainerConfig,
    BaseProbeTrainer,
    SupervisedProbeTrainer,
    DirectionalProbeTrainer,
)


# --- Fixtures ---


@pytest.fixture
def base_config():
    return BaseTrainerConfig(device="cpu", num_epochs=2, show_progress=False)


@pytest.fixture
def supervised_config(base_config):
    # Inherit from base_config and add specific fields
    return SupervisedTrainerConfig(
        **base_config.__dict__,
        train_ratio=0.8,
        patience=2,
        min_delta=1e-4,
    )


@pytest.fixture
def directional_config(base_config):
    # Inherit from base_config
    return DirectionalTrainerConfig(**base_config.__dict__)


@pytest.fixture
def mock_activation_store():
    store = MagicMock(spec=ActivationStore)
    # Simulate data: 10 samples, 5 features
    X = torch.randn(10, 5)
    y = torch.randint(0, 2, (10, 1)).float()  # Binary labels
    store.get_probe_data.return_value = (X, y)
    return store


@pytest.fixture
def mock_activation_store_multiclass():
    store = MagicMock(spec=ActivationStore)
    # Simulate data: 10 samples, 5 features, 3 classes
    X = torch.randn(10, 5)
    y = torch.randint(0, 3, (10, 1)).long()  # Multi-class integer labels
    store.get_probe_data.return_value = (X, y)
    return store


# --- Mock Models ---


class MockProbe(BaseProbe):
    def __init__(self, config, input_size=5, output_size=1):
        super().__init__(config)
        self.linear = nn.Linear(input_size, output_size)
        self.config = config  # Store config

    def forward(self, x):
        return self.linear(x)

    def get_loss_fn(self, **kwargs):
        return nn.BCEWithLogitsLoss(**kwargs)

    def _get_raw_direction_representation(self):
        return self.linear.weight.data.clone()

    def _set_raw_direction_representation(self, direction):
        self.linear.weight.data = direction.clone()


class MockLogisticProbe(MockProbe, LogisticProbe):
    def __init__(self, config, input_size=5, output_size=1):
        # Need to call __init__ of both MockProbe and BaseProbe (which LogisticProbe inherits)
        # But MockProbe already calls BaseProbe.__init__
        MockProbe.__init__(self, config, input_size, output_size)
        # LogisticProbe specific init if any (currently none needed for tests)


class MockMultiClassProbe(MockProbe, MultiClassLogisticProbe):
    def __init__(self, config, input_size=5, output_size=3):
        MockProbe.__init__(self, config, input_size, output_size)
        self.config.output_size = output_size  # Make sure output_size is set

    def get_loss_fn(self, **kwargs):
        return nn.CrossEntropyLoss(**kwargs)


class MockLinearProbe(MockProbe, LinearProbe):
    def __init__(self, config, input_size=5, output_size=1):
        MockProbe.__init__(self, config, input_size, output_size)
        self.config.loss_type = "mse"  # Example loss type

    def get_loss_fn(self, **kwargs):
        # Return instance, not class
        return nn.MSELoss(**kwargs)


class MockDirectionalProbe(MockProbe, DirectionalProbe):
    def __init__(self, config, input_size=5):
        # Directional probes don't have a linear layer in the same way initially
        BaseProbe.__init__(self, config)  # Call BaseProbe init directly
        self.config = config
        self.direction = nn.Parameter(torch.randn(1, input_size), requires_grad=False)

    def forward(self, x):
        # Simulate projection onto the direction
        return torch.matmul(x, self.direction.T)

    def fit(self, x, y):
        # Simple mock: calculate mean difference as direction
        y_squeeze = y.squeeze()
        class0_x = x[y_squeeze == 0]
        class1_x = x[y_squeeze == 1]

        # Handle cases where one class might be missing in a small batch/dataset
        if class0_x.nelement() > 0:  # Check if tensor is not empty
            class0_mean = class0_x.mean(dim=0, keepdim=True)
        else:
            class0_mean = torch.zeros(
                1, x.shape[1], device=x.device, dtype=x.dtype
            )  # Use x's device/dtype

        if class1_x.nelement() > 0:
            class1_mean = class1_x.mean(dim=0, keepdim=True)
        else:
            class1_mean = torch.zeros(1, x.shape[1], device=x.device, dtype=x.dtype)

        fitted_direction = class1_mean - class0_mean
        # Return the fitted direction (potentially scaled if standardization happened)
        return fitted_direction

    def _get_raw_direction_representation(self):
        return self.direction.data.clone()

    def _set_raw_direction_representation(self, direction):
        # Ensure direction is the correct shape [1, dim] or [dim] and reshape if needed
        if direction.dim() == 1:
            direction = direction.unsqueeze(0)
        if direction.shape != self.direction.shape:
            raise ValueError(
                f"Shape mismatch: expected {self.direction.shape}, got {direction.shape}"
            )
        self.direction.data = direction.clone()


# --- Test Classes ---


class TestBaseProbeTrainer:

    def test_init(self, base_config):
        trainer = BaseProbeTrainer(base_config)
        assert trainer.config == base_config
        assert trainer.feature_mean is None
        assert trainer.feature_std is None

    @pytest.mark.parametrize(
        "start_lr, end_lr, num_steps, expected_gamma",
        [
            (
                1e-3,
                1e-5,
                10,
                pytest.approx(math.exp(math.log(1e-2) / 10)),
            ),  # Standard decay
            (1e-3, 1e-3, 10, 1.0),  # No decay
            (1e-4, 1e-6, 5, pytest.approx(math.exp(math.log(1e-2) / 5))),
            (1e-3, 1e-6, 0, None),  # Edge case: 0 steps -> ConstantLR expected
            (0, 1e-5, 10, None),  # Edge case: 0 start_lr -> ConstantLR expected
            (1e-3, 0, 10, None),  # Edge case: 0 end_lr -> ConstantLR expected
        ],
    )
    def test_get_lr_scheduler(
        self, base_config, start_lr, end_lr, num_steps, expected_gamma
    ):
        trainer = BaseProbeTrainer(base_config)
        model = nn.Linear(1, 1)
        optimizer = torch.optim.Adam(model.parameters(), lr=start_lr)

        # Mock print to avoid console output during tests
        with patch("builtins.print") as mock_print:
            scheduler = trainer._get_lr_scheduler(
                optimizer, start_lr, end_lr, num_steps
            )

            if expected_gamma is None:  # Indicates ConstantLR should be used
                assert isinstance(scheduler, torch.optim.lr_scheduler.ConstantLR)
                mock_print.assert_called()  # Check that warning was printed
            else:
                assert isinstance(scheduler, torch.optim.lr_scheduler.ExponentialLR)
                assert scheduler.gamma == expected_gamma

    @pytest.mark.parametrize(
        "y_data, expected_weights",
        [
            (
                torch.tensor([[0.0], [0.0], [1.0], [0.0]]),
                torch.tensor([3.0 / (1.0 + 1e-8)]),
            ),  # Binary single output
            (
                torch.tensor([[0.0], [0.0], [0.0], [0.0]]),
                torch.tensor([4.0 / (0.0 + 1e-8)]),
            ),  # All negative
            (
                torch.tensor([[1.0], [1.0], [1.0], [1.0]]),
                torch.tensor([0.0 / (4.0 + 1e-8)]),
            ),  # All positive
            (
                torch.tensor([[0.0, 1.0], [1.0, 1.0], [0.0, 0.0], [1.0, 0.0]]),
                torch.tensor([2.0 / (2.0 + 1e-8), 2.0 / (2.0 + 1e-8)]),
            ),  # Multi-output
            (
                torch.tensor([0, 0, 1, 0]),
                torch.tensor([3.0 / (1.0 + 1e-8)]),
            ),  # Binary 1D input
        ],
    )
    def test_calculate_pos_weights(self, base_config, y_data, expected_weights):
        trainer = BaseProbeTrainer(base_config)
        weights = trainer._calculate_pos_weights(y_data.float())
        assert torch.allclose(weights, expected_weights.float(), atol=1e-7)

    @pytest.mark.parametrize(
        "y_data, num_classes, expected_weights",
        [
            (
                torch.tensor([0, 1, 0, 2, 1, 0]),
                3,
                torch.tensor(
                    [
                        4.0 / (3 * 3.0 + 1e-8),
                        4.0 / (3 * 2.0 + 1e-8),
                        4.0 / (3 * 1.0 + 1e-8),
                    ]
                )
                * 6
                / 4,
            ),  # Multi-class standard
            (
                torch.tensor([0, 0, 0]),
                3,
                torch.tensor(
                    [
                        3.0 / (3 * 3.0 + 1e-8),
                        3.0 / (3 * 0.0 + 1e-8),
                        3.0 / (3 * 0.0 + 1e-8),
                    ]
                )
                * 3
                / 3,
            ),  # One class only
            (torch.tensor([]), 3, None),  # Empty input
            (
                torch.tensor([[0], [1], [0], [2]]),
                3,
                torch.tensor(
                    [
                        4.0 / (3 * 2.0 + 1e-8),
                        4.0 / (3 * 1.0 + 1e-8),
                        4.0 / (3 * 1.0 + 1e-8),
                    ]
                )
                * 4
                / 4,
            ),  # Multi-class [N, 1] input
            (torch.tensor([0, 1]).float(), 2, None),  # Incorrect dtype
        ],
    )
    def test_calculate_class_weights(
        self, base_config, y_data, num_classes, expected_weights
    ):
        trainer = BaseProbeTrainer(base_config)
        # Mock print for unsupported dtype warning
        with patch("builtins.print") as mock_print:
            weights = trainer._calculate_class_weights(y_data, num_classes)
            if expected_weights is None:
                assert weights is None
                if y_data.numel() > 0:  # Only expect print if data is not empty
                    mock_print.assert_called()
            else:
                assert weights is not None
                # Need to adjust the expected calculation slightly: it's total_samples / (num_classes * count)
                total_samples = y_data.numel()
                if y_data.dim() == 2 and y_data.shape[1] == 1:
                    y_data = y_data.squeeze(1)
                counts = torch.bincount(y_data.long(), minlength=num_classes)
                expected_tensor = total_samples / (num_classes * (counts + 1e-8))
                assert torch.allclose(weights, expected_tensor.float(), atol=1e-7)

    @pytest.mark.parametrize("optimizer_type", ["Adam", "SGD", "AdamW"])
    def test_create_optimizer(self, base_config, optimizer_type):
        config = BaseTrainerConfig(
            optimizer_type=optimizer_type, learning_rate=0.1, weight_decay=0.01
        )
        trainer = BaseProbeTrainer(config)
        model = nn.Linear(5, 1)  # Simple model with parameters
        optimizer = trainer._create_optimizer(model)

        assert isinstance(optimizer, getattr(torch.optim, optimizer_type))
        assert optimizer.defaults["lr"] == 0.1
        # Weight decay might be handled differently (e.g., AdamW), check param groups
        for group in optimizer.param_groups:
            assert group["weight_decay"] == 0.01
            # Check only trainable parameters are included
            model_params_ids = {id(p) for p in model.parameters() if p.requires_grad}
            optimizer_params_ids = {id(p) for p in group["params"]}
            assert optimizer_params_ids == model_params_ids

    def test_create_optimizer_invalid(self, base_config):
        config = BaseTrainerConfig(optimizer_type="InvalidOptim")
        trainer = BaseProbeTrainer(config)
        model = nn.Linear(5, 1)
        with pytest.raises(ValueError, match="Unknown optimizer type: InvalidOptim"):
            trainer._create_optimizer(model)

    def test_prepare_data_no_standardization(self, base_config, mock_activation_store):
        trainer = BaseProbeTrainer(base_config)
        X_expected, y_expected = mock_activation_store.get_probe_data("pos")
        X_train, y, X_orig = trainer.prepare_data(mock_activation_store, "pos")

        assert torch.equal(X_train, X_expected)
        assert torch.equal(y, y_expected)
        assert torch.equal(X_orig, X_expected)
        assert trainer.feature_mean is None
        assert trainer.feature_std is None

    def test_prepare_data_with_standardization(
        self, base_config, mock_activation_store
    ):
        config = base_config
        config.standardize_activations = True
        config.device = "cpu"
        trainer = BaseProbeTrainer(config)
        X_expected, y_expected = mock_activation_store.get_probe_data("pos")
        X_train, y, X_orig = trainer.prepare_data(mock_activation_store, "pos")

        assert not torch.equal(X_train, X_expected)  # X_train should be standardized
        assert torch.equal(y, y_expected)
        assert torch.equal(X_orig, X_expected)  # X_orig remains original
        assert trainer.feature_mean is not None
        assert trainer.feature_std is not None
        assert trainer.feature_mean.shape == (1, X_expected.shape[1])
        assert trainer.feature_std.shape == (1, X_expected.shape[1])
        # Check if standardization worked (mean approx 0, std approx 1)
        assert torch.allclose(
            X_train.mean(dim=0), torch.zeros(X_expected.shape[1]), atol=1e-6
        )
        assert torch.allclose(
            X_train.std(dim=0), torch.ones(X_expected.shape[1]), atol=1e-6
        )
        # Check stats are on the correct device
        assert trainer.feature_mean.device.type == "cpu"
        assert trainer.feature_std.device.type == "cpu"


class TestSupervisedProbeTrainer:

    def test_init(self, supervised_config):
        trainer = SupervisedProbeTrainer(supervised_config)
        assert trainer.config == supervised_config

    def test_prepare_supervised_data(self, supervised_config, mock_activation_store):
        trainer = SupervisedProbeTrainer(supervised_config)
        train_loader, val_loader = trainer.prepare_supervised_data(
            mock_activation_store, "pos"
        )

        assert isinstance(train_loader, DataLoader)
        assert isinstance(val_loader, DataLoader)

        # Check dataset sizes (8 train, 2 val based on ratio 0.8 and 10 total samples)
        assert len(train_loader.dataset) == 8
        assert len(val_loader.dataset) == 2

        # Check batch contents (X_train, y, X_orig)
        x_train_batch, y_batch, x_orig_batch = next(iter(train_loader))
        assert (
            x_train_batch.shape[0] <= supervised_config.batch_size
        )  # Check batch size respected
        assert x_train_batch.shape[1] == 5  # Feature dim
        assert y_batch.shape[0] == x_train_batch.shape[0]
        assert y_batch.shape[1] == 1  # Label dim
        assert x_orig_batch.shape == x_train_batch.shape
        assert y_batch.dtype == torch.float32  # Should be float

    def test_prepare_supervised_data_edge_cases(
        self, supervised_config, mock_activation_store
    ):
        # 1. Test train_ratio = 1.0 (should keep 1 for validation)
        config_all_train = supervised_config
        config_all_train.train_ratio = 1.0
        trainer_all_train = SupervisedProbeTrainer(config_all_train)
        with patch("builtins.print") as mock_print:
            train_loader, val_loader = trainer_all_train.prepare_supervised_data(
                mock_activation_store, "pos"
            )
            assert len(train_loader.dataset) == 9  # type: ignore
            assert len(val_loader.dataset) == 1  # type: ignore
            mock_print.assert_called_with(
                "Warning: train_ratio resulted in no validation data. Adjusting to keep one sample for validation."
            )

        # 2. Test train_ratio = 0.0 (should use all for training, warn)
        # Need a fresh config copy for this case
        config_no_train = SupervisedTrainerConfig(
            device=supervised_config.device,
            num_epochs=supervised_config.num_epochs,
            show_progress=supervised_config.show_progress,
            train_ratio=0.0,  # Override ratio
            patience=supervised_config.patience,
            min_delta=supervised_config.min_delta,
            # Add other fields if necessary
        )
        trainer_no_train = SupervisedProbeTrainer(config_no_train)
        with patch("builtins.print") as mock_print:
            train_loader, val_loader = trainer_no_train.prepare_supervised_data(
                mock_activation_store, "pos"
            )
            assert len(train_loader.dataset) == 10  # type: ignore
            assert len(val_loader.dataset) == 10  # type: ignore # Uses training data for validation
            mock_print.assert_any_call(
                "Warning: train_ratio resulted in no training data. Using all data for training."
            )
            mock_print.assert_any_call(
                "Warning: No validation samples after split. Using training data for validation."
            )

    def test_train_epoch(self, supervised_config):
        trainer = SupervisedProbeTrainer(supervised_config)
        model = MockLogisticProbe(LogisticProbeConfig(input_size=5), input_size=5)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        loss_fn = nn.BCEWithLogitsLoss()
        # Simple dataset/loader for one batch
        X = torch.randn(4, 5)
        y = torch.randint(0, 2, (4, 1)).float()
        loader = DataLoader(TensorDataset(X, y, X), batch_size=4)  # X_train, y, X_orig

        loss = trainer.train_epoch(model, loader, optimizer, loss_fn, 0, 1)
        assert isinstance(loss, float)
        assert loss > 0

    def test_validate(self, supervised_config):
        trainer = SupervisedProbeTrainer(supervised_config)
        model = MockLogisticProbe(LogisticProbeConfig(input_size=5), input_size=5)
        loss_fn = nn.BCEWithLogitsLoss()
        # Simple dataset/loader for one batch
        X = torch.randn(4, 5)  # This will be treated as X_train (ignored)
        y = torch.randint(0, 2, (4, 1)).float()
        X_orig = torch.randn(4, 5)  # This will be used for validation
        loader = DataLoader(TensorDataset(X, y, X_orig), batch_size=4)

        # Mock model's forward call to check if X_orig is used
        model.forward = MagicMock(return_value=torch.randn(4, 1))

        loss = trainer.validate(model, loader, loss_fn)
        assert isinstance(loss, float)
        assert loss > 0
        # Verify model.forward was called with X_orig
        assert model.forward.call_count == 1
        call_args, _ = model.forward.call_args
        assert torch.equal(call_args[0], X_orig)

    @patch("probity.training.trainer.SupervisedProbeTrainer.train_epoch")
    @patch("probity.training.trainer.SupervisedProbeTrainer.validate")
    def test_train_basic_logistic(
        self, mock_validate, mock_train_epoch, supervised_config, mock_activation_store
    ):
        mock_train_epoch.return_value = 0.5  # Mock train loss
        mock_validate.return_value = 0.4  # Mock validation loss
        trainer = SupervisedProbeTrainer(supervised_config)
        probe_config = LogisticProbeConfig(
            input_size=5, model_name="test", hook_point="test"
        )
        model = MockLogisticProbe(probe_config, input_size=5)
        train_loader, val_loader = trainer.prepare_supervised_data(
            mock_activation_store, "pos"
        )

        history = trainer.train(model, train_loader, val_loader)

        assert mock_train_epoch.call_count == supervised_config.num_epochs
        assert mock_validate.call_count == supervised_config.num_epochs
        assert "train_loss" in history
        assert "val_loss" in history
        assert "learning_rate" in history
        assert len(history["train_loss"]) == supervised_config.num_epochs
        assert len(history["val_loss"]) == supervised_config.num_epochs
        assert history["train_loss"][0] == 0.5
        assert history["val_loss"][0] == 0.4
        assert trainer.feature_mean is None  # No standardization by default

    @patch("probity.training.trainer.SupervisedProbeTrainer.train_epoch")
    @patch("probity.training.trainer.SupervisedProbeTrainer.validate")
    def test_train_multiclass(
        self,
        mock_validate,
        mock_train_epoch,
        supervised_config,
        mock_activation_store_multiclass,
    ):
        mock_train_epoch.return_value = 0.8  # Mock train loss
        mock_validate.return_value = 0.7  # Mock validation loss
        trainer = SupervisedProbeTrainer(supervised_config)
        # Adjust config for multiclass
        supervised_config.handle_class_imbalance = True  # Test weight calculation path
        probe_config = LogisticProbeConfig(
            input_size=5, output_size=3, model_name="test", hook_point="test"
        )
        model = MockMultiClassProbe(probe_config, input_size=5, output_size=3)
        train_loader, val_loader = trainer.prepare_supervised_data(
            mock_activation_store_multiclass, "pos"
        )

        # Check that the correct loss is used (mock the get_loss_fn)
        mock_loss_fn = MagicMock(spec=nn.CrossEntropyLoss)
        model.get_loss_fn = MagicMock(return_value=mock_loss_fn)

        history = trainer.train(model, train_loader, val_loader)

        assert mock_train_epoch.call_count == supervised_config.num_epochs
        assert mock_validate.call_count == supervised_config.num_epochs
        assert len(history["train_loss"]) == supervised_config.num_epochs

        # Verify get_loss_fn was called potentially with weights
        model.get_loss_fn.assert_called_once()
        call_kwargs = model.get_loss_fn.call_args.kwargs
        assert "class_weights" in call_kwargs
        assert isinstance(call_kwargs["class_weights"], torch.Tensor)

        # Check that train_epoch and validate were called with is_multi_class=True
        first_train_call_args = mock_train_epoch.call_args_list[0].kwargs
        assert first_train_call_args.get("is_multi_class") is True
        first_val_call_args = mock_validate.call_args_list[0].kwargs
        assert first_val_call_args.get("is_multi_class") is True

    @patch("probity.training.trainer.SupervisedProbeTrainer.train_epoch")
    @patch("probity.training.trainer.SupervisedProbeTrainer.validate")
    def test_train_with_standardization_and_unscaling(
        self, mock_validate, mock_train_epoch, supervised_config, mock_activation_store
    ):
        mock_train_epoch.return_value = 0.5
        mock_validate.return_value = 0.4
        config = supervised_config
        config.standardize_activations = True
        config.device = "cpu"
        trainer = SupervisedProbeTrainer(config)
        probe_config = LogisticProbeConfig(
            input_size=5, model_name="test", hook_point="test"
        )
        model = MockLogisticProbe(probe_config, input_size=5)
        initial_direction = model._get_raw_direction_representation().clone()

        train_loader, val_loader = trainer.prepare_supervised_data(
            mock_activation_store, "pos"
        )

        # Check standardization stats are computed
        assert trainer.feature_mean is not None
        assert trainer.feature_std is not None
        feature_std_copy = trainer.feature_std.clone()  # For checking unscaling

        # Mock the _set_raw_direction_representation to check the final unscaled direction
        model._set_raw_direction_representation = MagicMock()

        # Run training
        history = trainer.train(model, train_loader, val_loader)

        # Check train/validate were called
        assert mock_train_epoch.call_count == config.num_epochs
        assert mock_validate.call_count == config.num_epochs

        # Check that unscaling happened (_set_raw_direction_representation was called)
        model._set_raw_direction_representation.assert_called_once()

        # Check that the passed direction was different from the initial one (scaled)
        # Note: We don't know the exact final scaled direction from train_epoch mock,
        # but we can verify the unscaling logic *would* be applied.
        # Let's simulate a scaled direction and check the unscaling math
        # Get the final learned direction (which was learned on standardized data)
        # In a real scenario, the model's weights would change during train_epoch
        # Here, we assume the initial_direction is the "learned" scaled direction for simplicity of checking the unscaling step
        simulated_learned_scaled_direction = initial_direction.clone()
        expected_unscaled = (
            simulated_learned_scaled_direction / feature_std_copy.squeeze(0)
        )

        # Get the direction passed to _set_raw_direction_representation
        call_args, _ = model._set_raw_direction_representation.call_args
        final_unscaled_direction = call_args[0]

        assert torch.allclose(final_unscaled_direction, expected_unscaled)

    @patch("probity.training.trainer.SupervisedProbeTrainer.train_epoch")
    @patch("probity.training.trainer.SupervisedProbeTrainer.validate")
    def test_train_early_stopping(
        self, mock_validate, mock_train_epoch, supervised_config, mock_activation_store
    ):
        # Simulate validation loss improving then plateauing
        mock_validate.side_effect = [
            0.5,
            0.4,
            0.3,
            0.3,
            0.3,
            0.3,
        ]  # Improves for 2 epochs, then stalls
        mock_train_epoch.return_value = 0.6
        config = supervised_config
        config.num_epochs = 10
        config.patience = 2
        trainer = SupervisedProbeTrainer(config)
        probe_config = LogisticProbeConfig(
            input_size=5, model_name="test", hook_point="test"
        )
        model = MockLogisticProbe(probe_config, input_size=5)
        train_loader, val_loader = trainer.prepare_supervised_data(
            mock_activation_store, "pos"
        )

        with patch("builtins.print") as mock_print:
            history = trainer.train(model, train_loader, val_loader)

        # Stops after epoch 4 (0-indexed): losses 0.5, 0.4, 0.3, 0.3 (counter=1), 0.3 (counter=2 -> stop)
        # Train/validate called for epochs 0, 1, 2, 3, 4
        assert mock_train_epoch.call_count == 5  # Changed from 4 to 5
        assert mock_validate.call_count == 5  # Changed from 4 to 5
        assert len(history["train_loss"]) == 5  # Changed from 4 to 5
        assert len(history["val_loss"]) == 5  # Changed from 4 to 5
        mock_print.assert_called_with(
            "\nEarly stopping triggered after 5 epochs"
        )  # Changed from 4 to 5


class TestDirectionalProbeTrainer:

    def test_init(self, directional_config):
        trainer = DirectionalProbeTrainer(directional_config)
        assert trainer.config == directional_config

    def test_prepare_supervised_data(self, directional_config, mock_activation_store):
        trainer = DirectionalProbeTrainer(directional_config)
        loader1, loader2 = trainer.prepare_supervised_data(mock_activation_store, "pos")

        assert isinstance(loader1, DataLoader)
        assert isinstance(loader2, DataLoader)
        assert loader1 is loader2  # Should return the same loader instance

        # Check loader properties
        assert loader1.batch_size == 10  # Batch size should be full dataset size
        assert len(loader1.dataset) == 10  # type: ignore

        # Check batch contents
        x_train_batch, y_batch, x_orig_batch = next(iter(loader1))
        assert x_train_batch.shape == (10, 5)
        assert y_batch.shape == (10, 1)
        assert x_orig_batch.shape == (10, 5)
        assert y_batch.dtype == torch.float32

    def test_train_no_standardization(self, directional_config, mock_activation_store):
        trainer = DirectionalProbeTrainer(directional_config)
        # Use MagicMock for probe config as DirectionalProbeConfig import was problematic
        probe_config = MagicMock()
        probe_config.input_size = 5
        probe_config.model_name = "test"
        probe_config.hook_point = "test"
        model = MockDirectionalProbe(probe_config, input_size=5)
        initial_raw_direction = (
            model._get_raw_direction_representation().clone()
        )  # Get initial random direction

        # Mock the fit method to check args and return a predictable direction
        mock_fitted_direction = torch.ones(1, 5) * 0.5
        model.fit = MagicMock(return_value=mock_fitted_direction)

        # Mock _set_raw_direction_representation to check what's being set
        model._set_raw_direction_representation = MagicMock()

        train_loader, _ = trainer.prepare_supervised_data(mock_activation_store, "pos")
        x_train_expected, y_expected, x_orig_expected = next(iter(train_loader))

        history = trainer.train(model, train_loader)

        # Check fit was called with correct data (X_train)
        model.fit.assert_called_once()
        call_args, _ = model.fit.call_args
        assert torch.equal(call_args[0], x_train_expected)
        assert torch.equal(call_args[1], y_expected)

        # Check _set_raw_direction_representation was called with the fitted direction
        model._set_raw_direction_representation.assert_called_once()
        set_call_args, _ = model._set_raw_direction_representation.call_args
        assert torch.equal(set_call_args[0], mock_fitted_direction)

        # Check history contains loss (should be calculated based on X_orig and final direction)
        assert isinstance(history, dict)
        assert "train_loss" in history
        assert "val_loss" in history
        assert len(history["train_loss"]) == 1
        assert not torch.isnan(torch.tensor(history["train_loss"][0]))
        assert history["train_loss"][0] == history["val_loss"][0]

    def test_train_with_standardization_and_unscaling(
        self, directional_config, mock_activation_store
    ):
        config = directional_config
        config.standardize_activations = True
        config.device = "cpu"
        trainer = DirectionalProbeTrainer(config)
        # Use MagicMock for probe config
        probe_config = MagicMock()
        probe_config.input_size = 5
        probe_config.model_name = "test"
        probe_config.hook_point = "test"
        model = MockDirectionalProbe(probe_config, input_size=5)

        # Prepare data to get standardization stats
        train_loader, _ = trainer.prepare_supervised_data(mock_activation_store, "pos")
        x_train_standardized, y_expected, x_orig_expected = next(iter(train_loader))

        assert trainer.feature_mean is not None
        assert trainer.feature_std is not None
        feature_std_copy = trainer.feature_std.clone().squeeze()  # Shape [dim]

        # Mock fit to return a direction presumably learned on standardized data
        mock_fitted_scaled_direction = torch.ones(1, 5) * 0.5
        model.fit = MagicMock(return_value=mock_fitted_scaled_direction)

        # Mock set raw direction to check the final unscaled version
        model._set_raw_direction_representation = MagicMock()

        # Mock the forward pass for loss calculation (uses original data)
        model.forward = MagicMock(return_value=torch.randn_like(y_expected))

        # Run training
        history = trainer.train(model, train_loader)

        # Check fit was called with standardized data
        model.fit.assert_called_once()
        call_args, _ = model.fit.call_args
        assert torch.equal(call_args[0], x_train_standardized)
        assert torch.equal(call_args[1], y_expected)

        # Check unscaling happened and the correct direction was set
        expected_unscaled_direction = mock_fitted_scaled_direction / feature_std_copy
        model._set_raw_direction_representation.assert_called_once()
        set_call_args, _ = model._set_raw_direction_representation.call_args
        final_unscaled_direction = set_call_args[0]

        assert torch.allclose(final_unscaled_direction, expected_unscaled_direction)

        # Check forward pass (for loss) was called with original data AFTER direction was set
        model.forward.assert_called_once()
        forward_call_args, _ = model.forward.call_args
        assert torch.equal(forward_call_args[0], x_orig_expected)

        # Check loss calculation
        assert "train_loss" in history
        assert len(history["train_loss"]) == 1
        assert not torch.isnan(torch.tensor(history["train_loss"][0]))



================================================
FILE: tutorials/1-probity-basics.py
================================================
# %% [markdown]
# # Logistic Probe Basics
# This file demonstrates a complete workflow for:
# 1. Creating a movie sentiment dataset
# 2. Training a logistic regression probe
# 3. Running inference with the probe
# 4. Saving the probe to disk (in multiple formats)
# 5. Loading the probe back from disk
# 6. Verifying that the loaded probe gives consistent results

# %% [markdown]
# ## Setup

# %% Setup and imports
import torch
import os
import json
import numpy as np
import random
import matplotlib.pyplot as plt
import torch.backends

# Probity imports
from probity.datasets.templated import TemplatedDataset
from probity.datasets.tokenized import TokenizedProbingDataset
from probity.probes import LogisticProbe, LogisticProbeConfig
from probity.training.trainer import SupervisedProbeTrainer, SupervisedTrainerConfig
from probity.probes.inference import ProbeInference
from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig

# Third-party imports
from transformers import AutoTokenizer
from transformer_lens import HookedTransformer
from neuronpedia.np_vector import NPVector

# Set torch device consistently
if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print(f"Using device: {device}")


def set_seed(seed=42):
    """Set random seed for reproducibility."""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)


# Set seed for reproducibility
set_seed(42)

# %% [markdown]
# ## Dataset Creation
# To train our probe, we'll create a dataset similar to the one used in the paper Linear Representations of Sentiment (Tigges & Hollinsworth, et al). We will use Probity's TemplatedDataset class, which allows us to specify templates with auto-populating blanks. For convenience, we have a simple function that applies the movie sentiment template.
#
# Once the TemplatedDataset is created, we simply convert it to a ProbingDataset (which gives us features like labels, word positions by character, and test-train splits) and then a TokenizedProbingDataset (which gives us additional information about token positions, context length, and so forth). We keep these distinct because TokenizedProbingDatasets are tied to specific models, whereas ProbingDatasets are not.

# %%
# Create movie sentiment dataset
adjectives = {
    "positive": [
        "incredible",
        "amazing",
        "fantastic",
        "awesome",
        "beautiful",
        "brilliant",
        "exceptional",
        "extraordinary",
        "fabulous",
        "great",
        "lovely",
        "outstanding",
        "remarkable",
        "wonderful",
    ],
    "negative": [
        "terrible",
        "awful",
        "horrible",
        "bad",
        "disappointing",
        "disgusting",
        "dreadful",
        "horrendous",
        "mediocre",
        "miserable",
        "offensive",
        "terrible",
        "unpleasant",
        "wretched",
    ],
}
verbs = {
    "positive": ["loved", "enjoyed", "adored"],
    "negative": ["hated", "disliked", "detested"],
}

# Create dataset using multi-step approach
# Step 1: Create templated dataset
movie_dataset = TemplatedDataset.from_movie_sentiment_template(
    adjectives=adjectives, verbs=verbs
)

# Step 2: Convert to probing dataset with automatic position finding
# and label mapping from sentiment attributes
probing_dataset = movie_dataset.to_probing_dataset(
    label_from_attributes="sentiment",
    label_map={"positive": 1, "negative": 0},
    auto_add_positions=True,
)

# Convert to tokenized dataset
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
    dataset=probing_dataset,
    tokenizer=tokenizer,
    padding=True,  # Add padding
    max_length=128,  # Specify max length
    add_special_tokens=True,
)

# %%
# Verify the tokenization worked
example = tokenized_dataset.examples[0]
print("First example tokens:", example.tokens)
print("First example text:", example.text)

# Now print examples from the probing dataset
print("Sample probing dataset examples:")
for i in np.random.choice(
    range(len(probing_dataset.examples)),
    size=min(6, len(probing_dataset.examples)),
    replace=False,
):
    ex = probing_dataset.examples[i]
    label = "positive" if ex.label == 1 else "negative"
    print(f"Example {i}: '{ex.text}' (Label: {label})")

# %% [markdown]
# ## Probe Training
# ### Configuration
# We're now ready to train the probe! We specify the training parameters via the three config objects below. The Pipeline manages the whole process, and the Trainer and Probe have their own settings.

# %% Configure model and probe
model_name = "gpt2"
hook_point = "blocks.7.hook_resid_pre"

# Set up logistic probe configuration
probe_config = LogisticProbeConfig(
    input_size=768,
    normalize_weights=True,
    bias=False,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=7,
    name="sentiment_probe",
)

# Set up trainer configuration
trainer_config = SupervisedTrainerConfig(
    batch_size=32,
    learning_rate=1e-3,
    num_epochs=10,
    weight_decay=0.01,
    train_ratio=0.8,  # 80-20 train-val split
    handle_class_imbalance=True,
    show_progress=True,
    standardize_activations=True,  # Empirically, this produces better results
)

pipeline_config = ProbePipelineConfig(
    dataset=tokenized_dataset,
    probe_cls=LogisticProbe,
    probe_config=probe_config,
    trainer_cls=SupervisedProbeTrainer,
    trainer_config=trainer_config,
    position_key="ADJ",  # We want to probe at the adjective position
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/sentiment_probe_cache",  # Cache activations for reuse
)

# %%
# Let's make sure the position key is correct
model = HookedTransformer.from_pretrained(model_name)

example = tokenized_dataset.examples[0]
print(f"Example text: {model.to_str_tokens(example.text, prepend_bos=False)}")
print(f"Token positions: {example.token_positions}")
print(f"Available position keys: {list(example.token_positions.keys())}")

# Verify the position key matches what's in the dataset
print(f"\nPipeline position key: {pipeline_config.position_key}")

# %% [markdown]
# Looks like the key tokens are in the right positions. GPT2's default behavior (as implemented in the AutoTokenizer) is not to add a BOS, so we're fine in that respect.
#
# ### Training
# Let's train the probe!

# %% Collect activations
# Create and run pipeline
pipeline = ProbePipeline(pipeline_config)

probe, training_history = pipeline.run()

# The probe now contains our learned sentiment direction
sentiment_direction = probe.get_direction()

# %%
# We can analyze training history
plt.figure(figsize=(10, 5))
plt.plot(training_history["train_loss"], label="Train Loss")
plt.plot(training_history["val_loss"], label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Probe Training History")
plt.legend()
plt.show()

# %% [markdown]
# # Now we'll test the standardized inference path
#
# ## Understanding how inference differs from training
#
# When running inference with a trained probe, it's important to understand:
#
# 1. **Position keys are only used during training** - While we specified `position_key="ADJ"` during training
#    to learn the sentiment direction specifically from adjective positions, the trained probe doesn't "remember"
#    this position information during inference.
#
# 2. **Inference applies to all tokens** - The probe is applied to every token in the input text, giving
#    a score for each token position. This means even function words like "the" and "and" get scores.
#
# 3. **Format independence** - You can run inference on any text format, not just the template used for training.
#    However, the probe will be most reliable when applied to similar linguistic patterns as seen during training.


# %%
# Method 1: Inference with the trained probe object
print("===== Inference with trained probe object =====")

# Create a ProbeInference instance with the trained probe
inference = ProbeInference(
    model_name=model_name, hook_point=hook_point, probe=probe, device=device
)

# Create some test examples with different formats than our training template
test_examples = [
    "The movie was incredible and I loved every minute of it.",  # Similar to training format
    "That film was absolutely terrible and I hated it.",  # Similar format with synonyms
    "The acting was mediocre but I liked the soundtrack.",  # Mixed sentiment
    "A beautiful story with outstanding performances.",  # Different format
]

# Get raw activation scores along the probe direction
raw_scores = inference.get_direction_activations(test_examples)
print(f"Raw activation scores shape: {raw_scores.shape}")

# Get probabilities (applies sigmoid for logistic probes)
probabilities = inference.get_probabilities(test_examples)
print(f"Probabilities shape: {probabilities.shape}")

# Display the results with token-level breakdown
for i, example in enumerate(test_examples):
    print(f"\nText: {example}")

    # Show token-level analysis
    tokens = inference.model.to_str_tokens(example, prepend_bos=False)
    token_probs = probabilities[i]

    # Make sure we only process as many tokens as we have probabilities for
    max_tokens = min(len(tokens), len(token_probs) - 1)

    print("\nToken-level sentiment analysis:")
    print(f"{'Token':<15} {'Sentiment Score':<15} {'Interpretation'}")
    print("-" * 50)

    for j in range(max_tokens):
        token = tokens[j]

        score = token_probs[j].item()
        interp = "Positive" if score > 0.6 else "Negative" if score < 0.1 else "Neutral"
        print(f"{token:<15} {score:.4f}           {interp}")

    # Get mean probability across tokens (excluding BOS) as sentiment score
    overall_sentiment = token_probs[1 : max_tokens + 1].mean().item()
    print(
        f"\nOverall document sentiment: {overall_sentiment:.4f} ({'Positive' if overall_sentiment > 0.6 else 'Negative' if overall_sentiment < 0.4 else 'Neutral'})"
    )

# %% [markdown]
# ## Observations on the token-level scores
#
# Looking at the token-level breakdown:
#
# 1. **Content words matter most**: Words like "incredible", "terrible", "loved", and "hated" show
#    the strongest sentiment signals, while function words like "the" and "and" are more neutral.
#
# 2. **Contextual influence**: Even seemingly neutral words can be affected by surrounding context.
#    The same word might get different scores in different sentences.
#
# 3. **Format robustness**: The probe works well on sentences with different structures than our training
#    template, showing it captures generalizable sentiment features.
#
# 4. **Aggregation strategy**: Taking the mean across all tokens is simple but not always optimal.
#    In practice, you might want to weight content words more heavily or focus on specific parts of speech.

# %% [markdown]
# # Save the probe and load it back

# %%
# Save the probe in both formats
save_dir = "./saved_probes"
os.makedirs(save_dir, exist_ok=True)

# Option 1: Save as PyTorch model (full state and config)
probe_path = f"{save_dir}/sentiment_probe.pt"
probe.save(probe_path)
print(f"Saved probe to {probe_path}")

# Option 2: Save in JSON format for easier sharing
json_path = f"{save_dir}/sentiment_probe.json"
probe.save_json(json_path)
print(f"Saved probe JSON to {json_path}")

# %%
# Method 2: Inference with the saved probe
print("\n===== Inference with loaded probe =====")

# Load the probe using from_saved_probe
loaded_inference = ProbeInference.from_saved_probe(
    model_name=model_name,
    hook_point=hook_point,
    probe_path=json_path,  # Load from the JSON format
    device=device,
)

# Get results with loaded probe
loaded_probabilities = loaded_inference.get_probabilities(test_examples)

# Compare results between original and loaded probes
for i, example in enumerate(test_examples):
    print(f"\nText: {example}")

    # Show token-level analysis
    tokens = loaded_inference.model.to_str_tokens(example, prepend_bos=False)
    token_probs = loaded_probabilities[i]

    # Make sure we only process as many tokens as we have probabilities for
    # Skip the BOS token in the probabilities
    max_tokens = min(len(tokens), len(token_probs) - 1)

    # Get document-level sentiment score (excluding BOS)
    overall_sentiment = token_probs[1 : max_tokens + 1].mean().item()
    print(
        f"Overall document sentiment (loaded probe): {overall_sentiment:.4f} ({'Positive' if overall_sentiment > 0.6 else 'Negative' if overall_sentiment < 0.4 else 'Neutral'})"
    )

# %% [markdown]
# # Verify consistency between original and loaded probes

# %%
# Compare original and loaded probe results
print("\n===== Comparing original vs loaded probe results =====")

# Get the probe directions
original_direction = probe.get_direction()
loaded_direction = loaded_inference.probe.get_direction()

# Print direction norm and first few components
print(f"Original direction norm: {torch.norm(original_direction):.6f}")
print(f"Original direction [0:5]: {original_direction[:5].cpu().tolist()}")

print(f"Loaded direction norm: {torch.norm(loaded_direction):.6f}")
print(f"Loaded direction [0:5]: {loaded_direction[:5].cpu().tolist()}")

# Check if the directions are similar
cos_sim = torch.nn.functional.cosine_similarity(
    original_direction, loaded_direction, dim=0
)
print(f"Cosine similarity between directions: {cos_sim.item():.6f}")

# Get new raw scores for comparison if we need them
raw_scores = inference.get_direction_activations(test_examples)
loaded_raw_scores = loaded_inference.get_direction_activations(test_examples)

# Analyze where the differences are greatest
token_diffs = torch.abs(raw_scores - loaded_raw_scores).mean(dim=0)
print("\nToken-level mean absolute differences:")
for i, diff in enumerate(token_diffs):
    print(f"Token {i}: {diff.item():.6f}")

max_diff_token = torch.argmax(token_diffs).item()
print(f"\nLargest difference at token position {max_diff_token}")

# Check difference excluding the BOS token
non_bos_raw_diff = torch.abs(raw_scores[:, 1:] - loaded_raw_scores[:, 1:]).mean().item()
non_bos_prob_diff = (
    torch.abs(probabilities[:, 1:] - loaded_probabilities[:, 1:]).mean().item()
)
print(f"Mean absolute difference in raw scores (excluding BOS): {non_bos_raw_diff:.8f}")
print(
    f"Mean absolute difference in probabilities (excluding BOS): {non_bos_prob_diff:.8f}"
)

if non_bos_prob_diff < 0.3:
    print("The non-BOS tokens show reasonable consistency.")

# Verify the trend is similar (correlation between original and loaded results)
flattened_orig = probabilities[:, 1:].flatten()
flattened_loaded = loaded_probabilities[:, 1:].flatten()
correlation = torch.corrcoef(torch.stack([flattened_orig, flattened_loaded]))[
    0, 1
].item()
print(f"Correlation between original and loaded probe outputs: {correlation:.6f}")

if correlation > 0.8:
    print("SUCCESS: Original and loaded probes show strong correlation (same trend)!")
elif correlation > 0.5:
    print("PARTIAL SUCCESS: Original and loaded probes show moderate correlation.")

# %% [markdown]
# # Customizing token selection for arbitrary text
#
# While probes are applied to all tokens during inference, you often want to focus on specific tokens
# for different text formats. Here's a pattern for selecting tokens of interest:


# %%
def analyze_with_focus(text, inference, focus_words=None, pos_tags=None):
    """Analyze text with focus on specific words or POS tags."""
    print(f"\nAnalyzing: {text}")

    # Get probabilities for all tokens
    probs = inference.get_probabilities([text])[0]
    tokens = inference.model.to_str_tokens(text, prepend_bos=False)

    # Handle token count mismatch - skip BOS token in probabilities
    max_tokens = min(len(tokens), len(probs) - 1)

    # Print all token scores
    print("\nAll token scores:")
    for i in range(max_tokens):
        token = tokens[i]
        score = probs[i].item()
        print(f"{token:<12} {score:.4f}")

    # Focus on specific words if requested
    if focus_words:
        print("\nFocusing on specific words:")
        focus_indices = [
            i
            for i in range(max_tokens)
            if any(word in tokens[i].lower() for word in focus_words)
        ]
        if focus_indices:
            focus_tokens = [tokens[i] for i in focus_indices]
            adjusted_indices = [i for i in focus_indices]
            focus_scores = probs[adjusted_indices].mean().item()
            print(f"Words: {', '.join(focus_tokens)}")
            print(f"Average sentiment: {focus_scores:.4f}")
        else:
            print("No matching words found")

    # Calculate overall sentiment
    avg_score = probs[1:max_tokens].mean().item()
    sentiment = (
        "Positive" if avg_score > 0.55 else "Negative" if avg_score < 0.4 else "Neutral"
    )
    print(f"\nOverall sentiment: {avg_score:.4f} ({sentiment})")

    return avg_score


# Try with some very different text formats
complex_review = "Despite excellent visuals and strong performances from the lead actors, the plot was incoherent and the pacing dragged in the middle."

# Analyze with focus on content words related to different aspects
print("\n===== Customized token selection =====")
analyze_with_focus(
    complex_review, inference, focus_words=["visual", "perform", "actor"]
)
analyze_with_focus(complex_review, inference, focus_words=["plot", "pac", "middle"])


# %% [markdown]
# # Uploading to Neuronpedia
#
# Once you've trained a probe, you can upload it to Neuronpedia. The process is straightforward:
# 1. Get the probe direction
# 2. Set your API key
# 3. Upload the probe
# 4. Profit

# %%
# Get the probe direction
probe_direction = probe.get_direction()

# Convert to list of floats
probe_direction = probe_direction.tolist()

from neuronpedia.np_vector import NPVector

# from neuronpedia.org/account
os.environ["NEURONPEDIA_API_KEY"] = "YOUR_API_KEY"

# upload the custom vector
np_vector = NPVector.new(
    label="sentiment_probe",
    model_id="gpt2-small",
    layer_num=7,
    hook_type="hook_resid_pre",
    vector=probe_direction,
    default_steer_strength=20,
)

# steer with it
responseJson = np_vector.steer_completion(prompt="The movie was")

print(json.dumps(responseJson, indent=2))
print("UI Steering at: " + responseJson["shareUrl"])

# %% [markdown]
# # Conclusion
#
# - **The preferred workflow is**:
#    - Train a probe (using a trainer or pipeline)
#    - Save it with probe.save() or probe.save_json()
#    - Load it with ProbeInference.from_saved_probe()
#    - Use get_direction_activations() for raw scores or get_probabilities() for transformed outputs



================================================
FILE: tutorials/2-dataset-creation.py
================================================
# %% [markdown]
# # Dataset Creation Tutorial
# This file demonstrates several ways to create datasets in Probity:
# 1. Creating a custom templated dataset (question classification)
# 2. Creating a non-templated dataset from scratch (facts vs. opinions)
# 3. Tokenizing both datasets
# 4. Training probes on each dataset
# 5. Comparing the results

# %% [markdown]
# ## Setup

# %%
import torch
import numpy as np
import random
import torch.backends
import matplotlib.pyplot as plt

from probity.datasets.templated import TemplatedDataset, TemplateVariable, Template
from probity.datasets.tokenized import TokenizedProbingDataset
from probity.datasets.base import ProbingDataset, ProbingExample, CharacterPositions
from probity.datasets.position_finder import Position, PositionFinder
from transformers import AutoTokenizer
from probity.probes import LogisticProbe, LogisticProbeConfig
from probity.training.trainer import SupervisedProbeTrainer, SupervisedTrainerConfig
from probity.probes.inference import ProbeInference
from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig
from transformer_lens import HookedTransformer

# Set torch device consistently
if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print(f"Using device: {device}")


def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)


# Set seed for reproducibility
set_seed(42)

# %% [markdown]
# ## Dataset Creation Method 1: Custom Templated Dataset
# First, we'll create a custom templated dataset for question classification.
# We'll distinguish between yes/no questions and open-ended questions.

# %%
# Create question classification dataset using templates
question_words = {
    "yes_no": ["Do", "Did", "Will", "Would", "Can"],
    "open_ended": ["What", "Why", "How"],
}

subjects = [
    "scientists",
    "doctors",
    "students",
    "researchers",
    "experts",
    "philosophers",
    "historians",
    "engineers",
]

verbs = [
    "think",
    "believe",
    "understand",
    "know",
    "observe",
    "theorize",
    "hypothesize",
    "discover",
]

objects = [
    "this concept",
    "these methods",
    "the solution",
    "the problem",
    "the results",
    "the experiment",
    "the evidence",
    "the analysis",
]

# Create TemplateVariables
question_word_var = TemplateVariable(
    name="Q_WORD",
    values=[word for lst in question_words.values() for word in lst],
    attributes={"question_type": [k for k, v in question_words.items() for _ in v]},
    class_bound=True,
    class_key="question_type",
)

subject_var = TemplateVariable(name="SUBJECT", values=subjects)

verb_var = TemplateVariable(name="VERB", values=verbs)

object_var = TemplateVariable(name="OBJECT", values=objects)

# Create templates
template1 = Template(
    template="{Q_WORD} {SUBJECT} {VERB} {OBJECT}?",
    variables={
        "Q_WORD": question_word_var,
        "SUBJECT": subject_var,
        "VERB": verb_var,
        "OBJECT": object_var,
    },
    attributes={"task": "question_classification"},
)

# Create the dataset
question_dataset = TemplatedDataset(
    templates=[template1], attributes={"description": "Question classification dataset"}
)

# Convert to probing dataset
question_probing_dataset = question_dataset.to_probing_dataset(
    label_from_attributes="question_type",
    label_map={"yes_no": 1, "open_ended": 0},
    auto_add_positions=True,
)

# Add position finder for the question mark instead of the question word
question_mark_finder = PositionFinder.from_regex(r"\?")
question_probing_dataset.add_target_positions(
    key="QUESTION_MARK", finder=question_mark_finder
)


# %% [markdown]
# ### Dataset 1 Results
# Below we see some random examples from the dataset, as well as the position types and some statistics.
# These sentences are relatively short and trivial, but serve well to demonstrate the concept.

# %%
# Display dataset statistics and examples
print(f"Dataset size: {len(question_probing_dataset.examples)}")
print(
    f"Max sequence length: {max(len(ex.text) for ex in question_probing_dataset.examples)}"
)

print("\nQuestion Classification Dataset Examples:")
for i in np.random.choice(
    range(len(question_probing_dataset.examples)), size=6, replace=False
):
    ex = question_probing_dataset.examples[i]
    label = "yes/no question" if ex.label == 1 else "open-ended question"
    print(f"Example {i}: '{ex.text}' (Label: {label})")

# Verify positions
sample_ex = question_probing_dataset.examples[0]
print(f"\nPosition types: {question_probing_dataset.position_types}")
if question_probing_dataset.position_types:
    for key in question_probing_dataset.position_types:
        if (
            sample_ex.character_positions
            and key in sample_ex.character_positions.keys()
        ):
            pos = sample_ex.character_positions[key]
            if isinstance(pos, Position):
                print(
                    f"{key} position: {pos.start}-{pos.end} "
                    f"('{sample_ex.text[pos.start:pos.end]}')"
                )
            else:
                for i, p in enumerate(pos):
                    print(
                        f"{key} position {i}: {p.start}-{p.end} "
                        f"('{sample_ex.text[p.start:p.end]}')"
                    )


# %% [markdown]
# ## Dataset Creation Method 2: Code vs. Not-Code Dataset
# Now we'll create a dataset for distinguishing between code and non-code text.
# A probe on this dataset should learn a context feature for the presence of code.

# %%
# Create code vs. not-code dataset
code_examples = [
    "def factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)",
    "for i in range(10):\n    print(i**2)",
    "import numpy as np\nnp.array([1, 2, 3])",
    "class Person:\n    def __init__(self, name):\n        self.name = name",
    "try:\n    x = 1/0\nexcept ZeroDivisionError:\n    print('Error')",
    "with open('file.txt', 'r') as f:\n    data = f.read()",
    "lambda x: x * 2",
    "def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)",
    "async def fetch_data():\n    return await api.get_data()",
    "@decorator\ndef my_function():\n    pass",
    "x = {'a': 1, 'b': 2}",
    "if x > 0 and y < 10:\n    print('Valid')",
    "while not done:\n    process()",
    "result = [i*i for i in range(10)]",
    "def map_function(f, items):\n    return [f(x) for x in items]",
]

non_code_examples = [
    "The quick brown fox jumps over the lazy dog.",
    "To be or not to be, that is the question.",
    "It was the best of times, it was the worst of times.",
    "In a hole in the ground there lived a hobbit.",
    "The only thing we have to fear is fear itself.",
    "Four score and seven years ago our fathers brought forth on this continent, a new nation.",
    "I have a dream that one day this nation will rise up.",
    "Ask not what your country can do for you, ask what you can do for your country.",
    "That's one small step for man, one giant leap for mankind.",
    "Life is like a box of chocolates. You never know what you're gonna get.",
    "May the Force be with you.",
    "Elementary, my dear Watson.",
    "The greatest glory in living lies not in never falling, but in rising every time we fall.",
    "The way to get started is to quit talking and begin doing.",
    "You miss 100% of the shots you don't take.",
]

# Create position finder for the end of each example
# PositionFinder can take a variety of inputs, including regexes, character positions, and templates
end_finder = PositionFinder.from_regex(r"[\S].{0,1}$")

# Create ProbingExamples
code_vs_text_examples = []

# Add code examples
for text in code_examples:
    positions_dict = {}
    end_pos = end_finder(text)
    if end_pos:
        positions_dict["END_POSITION"] = end_pos[0]

    code_vs_text_examples.append(
        ProbingExample(
            text=text,
            label=1,  # 1 for code
            label_text="code",
            character_positions=(
                CharacterPositions(positions_dict) if positions_dict else None
            ),
            attributes={"type": "code"},
        )
    )

# Add non-code examples
for text in non_code_examples:
    positions_dict = {}
    end_pos = end_finder(text)
    if end_pos:
        positions_dict["END_POSITION"] = end_pos[0]

    code_vs_text_examples.append(
        ProbingExample(
            text=text,
            label=0,  # 0 for non-code
            label_text="non_code",
            character_positions=(
                CharacterPositions(positions_dict) if positions_dict else None
            ),
            attributes={"type": "non_code"},
        )
    )

# Create the dataset
code_dataset = ProbingDataset(
    examples=code_vs_text_examples,
    task_type="classification",
    label_mapping={"non_code": 0, "code": 1},
    dataset_attributes={"description": "Code vs. non-code classification dataset"},
)

# %% [markdown]
# ### Dataset 2 Results
# Once again, we can display some examples, as well as the position types and some statistics.

# Display examples from the code vs. non-code dataset
print("Code vs. Non-Code Dataset Examples:")
for i in np.random.choice(range(len(code_dataset.examples)), size=6, replace=False):
    ex = code_dataset.examples[i]
    label = "code" if ex.label == 1 else "non-code"
    print(f"Example {i}: '{ex.text[:50]}...' (Label: {label})")

# Verify positions
sample_ex = code_dataset.examples[0]
print(f"\nPosition types: {code_dataset.position_types}")
if code_dataset.position_types:
    for key in code_dataset.position_types:
        if (
            sample_ex.character_positions
            and key in sample_ex.character_positions.keys()
        ):
            pos = sample_ex.character_positions[key]
            if isinstance(pos, Position):
                print(f"{key} position: {pos.start}-{pos.end}")
            else:
                for i, p in enumerate(pos):
                    print(f"{key} position {i}: {p.start}-{p.end}")

# %% [markdown]
# ## Dataset 3: First-Person vs Third-Person Perspective
# Now we'll create a dataset for distinguishing between first-person and third-person perspectives.
# This method is the same as Dataset 2, but with a different approach to finding the target position.

# %%
# Create dataset for first-person vs third-person perspective
first_person_statements = [
    "I think artificial intelligence is fascinating.",
    "I feel excited about the future of technology.",
    "I believe climate change requires immediate action.",
    "I want to learn more about quantum computing.",
    "I remember my first day at college vividly.",
    "I prefer reading books to watching movies.",
    "I hope to travel to Japan someday.",
    "I enjoy solving complex mathematical problems.",
    "I consider philosophy to be essential for critical thinking.",
    "I dream of starting my own company.",
    "I wish people would be kinder to each other.",
    "I need more time to complete this project.",
    "I wonder what the world will be like in 100 years.",
    "I regret not learning to play an instrument.",
    "I understand the importance of regular exercise.",
    "I desire to improve my public speaking skills.",
    "I plan to learn a new language next year.",
    "I question many assumptions about human nature.",
    "I value honesty above all other qualities.",
    "I imagine a world without poverty or hunger.",
]

third_person_statements = [
    "He thinks artificial intelligence is fascinating.",
    "She feels excited about the future of technology.",
    "They believe climate change requires immediate action.",
    "He wants to learn more about quantum computing.",
    "She remembers her first day at college vividly.",
    "He prefers reading books to watching movies.",
    "She hopes to travel to Japan someday.",
    "They enjoy solving complex mathematical problems.",
    "She considers philosophy to be essential for critical thinking.",
    "He dreams of starting his own company.",
    "She wishes people would be kinder to each other.",
    "He needs more time to complete this project.",
    "They wonder what the world will be like in 100 years.",
    "She regrets not learning to play an instrument.",
    "He understands the importance of regular exercise.",
    "She desires to improve her public speaking skills.",
    "They plan to learn a new language next year.",
    "He questions many assumptions about human nature.",
    "She values honesty above all other qualities.",
    "They imagine a world without poverty or hunger.",
]

# Create position finder for the beginning pronoun (I, He, She, They)
pronoun_finder = PositionFinder.from_regex(r"^(I|He|She|They)\b")

# Create ProbingExamples
perspective_examples = []

# Add first-person examples
for text in first_person_statements:
    positions_dict = {}
    pronoun_pos = pronoun_finder(text)
    if pronoun_pos:
        positions_dict["PRONOUN_POSITION"] = pronoun_pos[0]

    perspective_examples.append(
        ProbingExample(
            text=text,
            label=1,  # 1 for first-person
            label_text="first_person",
            character_positions=(
                CharacterPositions(positions_dict) if positions_dict else None
            ),
            attributes={"perspective": "first_person"},
        )
    )

# Add third-person examples
for text in third_person_statements:
    positions_dict = {}
    pronoun_pos = pronoun_finder(text)
    if pronoun_pos:
        positions_dict["PRONOUN_POSITION"] = pronoun_pos[0]

    perspective_examples.append(
        ProbingExample(
            text=text,
            label=0,  # 0 for third-person
            label_text="third_person",
            character_positions=(
                CharacterPositions(positions_dict) if positions_dict else None
            ),
            attributes={"perspective": "third_person"},
        )
    )

# Create the dataset
perspective_dataset = ProbingDataset(
    examples=perspective_examples,
    task_type="classification",
    label_mapping={"third_person": 0, "first_person": 1},
    dataset_attributes={
        "description": "First-person vs third-person perspective classification dataset"
    },
)

# %%

# Display examples from the perspective dataset
print("First-Person vs Third-Person Dataset Examples:")
for i in np.random.choice(
    range(len(perspective_dataset.examples)), size=6, replace=False
):
    ex = perspective_dataset.examples[i]
    label = "first-person" if ex.label == 1 else "third-person"
    print(f"Example {i}: '{ex.text}' (Label: {label})")

# Verify positions
sample_ex = perspective_dataset.examples[0]
print(f"\nPosition types: {perspective_dataset.position_types}")
if perspective_dataset.position_types:
    for key in perspective_dataset.position_types:
        if (
            sample_ex.character_positions
            and key in sample_ex.character_positions.keys()
        ):
            pos = sample_ex.character_positions[key]
            if isinstance(pos, Position):
                print(
                    f"{key} position: {pos.start}-{pos.end} "
                    f"('{sample_ex.text[pos.start:pos.end]}')"
                )
            else:
                for i, p in enumerate(pos):
                    print(
                        f"{key} position {i}: {p.start}-{p.end} "
                        f"('{sample_ex.text[p.start:p.end]}')"
                    )

# %% [markdown]
# ## Tokenization
# Now that we have our datasets, we need to tokenize them for use with our model.

# %%
# Set up tokenizer
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")
tokenizer.pad_token = tokenizer.eos_token

# Tokenize the question dataset
tokenized_question_dataset = TokenizedProbingDataset.from_probing_dataset(
    dataset=question_probing_dataset,
    tokenizer=tokenizer,
    padding="max_length",
    max_length=12,
    add_special_tokens=True,
)
print("\nQuestion Dataset:")
print(f"Dataset size: {len(tokenized_question_dataset.examples)}")
print(
    f"Max token length: {max(len(ex.tokens) for ex in tokenized_question_dataset.examples)}"
)

# Tokenize the code dataset
tokenized_code_dataset = TokenizedProbingDataset.from_probing_dataset(
    dataset=code_dataset,
    tokenizer=tokenizer,
    padding="max_length",
    max_length=128,  # Longer max_length for code examples
    add_special_tokens=True,
)
print("\nCode Dataset:")
print(f"Dataset size: {len(tokenized_code_dataset.examples)}")
print(
    f"Max token length: {max(len(ex.tokens) for ex in tokenized_code_dataset.examples)}"
)

# Tokenize the perspective dataset
tokenized_perspective_dataset = TokenizedProbingDataset.from_probing_dataset(
    dataset=perspective_dataset,
    tokenizer=tokenizer,
    padding="max_length",
    max_length=24,
    add_special_tokens=True,
)
print("\nPerspective Dataset:")
print(f"Dataset size: {len(tokenized_perspective_dataset.examples)}")
print(
    f"Max token length: {max(len(ex.tokens) for ex in tokenized_perspective_dataset.examples)}"
)

# %% [markdown]
# ## Probe Training
# Now let's train probes for each dataset to see if we can detect their respective features.
# We'll try multiple model layers to see which ones work best.

# %% Configure models and hook points
model_name = "google/gemma-2-2b"

# IMPROVEMENT: Test multiple hook points (layers) to find the best one
hook_points = [f"blocks.{layer}.hook_resid_pre" for layer in [8, 16, 24]]

# Get model's hidden dimension dynamically
model = HookedTransformer.from_pretrained(model_name, device=device)
hidden_size = model.cfg.d_model
print(f"Model {model_name} has hidden dimension: {hidden_size}")

# Common trainer configuration
trainer_config = SupervisedTrainerConfig(
    batch_size=16,
    learning_rate=1e-3,
    num_epochs=10,
    weight_decay=0.01,
    train_ratio=0.7,  # 70-30 train-val split
    handle_class_imbalance=True,
    show_progress=True,
    device=device,
)

# %% Train question type probes across multiple layers
question_probes = {}
question_training_histories = {}

for hook_point in hook_points:
    layer = int(hook_point.split(".")[1])

    # Set up probe configuration
    question_probe_config = LogisticProbeConfig(
        input_size=hidden_size,  # Use dynamically determined hidden size
        normalize_weights=True,
        bias=True,
        model_name=model_name,
        hook_point=hook_point,
        hook_layer=layer,
        name=f"question_type_probe_layer_{layer}",
    )

    # Create pipeline config
    question_pipeline_config = ProbePipelineConfig(
        dataset=tokenized_question_dataset,
        probe_cls=LogisticProbe,
        probe_config=question_probe_config,
        trainer_cls=SupervisedProbeTrainer,
        trainer_config=trainer_config,
        position_key="QUESTION_MARK",  # Changed from Q_WORD to QUESTION_MARK
        model_name=model_name,
        hook_points=[hook_point],
        cache_dir=f"./cache/question_probe_cache_layer_{layer}",
    )

    # Train the probe
    print(f"Training Question Type Probe for layer {layer}...")
    question_pipeline: ProbePipeline = ProbePipeline(question_pipeline_config)
    probe, history = question_pipeline.run()

    # Store results
    question_probes[layer] = probe
    question_training_histories[layer] = history


# %% Train code detection probes across multiple layers
code_probes = {}
code_training_histories = {}

for hook_point in hook_points:
    layer = int(hook_point.split(".")[1])

    # Set up probe configuration
    code_probe_config = LogisticProbeConfig(
        input_size=hidden_size,  # Use dynamically determined hidden size
        normalize_weights=True,
        bias=True,
        model_name=model_name,
        hook_point=hook_point,
        hook_layer=layer,
        name=f"code_probe_layer_{layer}",
    )

    # Create pipeline config
    code_pipeline_config = ProbePipelineConfig(
        dataset=tokenized_code_dataset,
        probe_cls=LogisticProbe,
        probe_config=code_probe_config,
        trainer_cls=SupervisedProbeTrainer,
        trainer_config=trainer_config,
        position_key="END_POSITION",  # Probe at the end of the example
        model_name=model_name,
        hook_points=[hook_point],
        cache_dir=f"./cache/code_probe_cache_layer_{layer}",
    )

    # Train the probe
    print(f"Training Code Detection Probe for layer {layer}...")
    code_pipeline: ProbePipeline = ProbePipeline(code_pipeline_config)
    probe, history = code_pipeline.run()

    # Store results
    code_probes[layer] = probe
    code_training_histories[layer] = history

# %% Train perspective probes across multiple layers
perspective_probes = {}
perspective_training_histories = {}

for hook_point in hook_points:
    layer = int(hook_point.split(".")[1])

    # Set up probe configuration
    perspective_probe_config = LogisticProbeConfig(
        input_size=hidden_size,  # Use dynamically determined hidden size
        normalize_weights=True,
        bias=True,
        model_name=model_name,
        hook_point=hook_point,
        hook_layer=layer,
        name=f"perspective_probe_layer_{layer}",
    )

    # Create pipeline config
    perspective_pipeline_config = ProbePipelineConfig(
        dataset=tokenized_perspective_dataset,
        probe_cls=LogisticProbe,
        probe_config=perspective_probe_config,
        trainer_cls=SupervisedProbeTrainer,
        trainer_config=trainer_config,
        position_key="PRONOUN_POSITION",  # Probe at the pronoun position
        model_name=model_name,
        hook_points=[hook_point],
        cache_dir=f"./cache/perspective_probe_cache_layer_{layer}",
    )

    # Train the probe
    print(f"Training Perspective Probe for layer {layer}...")
    perspective_pipeline: ProbePipeline = ProbePipeline(perspective_pipeline_config)
    probe, history = perspective_pipeline.run()

    # Store results
    perspective_probes[layer] = probe
    perspective_training_histories[layer] = history

# %% Plot training histories across layers
plt.figure(figsize=(15, 20))

# Plot question probe training histories
plt.subplot(3, 1, 1)
for layer, history in question_training_histories.items():
    plt.plot(history["train_loss"], label=f"Layer {layer} Train")
    plt.plot(history["val_loss"], label=f"Layer {layer} Val", linestyle="--")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Question Type Probe Training History Across Layers")
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")

# Plot code probe training histories
plt.subplot(3, 1, 2)
for layer, history in code_training_histories.items():
    plt.plot(history["train_loss"], label=f"Layer {layer} Train")
    plt.plot(history["val_loss"], label=f"Layer {layer} Val", linestyle="--")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Code Detection Probe Training History Across Layers")
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")

# Plot perspective probe training histories
plt.subplot(3, 1, 3)
for layer, history in perspective_training_histories.items():
    plt.plot(history["train_loss"], label=f"Layer {layer} Train")
    plt.plot(history["val_loss"], label=f"Layer {layer} Val", linestyle="--")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Perspective Probe Training History Across Layers")
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")

plt.tight_layout()
plt.show()

# %% Compare layer performance and find the best layer for each task
layer_results = {}

for task, histories in [
    ("question_type", question_training_histories),
    ("code_detection", code_training_histories),
    ("perspective", perspective_training_histories),
]:
    layer_results[task] = {}
    for layer, history in histories.items():
        # Use the final validation loss as the metric
        final_val_loss = history["val_loss"][-1]
        layer_results[task][layer] = final_val_loss

# Find the best layer for each task
best_question_layer = min(
    layer_results["question_type"], key=lambda k: layer_results["question_type"][k]
)
best_code_layer = min(
    layer_results["code_detection"], key=lambda k: layer_results["code_detection"][k]
)
best_perspective_layer = min(
    layer_results["perspective"], key=lambda k: layer_results["perspective"][k]
)

print(f"Best layer for question type classification: {best_question_layer}")
print(f"Best layer for code detection: {best_code_layer}")
print(f"Best layer for perspective classification: {best_perspective_layer}")

# %% [markdown]
# ## Inference with Best Trained Probes
# Now let's test our best trained probes on some new examples. Note that we've set some manual thresholds for the confidence scores to make the results more prominent.
# However, in practice, you may want to use a more sophisticated approach to setting these thresholds (e.g. a ROC curve). Nevertheless, if you look at the results it should
# be clear that there is a dichotomy between the classes.

# You could also get the "activation in the probe direction" instead of the probability, and then use that to make predictions. This can be done with the
# `ProbeInference.get_direction_activations()` method.

# %%
# Inference with best Question Type Probe
best_hook_point = f"blocks.{best_question_layer}.hook_resid_pre"
question_inference = ProbeInference(
    model_name=model_name,
    hook_point=best_hook_point,
    probe=question_probes[best_question_layer],
    device=device,
)

# Create some test examples
test_questions = [
    "Is artificial intelligence a threat to humanity?",
    "Are electric vehicles better for the environment?",
    "What factors contribute to climate change?",
    "How do quantum computers work?",
    "Will humans colonize Mars within this century?",
    "Where should I go for my next vacation?",
]

# Get probabilities (applies sigmoid for logistic probes)
question_probs = question_inference.get_probabilities(test_questions)

# Display the results
print(f"===== Question Type Probe Results (Layer {best_question_layer}) =====")
print("Probing at the question mark position")
for i, example in enumerate(test_questions):
    print(f"\nText: {example}")
    # Get mean probability across all tokens as an overall score
    overall_score = question_probs[i].mean().item()
    prediction = "Yes/No Question" if overall_score > 0.90 else "Open-Ended Question"
    confidence = max(overall_score, 1 - overall_score)
    print(
        f"Prediction: {prediction} (confidence: {confidence:.4f}) (overall score: {overall_score:.4f})"
    )

# %%
# Inference with best Code Detection Probe
best_hook_point = f"blocks.{best_code_layer}.hook_resid_pre"
code_inference = ProbeInference(
    model_name=model_name,
    hook_point=best_hook_point,
    probe=code_probes[best_code_layer],
    device=device,
)

# Create some test examples
test_code_examples = [
    "from datetime import datetime\nprint(datetime.now())",
    "All the world's a stage, and all the men and women merely players.",
    "def fibonacci(n):\n    a, b = 0, 1\n    for _ in range(n):\n        a, b = b, a + b\n    return a",
    "The early bird catches the worm, but the second mouse gets the cheese.",
    "x = 5\ny = 10\nprint(x + y)",
    "Four score and seven years ago our fathers brought forth on this continent a new nation.",
]

# Get probabilities (applies sigmoid for logistic probes)
code_probs = code_inference.get_probabilities(test_code_examples)

# Display the results
print(f"===== Code Detection Probe Results (Layer {best_code_layer}) =====")
print("Probing at the end of examples")
for i, example in enumerate(test_code_examples):
    print(f"\nText: {example[:50]}...")
    # Get mean probability across all tokens as an overall score
    overall_score = code_probs[i].mean().item()
    prediction = "Code" if overall_score > 0.5 else "Not Code"
    confidence = max(overall_score, 1 - overall_score)
    print(
        f"Prediction: {prediction} (confidence: {confidence:.4f}) (overall score: {overall_score:.4f})"
    )

# %%
# Inference with best Perspective Probe
best_hook_point = f"blocks.{best_perspective_layer}.hook_resid_pre"
perspective_inference = ProbeInference(
    model_name=model_name,
    hook_point=best_hook_point,
    probe=perspective_probes[best_perspective_layer],
    device=device,
)

# Create some test examples
test_perspective_statements = [
    "I am going to study computer science in college.",
    "She is going to study computer science in college.",
    "I don't think this approach will work in practice.",
    "They don't think this approach will work in practice.",
    "I always wanted to visit the Grand Canyon.",
    "He always wanted to visit the Grand Canyon.",
]

# Get probabilities (applies sigmoid for logistic probes)
perspective_probs = perspective_inference.get_probabilities(test_perspective_statements)

# Display the results
print(f"===== Perspective Probe Results (Layer {best_perspective_layer}) =====")
print("Probing at the pronoun position")
for i, example in enumerate(test_perspective_statements):
    print(f"\nText: {example}")
    # Get mean probability across all tokens as an overall score
    overall_score = perspective_probs[i].mean().item()
    prediction = "First-Person" if overall_score < 0.70 else "Third-Person"
    confidence = max(overall_score, 1 - overall_score)
    print(
        f"Prediction: {prediction} (confidence: {confidence:.4f}) (overall score: {overall_score:.4f})"
    )

# %% [markdown]
# ## Conclusion
#
# This tutorial demonstrated multiple approaches to creating datasets with Probity:
#
# 1. Using the **TemplatedDataset** class to create structured datasets with predefined
#    patterns and variables - demonstrated with our question type classification dataset.
#
# 2. Creating **non-templated datasets** directly using ProbingExample objects, as
#    shown with our code detection and perspective classification datasets.
#
# In all cases, we ensured that position information was carefully tracked, allowing our
# probes to focus on specific parts of the input that are most relevant to the
# classification task:
#
# - For question classification, we probed at the question mark position (demonstrating that models often deposit sentence-level information in punctuation at the end of a sentence or phrase)
# - For code detection, we probed at the end of each example
# - For perspective classification, we probed at the initial pronoun position
#
# The results show that:
#
# - Different tasks are best probed from different layers
# - Models encode rich semantic information that can be extracted with linear probes
# - Position-specific probing often gives better insights than whole-sequence probing
#
# For your own projects, consider which dataset creation approach best suits your needs
# based on the structure and source of your data, and carefully select positions that
# are most relevant to your probing task.
# %%



================================================
FILE: tutorials/3-probe-variants.py
================================================
# %% [markdown]
# # Probe Type Comparison
# This file demonstrates how to:
# 1. Create a movie sentiment dataset
# 2. Train different types of probes (Linear, Logistic, KMeans, PCA, MeanDiff)
# 3. Run inference with each probe
# 4. Compare the learned directions using cosine similarity

# %% [markdown]
# ## Setup

# %% Setup and imports
import os
import torch
import numpy as np
import random
import torch.backends
import matplotlib.pyplot as plt
from tabulate import tabulate

from probity.datasets.templated import TemplatedDataset
from probity.datasets.tokenized import TokenizedProbingDataset
from transformers import AutoTokenizer
from probity.probes import (
    LinearProbe,
    LinearProbeConfig,
    LogisticProbe,
    LogisticProbeConfig,
    KMeansProbe,
    KMeansProbeConfig,
    PCAProbe,
    PCAProbeConfig,
    MeanDifferenceProbe,
    MeanDiffProbeConfig,
)
from probity.training.trainer import (
    SupervisedProbeTrainer,
    SupervisedTrainerConfig,
    DirectionalProbeTrainer,
    DirectionalTrainerConfig,
)
from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig
from probity.probes.inference import ProbeInference

# Set torch device consistently
if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print(f"Using device: {device}")


def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)


# Set seed for reproducibility
set_seed(42)

# %% [markdown]
# ## Dataset Creation
# We'll use the same movie sentiment dataset as in 1-probity-basics.py for consistency.

# %%
# Create movie sentiment dataset
adjectives = {
    "positive": [
        "incredible",
        "amazing",
        "fantastic",
        "awesome",
        "beautiful",
        "brilliant",
        "exceptional",
        "extraordinary",
        "fabulous",
        "great",
        "lovely",
        "outstanding",
        "remarkable",
        "wonderful",
    ],
    "negative": [
        "terrible",
        "awful",
        "horrible",
        "bad",
        "disappointing",
        "disgusting",
        "dreadful",
        "horrendous",
        "mediocre",
        "miserable",
        "offensive",
        "terrible",
        "unpleasant",
        "wretched",
    ],
}
verbs = {
    "positive": ["loved", "enjoyed", "adored"],
    "negative": ["hated", "disliked", "detested"],
}

# Create dataset using factory method
movie_dataset = TemplatedDataset.from_movie_sentiment_template(
    adjectives=adjectives, verbs=verbs
)

# Convert to probing dataset with automatic position finding
# and label mapping from sentiment metadata
probing_dataset = movie_dataset.to_probing_dataset(
    label_from_attributes="sentiment",
    label_map={"positive": 1, "negative": 0},
    auto_add_positions=True,
)

# Convert to tokenized dataset
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(
    dataset=probing_dataset,
    tokenizer=tokenizer,
    padding=True,  # Add padding
    max_length=128,  # Specify max length
    add_special_tokens=True,
)

# %% [markdown]
# ## Probe Training
# We'll train each type of probe and store their results.

# %%
# Common configuration
model_name = "gpt2"
hook_point = "blocks.7.hook_resid_pre"
hidden_size = 768  # GPT-2's hidden size

# Common trainer configuration for supervised probes
supervised_trainer_config = SupervisedTrainerConfig(
    batch_size=32,
    learning_rate=1e-3,
    num_epochs=10,
    weight_decay=0.01,
    train_ratio=0.8,
    handle_class_imbalance=True,
    show_progress=True,
    device=device,
    standardize_activations=True,
)

# Common trainer configuration for directional probes
directional_trainer_config = DirectionalTrainerConfig(
    batch_size=32, device=device, standardize_activations=True
)

# Dictionary to store probes and their training histories
probes = {}
training_histories = {}

# %% [markdown]
# ### 1. Linear Probe
# A simple linear probe that learns a direction through gradient descent.

# %%
# Train Linear Probe
linear_probe_config = LinearProbeConfig(
    input_size=hidden_size,
    normalize_weights=True,
    bias=False,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=7,
    name="sentiment_linear_probe",
)

linear_pipeline_config = ProbePipelineConfig(
    dataset=tokenized_dataset,
    probe_cls=LinearProbe,
    probe_config=linear_probe_config,
    trainer_cls=SupervisedProbeTrainer,
    trainer_config=supervised_trainer_config,
    position_key="ADJ",
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/linear_probe_cache",
)

print("Training Linear Probe...")
linear_pipeline = ProbePipeline(linear_pipeline_config)
probe, history = linear_pipeline.run()
probes["linear"] = probe
training_histories["linear"] = history

# %% [markdown]
# ### 2. Logistic Probe
# A logistic regression probe that learns a direction for binary classification.

# %%
# Train Logistic Probe
logistic_probe_config = LogisticProbeConfig(
    input_size=hidden_size,
    normalize_weights=True,
    bias=False,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=7,
    name="sentiment_logistic_probe",
)

logistic_pipeline_config = ProbePipelineConfig(
    dataset=tokenized_dataset,
    probe_cls=LogisticProbe,
    probe_config=logistic_probe_config,
    trainer_cls=SupervisedProbeTrainer,
    trainer_config=supervised_trainer_config,
    position_key="ADJ",
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/logistic_probe_cache",
)

print("Training Logistic Probe...")
logistic_pipeline = ProbePipeline(logistic_pipeline_config)
probe, history = logistic_pipeline.run()
probes["logistic"] = probe
training_histories["logistic"] = history

# %% [markdown]
# ### 3. KMeans Probe
# A probe that finds directions through K-means clustering.

# %%
# Train KMeans Probe
kmeans_probe_config = KMeansProbeConfig(
    input_size=hidden_size,
    n_clusters=2,
    normalize_weights=True,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=7,
    name="sentiment_kmeans_probe",
)

kmeans_pipeline_config = ProbePipelineConfig(
    dataset=tokenized_dataset,
    probe_cls=KMeansProbe,
    probe_config=kmeans_probe_config,
    trainer_cls=DirectionalProbeTrainer,
    trainer_config=directional_trainer_config,
    position_key="ADJ",
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/kmeans_probe_cache",
)

print("Training KMeans Probe...")
kmeans_pipeline = ProbePipeline(kmeans_pipeline_config)
probe, history = kmeans_pipeline.run()
probes["kmeans"] = probe
training_histories["kmeans"] = history

# %% [markdown]
# ### 4. PCA Probe
# A probe that finds directions through principal component analysis.

# %%
# Train PCA Probe
pca_probe_config = PCAProbeConfig(
    input_size=hidden_size,
    n_components=1,
    normalize_weights=True,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=7,
    name="sentiment_pca_probe",
)

pca_pipeline_config = ProbePipelineConfig(
    dataset=tokenized_dataset,
    probe_cls=PCAProbe,
    probe_config=pca_probe_config,
    trainer_cls=DirectionalProbeTrainer,
    trainer_config=directional_trainer_config,
    position_key="ADJ",
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/pca_probe_cache",
)

print("Training PCA Probe...")
pca_pipeline = ProbePipeline(pca_pipeline_config)
probe, history = pca_pipeline.run()
probes["pca"] = probe
training_histories["pca"] = history

# %% [markdown]
# ### 5. Mean Difference Probe
# A probe that finds directions through mean differences between classes.

# %%
# Train Mean Difference Probe
meandiff_probe_config = MeanDiffProbeConfig(
    input_size=hidden_size,
    normalize_weights=True,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=7,
    name="sentiment_meandiff_probe",
)

meandiff_pipeline_config = ProbePipelineConfig(
    dataset=tokenized_dataset,
    probe_cls=MeanDifferenceProbe,
    probe_config=meandiff_probe_config,
    trainer_cls=DirectionalProbeTrainer,
    trainer_config=directional_trainer_config,
    position_key="ADJ",
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/meandiff_probe_cache",
)

print("Training Mean Difference Probe...")
meandiff_pipeline = ProbePipeline(meandiff_pipeline_config)
probe, history = meandiff_pipeline.run()
probes["meandiff"] = probe
training_histories["meandiff"] = history

# %%
# save all probes
save_dir = "./sentiment_probes"
os.makedirs(save_dir, exist_ok=True)
for probe_type, probe in probes.items():
    json_path = f"{save_dir}/{probe_type}_probe.json"
    probe.save_json(json_path)
    print(f"Saved probe JSON to {json_path}")

# %% [markdown]
# ## Training History Visualization
# Let's plot the training histories for the supervised probes (Linear and Logistic).

# %%
plt.figure(figsize=(12, 6))

# Plot training histories for supervised probes
for probe_type in ["linear", "logistic"]:
    history = training_histories[probe_type]
    plt.plot(history["train_loss"], label=f"{probe_type.title()} Train")
    plt.plot(history["val_loss"], label=f"{probe_type.title()} Val", linestyle="--")

plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training History for Supervised Probes")
plt.legend()
plt.show()

# %% [markdown]
# ## Inference Comparison
# Now let's compare how each probe performs on some test examples.

# %%
# Create test examples
test_examples = [
    "The movie was incredible and I loved every minute of it.",
    "That film was absolutely terrible and I hated it.",
    "The acting was mediocre but I liked the soundtrack.",
    "A beautiful story with outstanding performances.",
]

# Create inference objects for each probe
inference_objects = {}
for probe_type, probe in probes.items():
    inference_objects[probe_type] = ProbeInference(
        model_name=model_name, hook_point=hook_point, probe=probe, device=device
    )

# Get probabilities for each probe
results = {}
for probe_type, inference in inference_objects.items():
    results[probe_type] = inference.get_direction_activations(test_examples)

# Display results
print("\nInference Results:")
print("-" * 80)
for i, example in enumerate(test_examples):
    print(f"\nText: {example}")
    print("-" * 40)
    for probe_type, probs in results.items():
        # Get mean probability across all tokens as an overall sentiment score
        overall_sentiment = probs[i].mean().item()
        print(f"{probe_type.title()} Probe: {overall_sentiment:.4f}")

# %% [markdown]
# ## Direction Comparison
# Finally, let's compare the learned directions using cosine similarity.

# %%
# Get directions for each probe
directions = {}
for probe_type, probe in probes.items():
    directions[probe_type] = probe.get_direction()

# Create a matrix of cosine similarities
n_probes = len(directions)
similarity_matrix = torch.zeros((n_probes, n_probes))
probe_types = list(directions.keys())

for i, probe_type1 in enumerate(probe_types):
    for j, probe_type2 in enumerate(probe_types):
        # Compute cosine similarity between directions
        cos_sim = torch.nn.functional.cosine_similarity(
            directions[probe_type1], directions[probe_type2], dim=0
        )
        similarity_matrix[i, j] = cos_sim

# Display similarity matrix
print("\nDirection Similarity Matrix:")
print("-" * 80)

# Prepare data for tabulate
table_data = []
for i, probe_type1 in enumerate(probe_types):
    row = [probe_type1] + [f"{similarity_matrix[i, j]:.4f}" for j in range(n_probes)]
    table_data.append(row)

# Create headers
headers = ["Probe Type"] + probe_types

# Print table using tabulate
print(tabulate(table_data, headers=headers, tablefmt="grid"))

# %% [markdown]
# ## Notes
# You'll see that the linear probe has the lowest cosine similarity to the other probes, though the cosine similarity is still meaningful given how
# high-dimensional the activation space is. It likely differs due to the different loss function (MSE vs. cross-entropy) which makes it a regression task
# instead of a classification task.

# %% [markdown]
# ## Conclusion
# This comparison shows how different probe types learn directions for the same task:
#
# 1. **Supervised Probes** (Linear and Logistic):
#    - Learn through gradient descent
#    - Can handle complex decision boundaries
#    - May take longer to train
#
# 2. **Directional Probes** (KMeans, PCA, MeanDiff):
#    - Learn through direct computation
#    - Faster to train
#    - May be more interpretable
#
# The cosine similarities between directions show how different methods may converge
# to similar or different solutions, just as shown in the *Language Models Linearly Represent Sentiment* paper. High similarities suggest the methods are
# finding consistent patterns in the activation space.
# %%



================================================
FILE: tutorials/4-multiclass-probe.py
================================================
# %% [markdown]
# # Multi-Class Logistic Probe Tutorial
# This tutorial demonstrates how to create, train, and use the `MultiClassLogisticProbe`.
# We will create a simple dataset where the task is to identify the starting letter
# of a word from a small set of categories (e.g., 'A', 'B', 'C', 'Other').

# %% [markdown]
# ## Setup
# Import necessary libraries and set up device/seed.

# %%
import torch
import numpy as np
import random
import torch.backends
import matplotlib.pyplot as plt
from typing import List, Dict, Union

from probity.datasets.base import ProbingDataset, ProbingExample, CharacterPositions
from probity.datasets.tokenized import TokenizedProbingDataset
from probity.datasets.position_finder import Position, PositionFinder
from probity.probes import MultiClassLogisticProbe, MultiClassLogisticProbeConfig
from probity.training.trainer import SupervisedProbeTrainer, SupervisedTrainerConfig
from probity.probes.inference import ProbeInference
from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig

from transformers import AutoTokenizer
from transformer_lens import HookedTransformer

# Set torch device consistently
if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print(f"Using device: {device}")


# Function to set seed for reproducibility
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)


# Set seed
set_seed(42)

# %% [markdown]
# ## 1. Dataset Creation
# We create a dataset of words and classify them based on their starting letter.
# Categories: 0='A', 1='B', 2='C', 3='Other'.
# We will use `PositionFinder` to mark the position of the first character.

# %%
# Define words and categories
words = [
    # A words
    "Apple",
    "Ant",
    "Ace",
    "Art",
    "Actor",
    "Anchor",
    "Arrow",
    "Angel",
    "Autumn",
    "Attic",
    "Apron",
    "Author",
    "Airplane",
    "Argument",
    "Avenue",
    "Award",
    "Action",
    "Adventure",
    "Answer",
    "Article",
    # B words
    "Ball",
    "Bat",
    "Box",
    "Bus",
    "Banana",
    "Bear",
    "Book",
    "Boat",
    "Bread",
    "Brick",
    "Bridge",
    "Button",
    "Bubble",
    "Branch",
    "Breeze",
    "Bottle",
    "Balance",
    "Basket",
    "Beach",
    "Bedroom",
    # C words
    "Cat",
    "Cup",
    "Car",
    "Cow",
    "Cake",
    "Coat",
    "Coin",
    "Clock",
    "Cloud",
    "Castle",
    "Circle",
    "Candle",
    "Camera",
    "Canvas",
    "Corner",
    "Country",
    "Color",
    "Comfort",
    "Coffee",
    "Crayon",
    # Other starting letters
    "Dog",
    "Duck",
    "Door",
    "Desk",
    "Drum",
    "Dream",
    "Dance",
    "Diamond",
    "Elephant",
    "Egg",
    "Ear",
    "Eye",
    "Eagle",
    "Earth",
    "Engine",
    "Elbow",
    "Fish",
    "Fan",
    "Fox",
    "Frog",
    "Fire",
    "Flag",
    "Floor",
    "Flower",
    "Goat",
    "Gum",
    "Gas",
    "Gift",
    "Grape",
    "Glass",
    "Glove",
    "Grass",
    "Hat",
    "Hand",
    "Heart",
    "House",
    "Honey",
    "Horse",
    "Hammer",
    "Harp",
    "Island",
    "Ice",
    "Ink",
    "Idea",
    "Image",
    "Iron",
    "Insect",
    "Igloo",
    "Jacket",
    "Jar",
    "Jelly",
    "Jewel",
    "Joke",
    "Juice",
    "Jungle",
    "Journey",
    "Kite",
    "Key",
    "King",
    "Kitten",
    "Knife",
    "Knee",
    "Koala",
    "Kitchen",
    "Lamp",
    "Leaf",
    "Lion",
    "Lake",
    "Lemon",
    "Letter",
    "Light",
    "Lizard",
    "Map",
    "Moon",
    "Mouse",
    "Milk",
    "Money",
    "Music",
    "Mirror",
    "Mountain",
    "Nail",
    "Nest",
    "Nose",
    "Note",
    "Nurse",
    "Night",
    "Needle",
    "Number",
    "Orange",
    "Owl",
    "Ocean",
    "Oven",
    "Olive",
    "Onion",
    "Office",
    "Orbit",
    "Pen",
    "Pig",
    "Pie",
    "Pot",
    "Paper",
    "Paint",
    "Peach",
    "Pillow",
    "Queen",
    "Quilt",
    "Quiet",
    "Question",
    "Quick",
    "Quiver",
    "Quote",
    "Quiz",
    "Rabbit",
    "Rain",
    "Ring",
    "Road",
    "Robot",
    "River",
    "Rocket",
    "Rope",
    "Sun",
    "Star",
    "Shoe",
    "Ship",
    "Snow",
    "Soup",
    "Spoon",
    "Squirrel",
    "Table",
    "Tree",
    "Tiger",
    "Train",
    "Toast",
    "Thumb",
    "Turtle",
    "Tower",
    "Umbrella",
    "Unicorn",
    "Uniform",
    "Urn",
    "Uncle",
    "Unit",
    "User",
    "Utensil",
    "Vase",
    "Violin",
    "Volcano",
    "Van",
    "Vegetable",
    "Village",
    "Voice",
    "Vulture",
    "Watch",
    "Water",
    "Wheel",
    "Wind",
    "Worm",
    "Whale",
    "Window",
    "Wolf",
    "Xylophone",
    "Xerox",
    "X-ray",  # Limited X words
    "Yarn",
    "Yacht",
    "Yolk",
    "Yogurt",
    "Yellow",
    "Yard",
    "Yawn",
    "Youth",
    "Zebra",
    "Zipper",
    "Zoo",
    "Zero",
    "Zone",
    "Zigzag",  # Limited Z words
]

letter_map = {"A": 0, "B": 1, "C": 2}
num_classes = 4  # A, B, C, Other
label_map_inv = {0: "A", 1: "B", 2: "C", 3: "Other"}

# Position finder for the first character
first_char_finder = PositionFinder.from_char_position(0)

# Create ProbingExamples
start_letter_examples: List[ProbingExample] = []
for word in words:
    first_letter = word[0].upper()
    label = letter_map.get(first_letter, 3)  # Default to 'Other' (label 3)
    label_text = label_map_inv[label]

    # Find the position of the first character
    positions_dict = {}
    pos = first_char_finder(word)
    if pos:
        # PositionFinder returns a single Position object
        positions_dict["FIRST_CHAR"] = pos

    start_letter_examples.append(
        ProbingExample(
            text=word,
            label=label,  # Numeric label (0, 1, 2, 3)
            label_text=label_text,
            character_positions=(
                CharacterPositions(positions_dict) if positions_dict else None
            ),
            attributes={"start_letter": first_letter},
        )
    )

# Create the ProbingDataset
start_letter_dataset = ProbingDataset(
    examples=start_letter_examples,
    task_type="classification",
    label_mapping={
        v: k for k, v in label_map_inv.items()
    },  # Store inverse map for clarity
    dataset_attributes={
        "description": "Starting letter classification (A, B, C, Other)"
    },
)

# Display some examples
print("Starting Letter Dataset Examples:")
for i in np.random.choice(
    range(len(start_letter_dataset.examples)), size=5, replace=False
):
    ex = start_letter_dataset.examples[i]
    print(
        f"  '{ex.text}' -> Label: {ex.label} ({ex.label_text}), Pos: {ex.character_positions['FIRST_CHAR'] if ex.character_positions else 'N/A'}"
    )

print(f"\nPosition types: {start_letter_dataset.position_types}")
print(f"Label mapping: {start_letter_dataset.label_mapping}")

# %% [markdown]
# ## 2. Tokenization
# Tokenize the dataset using a pre-trained tokenizer. We pay attention to padding
# and special tokens. The `TokenizedProbingDataset` handles the conversion of
# character positions to token positions automatically.

# %%
# Set up tokenizer
# Using a small model for faster demonstration
model_name = "google/gemma-2-2b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Set pad token if not already set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Tokenize the dataset
# We add special tokens (like BOS) and pad to a fixed length.
tokenized_start_letter_dataset = TokenizedProbingDataset.from_probing_dataset(
    dataset=start_letter_dataset,
    tokenizer=tokenizer,
    padding="max_length",
    max_length=8,  # Short max length for single words
    add_special_tokens=True,  # Adds BOS/EOS if applicable
)

print(f"\nTokenized Dataset Size: {len(tokenized_start_letter_dataset.examples)}")
print(
    f"Max token length: {max(len(ex.tokens) for ex in tokenized_start_letter_dataset.examples)}"
)

# Verify token positions for a sample
sample_idx = 0
sample_tokenized_ex = tokenized_start_letter_dataset.examples[sample_idx]
print(f"\nSample Example {sample_idx}:")
print(f"  Text: '{sample_tokenized_ex.text}'")
print(f"  Tokens: {sample_tokenized_ex.tokens}")
print(
    f"  Decoded Tokens: {[tokenizer.decode([t]) for t in sample_tokenized_ex.tokens]}"
)
if sample_tokenized_ex.token_positions:
    first_char_token_pos = sample_tokenized_ex.token_positions["FIRST_CHAR"]
    print(f"  Token Position for FIRST_CHAR: {first_char_token_pos}")
    if 0 <= first_char_token_pos < len(sample_tokenized_ex.tokens):
        target_token_id = sample_tokenized_ex.tokens[first_char_token_pos]
        print(
            f"  Token at target position: {target_token_id} ('{tokenizer.decode([target_token_id])}')"
        )
    else:
        print("  Target token position is out of bounds!")
else:
    print("  Token positions not found for sample.")

# You can use verify_position_tokens for more detailed checking:
# verification_results = tokenized_start_letter_dataset.verify_position_tokens(tokenizer)
# print("\nVerification Results (first few):")
# for i, res in verification_results.items():
#     if i < 3: print(f" Example {i}: {res}")

# %% [markdown]
# ## 3. Probe Training
# Train the `MultiClassLogisticProbe` using the `SupervisedProbeTrainer`.
# We need to configure the probe with the correct input size (model's hidden dimension)
# and output size (number of classes).

# %%
# Configure model and hook point
# Using a mid-layer residual stream hook point
hook_point = "blocks.1.hook_resid_pre"
layer = 1

# Get model's hidden dimension
model = HookedTransformer.from_pretrained(model_name, device=device)
hidden_size = model.cfg.d_model
print(f"\nModel {model_name} hidden dimension: {hidden_size}")

# Configure the MultiClassLogisticProbe
probe_config = MultiClassLogisticProbeConfig(
    input_size=hidden_size,
    output_size=num_classes,  # 4 classes: A, B, C, Other
    normalize_weights=True,
    bias=True,
    model_name=model_name,
    hook_point=hook_point,
    hook_layer=layer,
    name="start_letter_multiclass_probe",
)

# Configure the Trainer
# Note: Handle class imbalance is useful here as 'Other' is more frequent
# Ensure standardize_activations is False as Probe expects raw data for inference
trainer_config = SupervisedTrainerConfig(
    batch_size=8,
    learning_rate=5e-3,  # Slightly higher LR can work for simple tasks
    num_epochs=20,  # More epochs for convergence
    weight_decay=0.01,
    train_ratio=0.7,
    handle_class_imbalance=True,  # Important for multi-class
    show_progress=True,
    device=device,
    patience=5,  # Add early stopping
    min_delta=1e-4,
    standardize_activations=False,  # Important: Keep False if probe needs raw activations
)

# Revert to using ProbePipeline
pipeline_config = ProbePipelineConfig(
    dataset=tokenized_start_letter_dataset,
    probe_cls=MultiClassLogisticProbe,
    probe_config=probe_config,
    trainer_cls=SupervisedProbeTrainer,
    trainer_config=trainer_config,
    position_key="FIRST_CHAR",
    model_name=model_name,
    hook_points=[hook_point],
    cache_dir="./cache/start_letter_probe_cache",  # Specify cache directory
    device=device,  # Specify device
)

# Train the probe using the pipeline's run method
print(f"\nTraining Multi-Class Probe for layer {layer} using Pipeline...")
pipeline: ProbePipeline = ProbePipeline(pipeline_config)

# Run the pipeline - this handles activation collection, training, etc.
start_letter_probe, history = pipeline.run(hook_point=hook_point)

print("Training complete.")

# %% [markdown]
# ## 4. Plot Training Results

# %%
# Plot training and validation loss
plt.figure(figsize=(8, 5))
plt.plot(history["train_loss"], label="Train Loss")
plt.plot(history["val_loss"], label="Validation Loss", linestyle="--")
plt.xlabel("Epoch")
plt.ylabel("Loss (Cross Entropy)")
plt.title(f"Multi-Class Probe Training History (Layer {layer})")
plt.legend()
plt.grid(True)
plt.show()

print(f"Final Validation Loss: {history['val_loss'][-1]:.4f}")

# %% [markdown]
# ## 5. Inference
# Use the trained multi-class probe with `ProbeInference` to predict the starting
# letter category for new words. We check the probability distribution output by
# the probe at the first token's position.

# %%
# Create ProbeInference instance
start_letter_inference = ProbeInference(
    model_name=model_name,
    hook_point=hook_point,
    probe=start_letter_probe,
    device=device,
)

# Test words
test_words = [
    "Antelope",
    "Bear",
    "Crocodile",
    "Zebra",
    "Banana",
    "Avocado",
    "Gorilla",
    "Camel",
]

# Get probabilities (softmax applied for MultiClassLogisticProbe)
# Shape: (batch_size, seq_len, num_classes)
probabilities = start_letter_inference.get_probabilities(test_words)

print(f"\n===== Inference Results (Layer {layer}) =====")
print(f"Probing at the first character's token position.")
print(f"Classes: {label_map_inv}")

# Ensure probe is in eval mode for inference
start_letter_probe.eval()
for i, word in enumerate(test_words):
    print(f"\nWord: '{word}'")

    # Tokenize the single word to find the token position corresponding to the first char
    # Note: Need consistent tokenization settings with training
    tokens_info = tokenizer(word, return_tensors="pt", add_special_tokens=True)
    input_ids = tokens_info["input_ids"][0]
    decoded_tokens = [tokenizer.decode([t]) for t in input_ids]

    # Find token position for the first character (index 0)
    # This uses the tokenizer directly, similar to how TokenizedProbingDataset does it.
    # We assume the first character corresponds to the first non-special token.
    first_char_token_index = -1
    for idx, token_id in enumerate(input_ids.tolist()):
        if token_id not in tokenizer.all_special_ids:
            first_char_token_index = idx
            break

    if first_char_token_index == -1 and len(input_ids) > 0:
        # Fallback if only special tokens? Use first token.
        first_char_token_index = 0

    print(f"  Tokens: {input_ids.tolist()}")
    print(f"  Decoded: {decoded_tokens}")
    print(f"  Inferred First Char Token Index: {first_char_token_index}")

    if first_char_token_index != -1 and first_char_token_index < probabilities.shape[1]:
        # Get the probability distribution at the first character's token position
        # Probabilities shape: [batch, seq_len, num_classes]
        prob_dist = probabilities[i, first_char_token_index, :]
        predicted_class_index = torch.argmax(prob_dist).item()
        predicted_class_label = label_map_inv[predicted_class_index]
        confidence = prob_dist[predicted_class_index].item()

        print(
            f"  Probabilities at token {first_char_token_index} ('{tokenizer.decode([input_ids[first_char_token_index]])}'):"
        )
        for cls_idx, prob in enumerate(prob_dist):
            print(f"    Class {cls_idx} ({label_map_inv[cls_idx]}): {prob:.4f}")
        print(
            f"  Prediction: Starts with '{predicted_class_label}' (Confidence: {confidence:.4f})"
        )
    else:
        print("  Could not determine valid token index for the first character.")

# %% [markdown]
# ## Conclusion
# This tutorial showed how to set up and train a `MultiClassLogisticProbe`.
# Key steps included:
# - Defining a multi-class dataset with integer labels.
# - Using `MultiClassLogisticProbeConfig` with `output_size` > 1.
# - Training with `SupervisedProbeTrainer`, which automatically handles `CrossEntropyLoss` and class imbalance for this probe type.
# - Performing inference using `ProbeInference`, where `get_probabilities` applies `softmax` to the multi-class probe outputs.
#
# This probe type is useful for analyzing how models represent categorical features with more than two options within their activations.
# %%


