{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probity.probing.datasets.templated import TemplatedDataset\n",
    "from probity.probing.datasets.tokenized import TokenizedProbingDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create movie sentiment dataset\n",
    "adjectives = {\n",
    "    \"positive\": [\"incredible\", \"amazing\", \"fantastic\"],\n",
    "    \"negative\": [\"terrible\", \"awful\", \"horrible\"]\n",
    "}\n",
    "verbs = {\n",
    "    \"positive\": [\"loved\", \"enjoyed\", \"adored\"],\n",
    "    \"negative\": [\"hated\", \"disliked\", \"detested\"]\n",
    "}\n",
    "\n",
    "# Create dataset using factory method\n",
    "movie_dataset = TemplatedDataset.from_movie_sentiment_template(\n",
    "    adjectives=adjectives,\n",
    "    verbs=verbs\n",
    ")\n",
    "\n",
    "# Convert to probing dataset with automatic position finding\n",
    "# and label mapping from sentiment metadata\n",
    "probing_dataset = movie_dataset.to_probing_dataset(\n",
    "    label_from_metadata=\"sentiment\",\n",
    "    label_map={\"positive\": 1, \"negative\": 0},\n",
    "    auto_add_positions=True\n",
    ")\n",
    "\n",
    "# Convert to tokenized dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probing Dataset Example:\n",
      "Text: I thought this movie was fantastic, I enjoyed it.\n",
      "Label: 1\n",
      "Label Text: positive\n",
      "Character Positions: CharacterPositions(positions={'ADJ': Position(start=25, end=34), 'VERB': Position(start=38, end=45)})\n",
      "Metadata: {'template': 'I thought this movie was {ADJ}, I {VERB} it.', 'variables': {'ADJ': {'sentiment': ['positive', 'positive', 'positive', 'negative', 'negative', 'negative']}, 'VERB': {'sentiment': ['positive', 'positive', 'positive', 'negative', 'negative', 'negative']}}, 'class': 'positive', 'task': 'sentiment_classification'}\n",
      "\n",
      "Tokenized Dataset Example:\n",
      "Text: I thought this movie was fantastic, I enjoyed it.\n",
      "Label: 1\n",
      "Label Text: positive\n",
      "Token Positions: TokenPositions(positions={'ADJ': 6, 'VERB': 9})\n",
      "Tokens: [2, 235285, 3421, 736, 7344, 729, 15814, 235269, 590, 13177, 665, 235265]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metadata: {'template': 'I thought this movie was {ADJ}, I {VERB} it.', 'variables': {'ADJ': {'sentiment': ['positive', 'positive', 'positive', 'negative', 'negative', 'negative']}, 'VERB': {'sentiment': ['positive', 'positive', 'positive', 'negative', 'negative', 'negative']}}, 'class': 'positive', 'task': 'sentiment_classification'}\n"
     ]
    }
   ],
   "source": [
    "# Get an example from the probing dataset\n",
    "probing_example = probing_dataset.examples[16]\n",
    "print(\"Probing Dataset Example:\")\n",
    "print(f\"Text: {probing_example.text}\")\n",
    "print(f\"Label: {probing_example.label}\")\n",
    "print(f\"Label Text: {probing_example.label_text}\")\n",
    "print(f\"Character Positions: {probing_example.character_positions}\")\n",
    "print(f\"Metadata: {probing_example.metadata}\\n\")\n",
    "\n",
    "# Get an example from the tokenized dataset \n",
    "tokenized_example = tokenized_dataset.examples[16]\n",
    "print(\"Tokenized Dataset Example:\")\n",
    "print(f\"Text: {tokenized_example.text}\")\n",
    "print(f\"Label: {tokenized_example.label}\")\n",
    "print(f\"Label Text: {tokenized_example.label_text}\")\n",
    "print(f\"Token Positions: {tokenized_example.token_positions}\")\n",
    "print(f\"Tokens: {tokenized_example.tokens}\")\n",
    "print(f\"Attention Mask: {tokenized_example.attention_mask}\")\n",
    "print(f\"Metadata: {tokenized_example.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full text: I thought this movie was terrible, I hated it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  terrible\n",
      "  VERB:  hated\n",
      "\n",
      "Full text: I thought this movie was terrible, I disliked it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  terrible\n",
      "  VERB:  disliked\n",
      "\n",
      "Full text: I thought this movie was terrible, I detested it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  terrible\n",
      "  VERB:  de\n",
      "\n",
      "Full text: I thought this movie was awful, I hated it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  awful\n",
      "  VERB:  hated\n",
      "\n",
      "Full text: I thought this movie was awful, I disliked it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  awful\n",
      "  VERB:  disliked\n",
      "\n",
      "Full text: I thought this movie was awful, I detested it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  awful\n",
      "  VERB:  de\n",
      "\n",
      "Full text: I thought this movie was horrible, I hated it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  horrible\n",
      "  VERB:  hated\n",
      "\n",
      "Full text: I thought this movie was horrible, I disliked it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  horrible\n",
      "  VERB:  disliked\n",
      "\n",
      "Full text: I thought this movie was horrible, I detested it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  horrible\n",
      "  VERB:  de\n",
      "\n",
      "Full text: I thought this movie was incredible, I loved it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  incredible\n",
      "  VERB:  loved\n",
      "\n",
      "Full text: I thought this movie was incredible, I enjoyed it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  incredible\n",
      "  VERB:  enjoyed\n",
      "\n",
      "Full text: I thought this movie was incredible, I adored it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  incredible\n",
      "  VERB:  adored\n",
      "\n",
      "Full text: I thought this movie was amazing, I loved it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  amazing\n",
      "  VERB:  loved\n",
      "\n",
      "Full text: I thought this movie was amazing, I enjoyed it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  amazing\n",
      "  VERB:  enjoyed\n",
      "\n",
      "Full text: I thought this movie was amazing, I adored it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  amazing\n",
      "  VERB:  adored\n",
      "\n",
      "Full text: I thought this movie was fantastic, I loved it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  fantastic\n",
      "  VERB:  loved\n",
      "\n",
      "Full text: I thought this movie was fantastic, I enjoyed it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  fantastic\n",
      "  VERB:  enjoyed\n",
      "\n",
      "Full text: I thought this movie was fantastic, I adored it.\n",
      "Decoded tokens at positions:\n",
      "  ADJ:  fantastic\n",
      "  VERB:  adored\n"
     ]
    }
   ],
   "source": [
    "# Loop through dataset and decode token positions\n",
    "for example in tokenized_dataset.examples:\n",
    "    print(f\"\\nFull text: {example.text}\")\n",
    "    print(\"Decoded tokens at positions:\")\n",
    "    for pos_key in example.token_positions.keys():\n",
    "        pos = example.token_positions[pos_key]\n",
    "        decoded = tokenizer.decode(example.tokens[pos])\n",
    "        print(f\"  {pos_key}: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horrible'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probing_example.text[probing_example.character_positions['ADJ'].start:probing_example.character_positions['ADJ'].end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or create a custom template\n",
    "from probity.probing.datasets.templated import Template, TemplateVariable\n",
    "\n",
    "# Create variables\n",
    "subject = TemplateVariable(\n",
    "    name=\"SUBJECT\",\n",
    "    values=[\"cat\", \"dog\"],\n",
    "    metadata={\"entity_type\": \"animal\"}\n",
    ")\n",
    "\n",
    "action = TemplateVariable(\n",
    "    name=\"ACTION\",\n",
    "    values=[\"jumped over\", \"ran under\"],\n",
    "    metadata={\"movement_type\": \"locomotion\"}\n",
    ")\n",
    "\n",
    "object_var = TemplateVariable(\n",
    "    name=\"OBJECT\",\n",
    "    values=[\"fence\", \"table\"],\n",
    "    metadata={\"object_type\": \"barrier\"}\n",
    ")\n",
    "\n",
    "# Create template\n",
    "template = Template(\n",
    "    template=\"The {SUBJECT} {ACTION} the {OBJECT}.\",\n",
    "    variables={\n",
    "        \"SUBJECT\": subject,\n",
    "        \"ACTION\": action,\n",
    "        \"OBJECT\": object_var\n",
    "    },\n",
    "    metadata={\"task\": \"entity_movement\"}\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "custom_dataset = TemplatedDataset(templates=[template])\n",
    "\n",
    "# Convert to probing dataset\n",
    "probing_dataset = custom_dataset.to_probing_dataset(auto_add_positions=True)\n",
    "\n",
    "# Convert to tokenized dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full text: The cat jumped over the fence.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  jumped\n",
      "  SUBJECT:  cat\n",
      "  OBJECT:  fence\n",
      "\n",
      "Full text: The cat jumped over the table.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  jumped\n",
      "  SUBJECT:  cat\n",
      "  OBJECT:  table\n",
      "\n",
      "Full text: The cat ran under the fence.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  ran\n",
      "  SUBJECT:  cat\n",
      "  OBJECT:  fence\n",
      "\n",
      "Full text: The cat ran under the table.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  ran\n",
      "  SUBJECT:  cat\n",
      "  OBJECT:  table\n",
      "\n",
      "Full text: The dog jumped over the fence.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  jumped\n",
      "  SUBJECT:  dog\n",
      "  OBJECT:  fence\n",
      "\n",
      "Full text: The dog jumped over the table.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  jumped\n",
      "  SUBJECT:  dog\n",
      "  OBJECT:  table\n",
      "\n",
      "Full text: The dog ran under the fence.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  ran\n",
      "  SUBJECT:  dog\n",
      "  OBJECT:  fence\n",
      "\n",
      "Full text: The dog ran under the table.\n",
      "Decoded tokens at positions:\n",
      "  ACTION:  ran\n",
      "  SUBJECT:  dog\n",
      "  OBJECT:  table\n"
     ]
    }
   ],
   "source": [
    "# Loop through dataset and decode token positions\n",
    "for example in tokenized_dataset.examples:\n",
    "    print(f\"\\nFull text: {example.text}\")\n",
    "    print(\"Decoded tokens at positions:\")\n",
    "    for pos_key in example.token_positions.keys():\n",
    "        pos = example.token_positions[pos_key]\n",
    "        decoded = tokenizer.decode(example.tokens[pos])\n",
    "        print(f\"  {pos_key}: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full text: When Sarah frowned deeply at the coffee shop while reading books, her friends avoided her company.\n",
      "Emotion class: negative\n",
      "Decoded tokens at positions:\n",
      "  ACTIVITY:  reading\n",
      "  REACTION:  frowned\n",
      "  LOCATION:  coffee\n",
      "  RESPONSE:  avoided\n",
      "\n",
      "Full text: When Sarah frowned deeply at the library while sharing stories, her friends avoided her company.\n",
      "Emotion class: negative\n",
      "Decoded tokens at positions:\n",
      "  ACTIVITY:  sharing\n",
      "  REACTION:  frowned\n",
      "  LOCATION:  library\n",
      "  RESPONSE:  avoided\n",
      "\n",
      "Full text: When Sarah smiled warmly at the coffee shop while playing chess, her friends embraced her company.\n",
      "Emotion class: positive\n",
      "Decoded tokens at positions:\n",
      "  ACTIVITY:  playing\n",
      "  REACTION:  smiled\n",
      "  LOCATION:  coffee\n",
      "  RESPONSE:  embraced\n"
     ]
    }
   ],
   "source": [
    "from probity.probing.datasets.templated import Template, TemplateVariable\n",
    "\n",
    "# Create emotion-based variables\n",
    "reactions = TemplateVariable(\n",
    "    name=\"REACTION\",\n",
    "    values=[\"smiled warmly\", \"laughed joyfully\", \"frowned deeply\", \"scowled angrily\"],\n",
    "    metadata={\"emotion\": [\"positive\", \"positive\", \"negative\", \"negative\"]},\n",
    "    class_bound=True,\n",
    "    class_key=\"emotion\"\n",
    ")\n",
    "\n",
    "responses = TemplateVariable(\n",
    "    name=\"RESPONSE\",\n",
    "    values=[\"embraced\", \"welcomed\", \"rejected\", \"avoided\"],\n",
    "    metadata={\"emotion\": [\"positive\", \"positive\", \"negative\", \"negative\"]},\n",
    "    class_bound=True,\n",
    "    class_key=\"emotion\"\n",
    ")\n",
    "\n",
    "# Create neutral variables for context\n",
    "locations = TemplateVariable(\n",
    "    name=\"LOCATION\",\n",
    "    values=[\"coffee shop\", \"library\", \"park\", \"restaurant\"],\n",
    "    metadata={\"setting_type\": \"public_space\"}\n",
    ")\n",
    "\n",
    "activities = TemplateVariable(\n",
    "    name=\"ACTIVITY\",\n",
    "    values=[\"reading books\", \"playing chess\", \"sharing stories\", \"having lunch\"],\n",
    "    metadata={\"activity_type\": \"social\"}\n",
    ")\n",
    "\n",
    "# Create template with more complex structure\n",
    "template = Template(\n",
    "    template=\"When Sarah {REACTION} at the {LOCATION} while {ACTIVITY}, her friends {RESPONSE} her company.\",\n",
    "    variables={\n",
    "        \"REACTION\": reactions,\n",
    "        \"RESPONSE\": responses,\n",
    "        \"LOCATION\": locations,\n",
    "        \"ACTIVITY\": activities\n",
    "    },\n",
    "    metadata={\"task\": \"emotion_analysis\"}\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "social_dataset = TemplatedDataset(templates=[template])\n",
    "\n",
    "# Convert to probing dataset with emotion labels\n",
    "probing_dataset = social_dataset.to_probing_dataset(\n",
    "    label_from_metadata=\"emotion\",\n",
    "    label_map={\"positive\": 1, \"negative\": 0},\n",
    "    auto_add_positions=True\n",
    ")\n",
    "\n",
    "# Convert to tokenized dataset\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Print some random examples\n",
    "import random\n",
    "for example in random.sample(list(tokenized_dataset.examples), 3):\n",
    "    print(f\"\\nFull text: {example.text}\")\n",
    "    print(f\"Emotion class: {example.label_text}\")\n",
    "    print(\"Decoded tokens at positions:\")\n",
    "    for pos_key in example.token_positions.keys():\n",
    "        pos = example.token_positions[pos_key]\n",
    "        decoded = tokenizer.decode(example.tokens[pos])\n",
    "        print(f\"  {pos_key}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_with_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from probity.datasets.templated import TemplatedDataset\n",
    "from probity.datasets.tokenized import TokenizedProbingDataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch device to mps\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example tokens: [40, 1807, 428, 3807, 373, 8082, 11, 314, 6151, 340, 13]\n",
      "First example text: I thought this movie was incredible, I loved it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2852: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create movie sentiment dataset\n",
    "adjectives = {\n",
    "    \"positive\": [\"incredible\", \"amazing\", \"fantastic\", \"awesome\", \"beautiful\", \"brilliant\", \"exceptional\", \"extraordinary\", \"fabulous\", \"great\", \"lovely\", \"outstanding\", \"remarkable\", \"wonderful\"],\n",
    "    \"negative\": [\"terrible\", \"awful\", \"horrible\", \"bad\", \"disappointing\", \"disgusting\", \"dreadful\", \"horrendous\", \"mediocre\", \"miserable\", \"offensive\", \"terrible\", \"unpleasant\", \"wretched\"]\n",
    "}\n",
    "verbs = {\n",
    "    \"positive\": [\"loved\", \"enjoyed\", \"adored\"],\n",
    "    \"negative\": [\"hated\", \"disliked\", \"detested\"]\n",
    "}\n",
    "\n",
    "# Create dataset using factory method\n",
    "movie_dataset = TemplatedDataset.from_movie_sentiment_template(\n",
    "    adjectives=adjectives,\n",
    "    verbs=verbs\n",
    ")\n",
    "\n",
    "# Convert to probing dataset with automatic position finding\n",
    "# and label mapping from sentiment metadata\n",
    "probing_dataset = movie_dataset.to_probing_dataset(\n",
    "    label_from_metadata=\"sentiment\",\n",
    "    label_map={\"positive\": 1, \"negative\": 0},\n",
    "    auto_add_positions=True\n",
    ")\n",
    "\n",
    "# Convert to tokenized dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,  # Add padding\n",
    "    max_length=128  # Specify max length\n",
    ")\n",
    "\n",
    "# Verify the tokenization worked\n",
    "example = tokenized_dataset.examples[0]\n",
    "print(\"First example tokens:\", example.tokens)\n",
    "print(\"First example text:\", example.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/curttigges/miniconda3/envs/sae-l/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from probity.probes.linear_probe import LinearProbe, LinearProbeConfig\n",
    "from probity.training.trainer import SupervisedProbeTrainer, SupervisedTrainerConfig\n",
    "from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig\n",
    "\n",
    "# First, configure the probe\n",
    "# GPT2-small has hidden size 768\n",
    "probe_config = LinearProbeConfig(\n",
    "    input_size=768,\n",
    "    normalize_weights=True,  # Normalize the learned direction\n",
    "    bias=False  # No bias term needed for direction finding\n",
    ")\n",
    "\n",
    "# Configure the trainer\n",
    "trainer_config = SupervisedTrainerConfig(\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    "    num_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    train_ratio=0.8,  # 80-20 train-val split\n",
    "    handle_class_imbalance=True,  # Important since our classes are balanced\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Create pipeline configuration\n",
    "pipeline_config = ProbePipelineConfig(\n",
    "    dataset=tokenized_dataset,\n",
    "    probe_cls=LinearProbe,\n",
    "    probe_config=probe_config,\n",
    "    trainer_cls=SupervisedProbeTrainer,\n",
    "    trainer_config=trainer_config,\n",
    "    position_key=\"ADJ\",  # We want to probe at the adjective position\n",
    "    model_name=\"gpt2\",\n",
    "    hook_points=[\"blocks.6.hook_resid_post\"],  # Layer 6\n",
    "    cache_dir=\"./sentiment_probe_cache\"  # Cache activations for reuse\n",
    ")\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = ProbePipeline(pipeline_config)\n",
    "probe, training_history = pipeline.run()\n",
    "\n",
    "# The probe now contains our learned sentiment direction\n",
    "sentiment_direction = probe.get_direction()\n",
    "\n",
    "# We can analyze training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_history['train_loss'], label='Train Loss')\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Probe Training History')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the pipeline for later use\n",
    "pipeline.save(\"./sentiment_probe\")\n",
    "\n",
    "# To test the probe, we can get predictions for new examples\n",
    "def analyze_sentiment(text: str, pipeline: ProbePipeline):\n",
    "    # Tokenize new text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    \n",
    "    # Get activations for the new text\n",
    "    with torch.no_grad():\n",
    "        _, cache = pipeline.collector.model.run_with_cache(\n",
    "            tokens,\n",
    "            names_filter=[\"blocks.6.hook_resid_post\"]\n",
    "        )\n",
    "    \n",
    "    # Get the activations at layer 6\n",
    "    activations = cache[\"blocks.6.hook_resid_post\"]\n",
    "    \n",
    "    # Apply the probe\n",
    "    logits = pipeline.probe(activations)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    \n",
    "    return probs.item()\n",
    "\n",
    "# Test the probe\n",
    "test_text = \"I thought this movie was fantastic, I loved it.\"\n",
    "sentiment_score = analyze_sentiment(test_text, pipeline)\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {sentiment_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history: {'train_loss': [0.8292038440704346, 0.7475414474805196, 0.7830476959546407, 0.6590535243352255, 0.6349036892255148, 0.603614350159963, 0.5691607991854349, 0.5244813859462738, 0.6492942372957865, 0.4872782329718272, 0.5733036895593008, 0.5423712829748789, 0.441368967294693, 0.39737847447395325, 0.46008838216463727, 0.37618409593900043, 0.3719136615594228, 0.4497372309366862, 0.3339608907699585, 0.34183535973231], 'train_accuracy': [0.4305555621782939, 0.4826388955116272, 0.3854166666666667, 0.71875, 0.6180555621782938, 0.7604166666666666, 0.6805555621782938, 0.8125, 0.621527781089147, 0.875, 0.7743055621782938, 0.8055555621782938, 0.8055555621782938, 0.9375, 0.8368055621782938, 0.9375, 0.9375, 0.8368055621782938, 0.9583333333333334, 0.96875], 'train_precision': [np.float64(0.6062271062271062), np.float64(0.6749999999999999), np.float64(0.45555555555555555), np.float64(0.8083333333333332), np.float64(0.49673202614379086), np.float64(0.882051282051282), np.float64(0.9222222222222222), np.float64(0.9305555555555555), np.float64(0.9333333333333332), np.float64(0.9440789473684211), np.float64(0.8555555555555555), np.float64(0.9441176470588234), np.float64(0.9444444444444445), np.float64(0.9565217391304347), np.float64(0.9515873015873016), np.float64(0.9500000000000001), np.float64(0.9500000000000001), np.float64(0.9500000000000001), np.float64(0.9814814814814815), np.float64(1.0)], 'train_recall': [np.float64(0.41081871345029236), np.float64(0.43065998329156224), np.float64(0.39897698209718674), np.float64(0.6992481203007519), np.float64(0.3783371472158657), np.float64(0.7105263157894738), np.float64(0.6002923976608187), np.float64(0.7485380116959064), np.float64(0.5740740740740741), np.float64(0.8531746031746031), np.float64(0.8468013468013469), np.float64(0.8042186571598336), np.float64(0.7474747474747474), np.float64(0.9507936507936509), np.float64(0.7999999999999999), np.float64(0.9523809523809524), np.float64(0.9507101086048454), np.float64(0.8546783625730994), np.float64(0.9500000000000001), np.float64(0.9482456140350877)], 'train_f1': [np.float64(0.4897058823529412), np.float64(0.5200494612259319), np.float64(0.42538759689922484), np.float64(0.746031746031746), np.float64(0.4294117647058823), np.float64(0.7781512605042017), np.float64(0.7215686274509805), np.float64(0.8221447253705318), np.float64(0.6885521885521886), np.float64(0.8944444444444445), np.float64(0.8319508448540707), np.float64(0.8623716153127917), np.float64(0.8174603174603173), np.float64(0.9521531100478469), np.float64(0.8478319783197832), np.float64(0.9508025849489264), np.float64(0.9499687304565354), np.float64(0.8910256410256411), np.float64(0.9649122807017544), np.float64(0.9729344729344729)], 'val_loss': [1.2247745990753174, 1.173364281654358, 1.119306206703186, 1.066349983215332, 1.015355110168457, 0.965655505657196, 0.9177014231681824, 0.872359573841095, 0.8290316462516785, 0.7884606719017029, 0.7495688199996948, 0.7129863500595093, 0.6799600720405579, 0.64825040102005, 0.6180184483528137, 0.5907432436943054, 0.5648606419563293, 0.5401384830474854, 0.5166792273521423, 0.49310386180877686], 'val_accuracy': [0.29411765933036804, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.47058823704719543, 0.6470588445663452, 0.8235294222831726, 0.8235294222831726, 0.8235294222831726, 0.8235294222831726, 0.8235294222831726, 0.8235294222831726], 'val_precision': [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)], 'val_recall': [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)], 'val_f1': [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training history: {history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8914],\n",
       "         [ 1.1352],\n",
       "         [-0.6983],\n",
       "         [ 1.2239],\n",
       "         [ 1.4124],\n",
       "         [ 3.0506]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.get_direction_activations(\"I thought this movie was superb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8914],\n",
       "         [ 1.1352],\n",
       "         [-0.6983],\n",
       "         [ 1.2239],\n",
       "         [ 1.4124],\n",
       "         [ 1.5386]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.get_direction_activations(\"I thought this movie was lousy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[119.5551],\n",
      "         [ -1.9024],\n",
      "         [ -2.5790],\n",
      "         [ -1.0393],\n",
      "         [ -0.1709],\n",
      "         [ -0.9175],\n",
      "         [ -0.9112],\n",
      "         [ -4.0115],\n",
      "         [ -1.5986]]])\n",
      "tensor([[[119.5551],\n",
      "         [ -1.9024],\n",
      "         [ -2.5790],\n",
      "         [ -1.0393],\n",
      "         [ -1.5023],\n",
      "         [ -2.1044],\n",
      "         [ -3.8507],\n",
      "         [ -3.3782],\n",
      "         [ -2.1310]]])\n"
     ]
    }
   ],
   "source": [
    "# Test with very clear positive/negative examples\n",
    "print(inference.get_direction_activations(\"This was the most amazing wonderful perfect movie ever\"))\n",
    "print(inference.get_direction_activations(\"This was the most horrible terrible awful movie ever\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/vlzhnvkj6s51xk5xshx4ldqc0000gn/T/ipykernel_98269/1342688068.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  probe_weights = torch.load(\"probes/probe.pt\")['state_dict']['linear.weight'].squeeze(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "# Load the probe weights\n",
    "probe_weights = torch.load(\"probes/probe.pt\")['state_dict']['linear.weight'].squeeze(0)\n",
    "\n",
    "# Convert to numpy array and then to list for JSON serialization\n",
    "weights_list = probe_weights.numpy().tolist()\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"probes/probe.json\", \"w\") as f:\n",
    "    json.dump(weights_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
