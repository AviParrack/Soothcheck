{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Built-in Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from probity.datasets.templated import TemplatedDataset\n",
    "from probity.datasets.tokenized import TokenizedProbingDataset\n",
    "\n",
    "# Define sentiment vocabularies\n",
    "adjectives = {\n",
    "    \"positive\": [\"excellent\", \"amazing\", \"fantastic\", \"wonderful\"],\n",
    "    \"negative\": [\"terrible\", \"awful\", \"horrible\", \"dreadful\"]\n",
    "}\n",
    "\n",
    "verbs = {\n",
    "    \"positive\": [\"loved\", \"enjoyed\", \"adored\", \"cherished\"],\n",
    "    \"negative\": [\"hated\", \"disliked\", \"despised\", \"loathed\"]\n",
    "}\n",
    "\n",
    "# Create templated dataset\n",
    "dataset = TemplatedDataset.from_movie_sentiment_template(\n",
    "    adjectives=adjectives,\n",
    "    verbs=verbs\n",
    ")\n",
    "\n",
    "# Convert to probing dataset with automatic position tracking\n",
    "probing_dataset = dataset.to_probing_dataset(\n",
    "    label_from_metadata=\"sentiment\",\n",
    "    label_map={\"positive\": 1, \"negative\": 0},\n",
    "    auto_add_positions=True\n",
    ")\n",
    "\n",
    "# Tokenize for your target model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first example\n",
    "print(tokenized_dataset.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a New Templated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from probity.datasets.templated import TemplatedDataset, Template, TemplateVariable\n",
    "from probity.datasets.tokenized import TokenizedProbingDataset\n",
    "from probity.probes.linear_probe import LinearProbeConfig, LinearProbe\n",
    "\n",
    "# Define vocabularies for subject-verb agreement template\n",
    "singular_subjects = [\"The cat\", \"The dog\", \"A student\", \"The teacher\"]\n",
    "plural_subjects = [\"The cats\", \"The dogs\", \"The students\", \"The teachers\"]\n",
    "\n",
    "singular_verbs = [\"walks\", \"runs\", \"sleeps\", \"jumps\"]\n",
    "plural_verbs = [\"walk\", \"run\", \"sleep\", \"jump\"]\n",
    "\n",
    "locations = [\"in the park\", \"at home\", \"near the school\", \"by the river\"]\n",
    "\n",
    "# Create template variables\n",
    "subject_var = TemplateVariable(\n",
    "    name=\"SUBJECT\",\n",
    "    values=singular_subjects + plural_subjects,\n",
    "    metadata={\n",
    "        \"number\": [\"singular\"] * len(singular_subjects) + [\"plural\"] * len(plural_subjects)\n",
    "    },\n",
    "    class_bound=True,\n",
    "    class_key=\"number\"\n",
    ")\n",
    "\n",
    "verb_var = TemplateVariable(\n",
    "    name=\"VERB\",\n",
    "    values=singular_verbs + plural_verbs,\n",
    "    metadata={\n",
    "        \"number\": [\"singular\"] * len(singular_verbs) + [\"plural\"] * len(plural_verbs)\n",
    "    },\n",
    "    class_bound=True,\n",
    "    class_key=\"number\"\n",
    ")\n",
    "\n",
    "location_var = TemplateVariable(\n",
    "    name=\"LOCATION\",\n",
    "    values=locations\n",
    ")\n",
    "\n",
    "# Create template\n",
    "template = Template(\n",
    "    template=\"{SUBJECT} {VERB} {LOCATION}.\",\n",
    "    variables={\n",
    "        \"SUBJECT\": subject_var,\n",
    "        \"VERB\": verb_var,\n",
    "        \"LOCATION\": location_var\n",
    "    },\n",
    "    metadata={\"task\": \"subject_verb_agreement\"}\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "dataset = TemplatedDataset(templates=[template])\n",
    "\n",
    "# Convert to probing dataset\n",
    "probing_dataset = dataset.to_probing_dataset(\n",
    "    label_from_metadata=\"number\",\n",
    "    label_map={\"singular\": 0, \"plural\": 1},\n",
    "    auto_add_positions=True\n",
    ")\n",
    "\n",
    "# Tokenize for GPT2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first example\n",
    "print(tokenized_dataset.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Probing Dataset Without Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probity.datasets.base import ProbingDataset, ProbingExample, Position, CharacterPositions\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create examples manually\n",
    "examples = [\n",
    "    ProbingExample(\n",
    "        text=\"The cat sat on the mat.\",\n",
    "        label=0,\n",
    "        label_text=\"animal\",\n",
    "        character_positions=CharacterPositions({\n",
    "            \"subject\": Position(start=4, end=7)  # \"cat\"\n",
    "        })\n",
    "    ),\n",
    "    ProbingExample(\n",
    "        text=\"The dog ran in the park.\",\n",
    "        label=0,\n",
    "        label_text=\"animal\",\n",
    "        character_positions=CharacterPositions({\n",
    "            \"subject\": Position(start=4, end=7)  # \"dog\"\n",
    "        })\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create probing dataset\n",
    "probing_dataset = ProbingDataset(\n",
    "    examples=examples,\n",
    "    task_type=\"classification\",\n",
    "    metadata={\"task\": \"subject_identification\"}\n",
    ")\n",
    "\n",
    "# Tokenize for your target model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first example\n",
    "print(tokenized_dataset.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from probity.datasets.templated import TemplatedDataset\n",
    "from probity.datasets.tokenized import TokenizedProbingDataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch device to mps\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create movie sentiment dataset\n",
    "adjectives = {\n",
    "    \"positive\": [\"incredible\", \"amazing\", \"fantastic\", \"awesome\", \"beautiful\", \"brilliant\", \"exceptional\", \"extraordinary\", \"fabulous\", \"great\", \"lovely\", \"outstanding\", \"remarkable\", \"wonderful\"],\n",
    "    \"negative\": [\"terrible\", \"awful\", \"horrible\", \"bad\", \"disappointing\", \"disgusting\", \"dreadful\", \"horrendous\", \"mediocre\", \"miserable\", \"offensive\", \"terrible\", \"unpleasant\", \"wretched\"]\n",
    "}\n",
    "verbs = {\n",
    "    \"positive\": [\"loved\", \"enjoyed\", \"adored\"],\n",
    "    \"negative\": [\"hated\", \"disliked\", \"detested\"]\n",
    "}\n",
    "\n",
    "# Create dataset using factory method\n",
    "movie_dataset = TemplatedDataset.from_movie_sentiment_template(\n",
    "    adjectives=adjectives,\n",
    "    verbs=verbs\n",
    ")\n",
    "\n",
    "# Convert to probing dataset with automatic position finding\n",
    "# and label mapping from sentiment metadata\n",
    "probing_dataset = movie_dataset.to_probing_dataset(\n",
    "    label_from_metadata=\"sentiment\",\n",
    "    label_map={\"positive\": 1, \"negative\": 0},\n",
    "    auto_add_positions=True\n",
    ")\n",
    "\n",
    "# Convert to tokenized dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = TokenizedProbingDataset.from_probing_dataset(\n",
    "    dataset=probing_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,  # Add padding\n",
    "    max_length=128  # Specify max length\n",
    ")\n",
    "\n",
    "# Verify the tokenization worked\n",
    "example = tokenized_dataset.examples[0]\n",
    "print(\"First example tokens:\", example.tokens)\n",
    "print(\"First example text:\", example.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Probe Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probity.probes.linear_probe import LogisticProbe, LogisticProbeConfig\n",
    "from probity.training.trainer import SupervisedProbeTrainer, SupervisedTrainerConfig\n",
    "from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig\n",
    "\n",
    "# First, configure the probe\n",
    "# GPT2-small has hidden size 768\n",
    "probe_config = LogisticProbeConfig(\n",
    "    input_size=768,\n",
    "    normalize_weights=True,  # Normalize the learned direction\n",
    "    bias=False  # No bias term needed for direction finding\n",
    ")\n",
    "\n",
    "# Configure the trainer\n",
    "trainer_config = SupervisedTrainerConfig(\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    "    num_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    train_ratio=0.8,  # 80-20 train-val split\n",
    "    handle_class_imbalance=True,  # Important since our classes are balanced\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(tokenized_dataset.examples)}\")\n",
    "\n",
    "# Create pipeline configuration\n",
    "pipeline_config = ProbePipelineConfig(\n",
    "    dataset=tokenized_dataset,\n",
    "    probe_cls=LogisticProbe,\n",
    "    probe_config=probe_config,\n",
    "    trainer_cls=SupervisedProbeTrainer,\n",
    "    trainer_config=trainer_config,\n",
    "    position_key=\"ADJ\",  # We want to probe at the adjective position\n",
    "    model_name=\"gpt2\",\n",
    "    hook_points=[\"blocks.7.hook_resid_pre\"],  # Layer 6\n",
    "    cache_dir=\"./cache/sentiment_probe_cache\"  # Cache activations for reuse\n",
    ")\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = ProbePipeline(pipeline_config)\n",
    "\n",
    "# Add this debugging code before running the pipeline\n",
    "example = tokenized_dataset.examples[0]\n",
    "print(f\"Example text: {example.text}\")\n",
    "print(f\"Token positions: {example.token_positions}\")\n",
    "print(f\"Available position keys: {list(example.token_positions.keys())}\")\n",
    "\n",
    "# Verify the position key matches what's in the dataset\n",
    "print(f\"\\nPipeline position key: {pipeline_config.position_key}\")\n",
    "\n",
    "probe, training_history = pipeline.run()\n",
    "\n",
    "# The probe now contains our learned sentiment direction\n",
    "sentiment_direction = probe.get_direction()\n",
    "\n",
    "# We can analyze training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_history['train_loss'], label='Train Loss')\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Probe Training History')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the pipeline for later use\n",
    "pipeline.save(\"./probes/sentiment_probe\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the probe, we can get predictions for new examples\n",
    "def analyze_sentiment(text: str, pipeline: ProbePipeline):\n",
    "    # Tokenize new text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    \n",
    "    # Get activations for the new text\n",
    "    with torch.no_grad():\n",
    "        _, cache = pipeline.collector.model.run_with_cache(\n",
    "            tokens,\n",
    "            names_filter=[\"blocks.7.hook_resid_pre\"]\n",
    "        )\n",
    "    \n",
    "    # Get the activations at target layer\n",
    "    activations = cache[\"blocks.7.hook_resid_pre\"]\n",
    "    \n",
    "    # Apply the probe\n",
    "    logits = pipeline.probe(activations)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    # Take the mean probability across all tokens\n",
    "    return probs[0, -2].item()  # Changed this line\n",
    "\n",
    "# Test the probe\n",
    "test_text = \"I thought this movie was amazing, I loved it.\"\n",
    "sentiment_score = analyze_sentiment(test_text, pipeline)\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {sentiment_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the probe\n",
    "test_text = \"I thought this movie was detestable, I hated it.\"\n",
    "sentiment_score = analyze_sentiment(test_text, pipeline)\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {sentiment_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-probe Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Type\n",
    "import torch\n",
    "from probity.collection.collectors import TransformerLensCollector, TransformerLensConfig\n",
    "from probity.probes.linear_probe import (\n",
    "    LinearProbe, LogisticProbe, KMeansProbe, PCAProbe, MeanDifferenceProbe,\n",
    "    LinearProbeConfig, LogisticProbeConfig, KMeansProbeConfig, PCAProbeConfig, MeanDiffProbeConfig,\n",
    "    SklearnLogisticProbe, SklearnLogisticProbeConfig\n",
    ")\n",
    "from probity.training.trainer import (\n",
    "    SupervisedProbeTrainer, \n",
    "    SupervisedTrainerConfig, \n",
    "    DirectionalProbeTrainer,\n",
    "    DirectionalTrainerConfig\n",
    ")\n",
    "from probity.pipeline.pipeline import ProbePipeline, ProbePipelineConfig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset setup (assuming you have this)\n",
    "dataset = tokenized_dataset\n",
    "\n",
    "# Model and hook point configuration\n",
    "model_name = \"gpt2-small\"\n",
    "hook_points = [\"blocks.7.hook_resid_pre\"]  # Examine nth layer residual stream\n",
    "\n",
    "# Function to run different probe types and compare results\n",
    "def compare_probes(dataset, hidden_size: int = 768) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Run all probe types and return their directions for comparison.\"\"\"\n",
    "    \n",
    "    # Common pipeline settings\n",
    "    base_config = {\n",
    "        \"dataset\": dataset,\n",
    "        \"position_key\": \"ADJ\",\n",
    "        \"model_name\": model_name,\n",
    "        \"hook_points\": [\"blocks.7.hook_resid_pre\"],\n",
    "        \"cache_dir\": \"cache\"\n",
    "    }\n",
    "    \n",
    "    # Set up configurations for each probe type\n",
    "    probe_configs = {\n",
    "        \"linear\": (\n",
    "            LinearProbe,\n",
    "            LinearProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                loss_type=\"hinge\",\n",
    "                normalize_weights=True,\n",
    "                bias=True  # Enable bias for better comparison with sklearn\n",
    "            ),\n",
    "            SupervisedProbeTrainer\n",
    "        ),\n",
    "        \"logistic\": (\n",
    "            LogisticProbe,\n",
    "            LogisticProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True,\n",
    "                bias=True\n",
    "            ),\n",
    "            SupervisedProbeTrainer\n",
    "        ),\n",
    "        \"logistic_skl\": (\n",
    "            SklearnLogisticProbe,\n",
    "            SklearnLogisticProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                standardize=True,\n",
    "                normalize_weights=True,\n",
    "                max_iter=100\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        ),\n",
    "        \"kmeans\": (\n",
    "            KMeansProbe,\n",
    "            KMeansProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        ),\n",
    "        \"pca\": (\n",
    "            PCAProbe,\n",
    "            PCAProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        ),\n",
    "        \"mean_diff\": (\n",
    "            MeanDifferenceProbe,\n",
    "            MeanDiffProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    directions = {}\n",
    "    \n",
    "    # Run each probe type\n",
    "    for name, (probe_cls, probe_config, trainer_cls) in probe_configs.items():\n",
    "        print(f\"\\nRunning {name} probe...\")\n",
    "        \n",
    "        # Configure trainer based on probe type\n",
    "        if trainer_cls == SupervisedProbeTrainer:\n",
    "            trainer_config = SupervisedTrainerConfig(\n",
    "                batch_size=32,\n",
    "                learning_rate=1e-3,\n",
    "                num_epochs=20,\n",
    "                weight_decay=0.01,  # Matches sklearn's default regularization\n",
    "                handle_class_imbalance=True,\n",
    "                patience=5,  # Early stopping patience\n",
    "                min_delta=1e-4  # Minimum improvement for early stopping\n",
    "            )\n",
    "        else:\n",
    "            trainer_config = DirectionalTrainerConfig(\n",
    "                batch_size=32,\n",
    "                num_epochs=1  # Direct computation methods only need one epoch\n",
    "            )\n",
    "        \n",
    "        # Create pipeline configuration\n",
    "        pipeline_config = ProbePipelineConfig(\n",
    "            **base_config,\n",
    "            probe_cls=probe_cls,\n",
    "            probe_config=probe_config,\n",
    "            trainer_cls=trainer_cls,\n",
    "            trainer_config=trainer_config\n",
    "        )\n",
    "        \n",
    "        # Run pipeline\n",
    "        pipeline = ProbePipeline(pipeline_config)\n",
    "        probe, history = pipeline.run()\n",
    "        \n",
    "        # Save direction\n",
    "        probe.save_json(f\"probes/{name}_direction.json\")\n",
    "\n",
    "        # Plot training history for gradient-based probes\n",
    "        if isinstance(probe, (LogisticProbe, LinearProbe)):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(history[\"train_loss\"], label='Train Loss')\n",
    "            plt.plot(history[\"val_loss\"], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'{name} Probe Training History')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        # Store direction\n",
    "        directions[name] = probe.get_direction()\n",
    "        \n",
    "        # Print final training loss if available\n",
    "        if \"train_loss\" in history:\n",
    "            final_train_loss = history[\"train_loss\"][-1]\n",
    "            print(f\"{name} final training loss: {final_train_loss:.4f}\")\n",
    "    \n",
    "    return directions\n",
    "\n",
    "def compare_directions(directions: Dict[str, torch.Tensor]) -> None:\n",
    "    \"\"\"Compare directions using cosine similarity.\"\"\"\n",
    "    # Calculate cosine similarities between all pairs\n",
    "    n_probes = len(directions)\n",
    "    names = list(directions.keys())\n",
    "    similarities = torch.zeros((n_probes, n_probes))\n",
    "    \n",
    "    for i, name1 in enumerate(names):\n",
    "        for j, name2 in enumerate(names):\n",
    "            dir1 = directions[name1]\n",
    "            dir2 = directions[name2]\n",
    "\n",
    "            # Ensure both directions are float32\n",
    "            dir1 = dir1.to(dtype=torch.float32)\n",
    "            dir2 = dir2.to(dtype=torch.float32)\n",
    "            \n",
    "            # Ensure directions are 1D and normalized\n",
    "            dir1 = dir1.flatten()\n",
    "            dir2 = dir2.flatten()\n",
    "            dir1 = dir1 / dir1.norm()\n",
    "            dir2 = dir2 / dir2.norm()\n",
    "            # Take absolute value of cosine similarity since direction sign is arbitrary\n",
    "            similarity = abs(torch.dot(dir1, dir2))\n",
    "            similarities[i, j] = similarity\n",
    "    \n",
    "    # Create a prettier visualization using pandas\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        similarities.numpy(),\n",
    "        index=names,\n",
    "        columns=names\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.heatmap(\n",
    "        df,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='viridis',\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    plt.title('Cosine Similarities Between Probe Directions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comparison\n",
    "directions = compare_probes(dataset)\n",
    "compare_directions(directions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_probes(dataset, hidden_size: int = 768) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Run all probe types and return their directions for comparison.\"\"\"\n",
    "    \n",
    "    # Common pipeline settings\n",
    "    base_config = {\n",
    "        \"dataset\": dataset,\n",
    "        \"position_key\": \"ADJ\",\n",
    "        \"model_name\": model_name,\n",
    "        \"hook_points\": [\"blocks.7.hook_resid_pre\"],\n",
    "        \"cache_dir\": \"cache\"\n",
    "    }\n",
    "    \n",
    "    # Set up configurations for each probe type\n",
    "    probe_configs = {\n",
    "        \"linear\": (\n",
    "            LinearProbe,\n",
    "            LinearProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                loss_type=\"hinge\",\n",
    "                normalize_weights=True,\n",
    "                bias=True  # Enable bias for better comparison with sklearn\n",
    "            ),\n",
    "            SupervisedProbeTrainer\n",
    "        ),\n",
    "        \"logistic\": (\n",
    "            LogisticProbe,\n",
    "            LogisticProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True,\n",
    "                bias=True\n",
    "            ),\n",
    "            SupervisedProbeTrainer\n",
    "        ),\n",
    "        \"logistic_skl\": (\n",
    "            SklearnLogisticProbe,\n",
    "            SklearnLogisticProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                standardize=True,\n",
    "                normalize_weights=True,\n",
    "                max_iter=100\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        ),\n",
    "        \"kmeans\": (\n",
    "            KMeansProbe,\n",
    "            KMeansProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        ),\n",
    "        \"pca\": (\n",
    "            PCAProbe,\n",
    "            PCAProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        ),\n",
    "        \"mean_diff\": (\n",
    "            MeanDifferenceProbe,\n",
    "            MeanDiffProbeConfig(\n",
    "                input_size=hidden_size,\n",
    "                normalize_weights=True\n",
    "            ),\n",
    "            DirectionalProbeTrainer\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    directions = {}\n",
    "    \n",
    "    # Run each probe type\n",
    "    for name, (probe_cls, probe_config, trainer_cls) in probe_configs.items():\n",
    "        print(f\"\\nRunning {name} probe...\")\n",
    "        \n",
    "        # Configure trainer based on probe type\n",
    "        if trainer_cls == SupervisedProbeTrainer:\n",
    "            trainer_config = SupervisedTrainerConfig(\n",
    "                batch_size=32,\n",
    "                learning_rate=1e-3,\n",
    "                num_epochs=20,\n",
    "                weight_decay=0.01,  # Matches sklearn's default regularization\n",
    "                handle_class_imbalance=True,\n",
    "                patience=5,  # Early stopping patience\n",
    "                min_delta=1e-4  # Minimum improvement for early stopping\n",
    "            )\n",
    "        else:\n",
    "            trainer_config = DirectionalTrainerConfig(\n",
    "                batch_size=32,\n",
    "                num_epochs=1  # Direct computation methods only need one epoch\n",
    "            )\n",
    "        \n",
    "        # Create pipeline configuration\n",
    "        pipeline_config = ProbePipelineConfig(\n",
    "            **base_config,\n",
    "            probe_cls=probe_cls,\n",
    "            probe_config=probe_config,\n",
    "            trainer_cls=trainer_cls,\n",
    "            trainer_config=trainer_config\n",
    "        )\n",
    "        \n",
    "        # Run pipeline\n",
    "        pipeline = ProbePipeline(pipeline_config)\n",
    "        probe, history = pipeline.run()\n",
    "        \n",
    "        # Save direction\n",
    "        probe.save_json(f\"probes/{name}_direction.json\")\n",
    "\n",
    "        # Plot training history for gradient-based probes\n",
    "        if isinstance(probe, (LogisticProbe, LinearProbe)):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(history[\"train_loss\"], label='Train Loss')\n",
    "            plt.plot(history[\"val_loss\"], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'{name} Probe Training History')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        # Store direction\n",
    "        directions[name] = probe.get_direction()\n",
    "        \n",
    "        # Print final training loss if available\n",
    "        if \"train_loss\" in history:\n",
    "            final_train_loss = history[\"train_loss\"][-1]\n",
    "            print(f\"{name} final training loss: {final_train_loss:.4f}\")\n",
    "    \n",
    "    return directions\n",
    "\n",
    "def compare_directions(directions: Dict[str, torch.Tensor]) -> None:\n",
    "    \"\"\"Compare directions using cosine similarity.\"\"\"\n",
    "    # Calculate cosine similarities between all pairs\n",
    "    n_probes = len(directions)\n",
    "    names = list(directions.keys())\n",
    "    similarities = torch.zeros((n_probes, n_probes))\n",
    "    \n",
    "    for i, name1 in enumerate(names):\n",
    "        for j, name2 in enumerate(names):\n",
    "            dir1 = directions[name1]\n",
    "            dir2 = directions[name2]\n",
    "\n",
    "            # Ensure both directions are float32\n",
    "            dir1 = dir1.to(dtype=torch.float32)\n",
    "            dir2 = dir2.to(dtype=torch.float32)\n",
    "            \n",
    "            # Ensure directions are 1D and normalized\n",
    "            dir1 = dir1.flatten()\n",
    "            dir2 = dir2.flatten()\n",
    "            dir1 = dir1 / dir1.norm()\n",
    "            dir2 = dir2 / dir2.norm()\n",
    "            # Take absolute value of cosine similarity since direction sign is arbitrary\n",
    "            similarity = abs(torch.dot(dir1, dir2))\n",
    "            similarities[i, j] = similarity\n",
    "    \n",
    "    # Create a prettier visualization using pandas\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        similarities.numpy(),\n",
    "        index=names,\n",
    "        columns=names\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.heatmap(\n",
    "        df,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='viridis',\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    plt.title('Cosine Similarities Between Probe Directions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comparison\n",
    "directions = compare_probes(dataset)\n",
    "compare_directions(directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using a single probe type\n",
    "def run_single_probe(\n",
    "    dataset,\n",
    "    probe_type: str = \"logistic\",\n",
    "    hidden_size: int = 768\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Run a single probe type on the dataset.\"\"\"\n",
    "    \n",
    "    # Probe type mapping\n",
    "    probe_mapping = {\n",
    "        \"linear\": (LinearProbe, LinearProbeConfig, SupervisedProbeTrainer),\n",
    "        \"logistic\": (LogisticProbe, LogisticProbeConfig, SupervisedProbeTrainer),\n",
    "        \"kmeans\": (KMeansProbe, KMeansProbeConfig, DirectionalProbeTrainer),\n",
    "        \"pca\": (PCAProbe, PCAProbeConfig, DirectionalProbeTrainer),\n",
    "        \"mean_diff\": (MeanDifferenceProbe, MeanDiffProbeConfig, DirectionalProbeTrainer)\n",
    "    }\n",
    "    \n",
    "    if probe_type not in probe_mapping:\n",
    "        raise ValueError(f\"Unknown probe type: {probe_type}\")\n",
    "        \n",
    "    probe_cls, config_cls, trainer_cls = probe_mapping[probe_type]\n",
    "    \n",
    "    # Create pipeline configuration\n",
    "    pipeline_config = ProbePipelineConfig(\n",
    "        dataset=dataset,\n",
    "        probe_cls=probe_cls,\n",
    "        probe_config=config_cls(input_size=hidden_size),\n",
    "        trainer_cls=trainer_cls,\n",
    "        trainer_config=SupervisedTrainerConfig(  # Use the config class directly\n",
    "            batch_size=32,\n",
    "            learning_rate=1e-3,\n",
    "            num_epochs=10 if probe_type in [\"linear\", \"logistic\"] else 1\n",
    "        ) if trainer_cls == SupervisedProbeTrainer else DirectionalTrainerConfig(\n",
    "            batch_size=32,\n",
    "            learning_rate=1e-3,\n",
    "            num_epochs=1\n",
    "        ),\n",
    "        position_key=\"ADJ\",\n",
    "        model_name=model_name,\n",
    "        hook_points=hook_points,\n",
    "        cache_dir=\"cache\"\n",
    "    )\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline = ProbePipeline(pipeline_config)\n",
    "    probe, history = pipeline.run()\n",
    "\n",
    "    \n",
    "    return probe.get_direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of single probe\n",
    "logistic_direction = run_single_probe(dataset, \"logistic\")\n",
    "kmeans_direction = run_single_probe(dataset, \"kmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
